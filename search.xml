<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ChamNet: Towards Efficient Network Design through Platform-Aware Model Adaptation</title>
    <url>/2020/02/20/ChamNet./</url>
    <content><![CDATA[<p><a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_ChamNet_Towards_Efficient_Network_Design_Through_Platform-Aware_Model_Adaptation_CVPR_2019_paper.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The rapid advancement of neural networks has led to breakthroughs in various domains such as computer vision, speech recognition, and natural language processing. However, the deployment of these powerful models is often hindered by the computational intensity and resource demands they impose, making them incompatible with resource-constrained platforms like mobile devices and IoT edge devices.</p>
<p>The key challenge lies in the significant variance in hardware characteristics across different platforms, which necessitates neural network architectures that can be efficiently adapted to leverage these traits. Furthermore, real-world applications often impose diverse constraints, such as strict latency requirements for real-time processing or energy efficiency for extended battery life, further complicating the design and deployment of neural networks.</p>
<p>To address these challenges, the paper introduces Chameleon, a novel framework for efficient neural network architecture design that prioritizes given resource constraints. Chameleon eschews the traditional approaches of developing new building blocks or employing computationally expensive reinforcement learning algorithms. Instead, it focuses on the strategic allocation of computation resources, adapting to the hardware traits of the target platform, and aligning with specific latency and energy constraints.</p>
<p>The framework operates within an optimization framework, utilizing predictive models for accuracy, latency, and energy to expedite the search for optimal architectures. By harnessing Gaussian Process with Bayesian optimization, Chameleon significantly reduces the time required to produce state-of-the-art models that excel in various platforms and scenarios, without the need for special features or extensive computational resources. This innovative methodology promises a new era of efficient neural network design, offering improved performance while respecting the limitations of real-world deployment environments.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>Firstly, it introduces a novel concept of computation distribution, demonstrating that adapting computation resources to building blocks is crucial for model performance. This insight allows the Chameleon framework to achieve substantial accuracy improvements over existing handcrafted and automatically designed architectures, without incorporating any special features.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330164901236.png" alt="image-20240330164901236"></p>
<p>Secondly, the paper proposes an efficient algorithm for searching optimal architectures through accurate and resource predictors. This algorithm leverages Gaussian Process with Bayesian optimization to enable a more effective search, outperforming reinforcement learning-based approaches in terms of search efficiency and effectiveness.</p>
<p>Additionally, the authors present a scalable and automated methodology for neural network architecture adaptation. The Chameleon framework, with its predictive models for accuracy, latency, and energy consumption, significantly reduces the search time and computational overhead. It achieves this by bypassing the need for extensive network training and direct performance metric measurements, which are typically required in traditional NAS and SMBO methods.</p>
<p>Lastly, the paper showcases the adaptability and generality of the Chameleon framework through extensive experiments on various hardware platforms, including mobile CPUs, DSPs, and GPUs. The results consistently show higher accuracy and reduced runtime latency compared to state-of-the-art models, highlighting the framework’s potential for large-scale heterogeneous deployment in real-world applications.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330164914312.png" alt="image-20240330164914312"></p>
<p>The methodology use a comprehensive framework designed to efficiently adapt neural network architectures to specific platforms and constraints. This approach, known as Chameleon, is centered around the concept of platform-aware model adaptation, which involves several key components and steps.</p>
<p><strong>Platform-aware Model Adaptation:</strong><br>The Chameleon framework begins by taking a default neural network architecture and a specific use scenario, which includes the target platform and resource budget. It then generates an adapted architecture that fits the scenario through an efficient evolutionary search (EES). This search is based on an adaptive genetic algorithm, where each architecture is represented by a vector of hyperparameters. The fitness of each candidate architecture is evaluated based on predictive models, and the best-performing ones are selected for the next generation through mutation and crossover operators.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165002398.png" alt="image-20240330165002398"></p>
<p><strong>Predictive Models:</strong><br>To expedite the search process, Chameleon employs predictive models for accuracy, latency, and energy consumption. These models bypass the time-consuming training and measurement process, allowing for immediate performance metric estimation. The accuracy predictor is built on a Gaussian Process (GP) regressor with Bayesian optimization, which provides reliable predictions and uncertainty estimations, guiding the sample architecture selection for training.</p>
<p><strong>Efficient Accuracy Predictor:</strong><br>The accuracy predictor is designed to estimate the final accuracy of a model without actual training. It uses a GP regressor, chosen for its reliability with scarce training data and its ability to produce predictions with uncertainty estimations. The sample architecture selection process is enhanced with Bayesian optimization, focusing on exploitation and exploration samples to improve convergence speed and sample efficiency.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165104157.png" alt="image-20240330165104157"></p>
<p><strong>Latency Predictor:</strong><br>Latency is a critical performance metric, and Chameleon constructs an operator latency look-up table (LUT) for fast and accurate latency estimation. This LUT, supported by a latency database from real-device benchmarks, allows for the summation of operator-level latencies to estimate the network-level latency. This method significantly reduces the time required for latency estimation compared to direct hardware measurements.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165115265.png" alt="image-20240330165115265"></p>
<p><strong>Energy Predictor:</strong><br>For devices with limited energy budgets, Chameleon includes an energy predictor. Similar to the accuracy predictor, it uses a GP model and Bayesian optimization but focuses solely on exploration samples to increase prediction confidence across the search space.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><strong>Experimental Setup:</strong> The authors employ a range of neural network architectures, including MobileNetV2 and ResNet, as the base models for adaptation. These models are chosen for their widespread use and proven performance in computer vision tasks. The experimental platforms include the Snapdragon 835 mobile CPU, Hexagon v62 DSP, Intel Xeon Broadwell CPU, and Nvidia GTX 1060 GPU. These diverse platforms allow for a comprehensive evaluation of the Chameleon framework’s ability to adapt to different hardware characteristics.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165156604.png" alt="image-20240330165156604"></p>
<p>The framework is tested under various latency and energy constraints to mimic real-world deployment scenarios. For mobile platforms, the constraints range from 4ms to 30ms, while for server models, they vary from 50ms to 400ms. Energy constraints are also set for mobile models, ranging from 15mJ to 150mJ, to reflect the energy limitations of battery-powered devices.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165206854.png" alt="image-20240330165206854"></p>
<p>The experiments utilize the ImageNet dataset, a standard benchmark for image classification, consisting of 1.2 million training images and 50,000 validation images. The dataset’s complexity and diversity provide a robust testbed for evaluating the adapted models’ accuracy.</p>
<p><strong>Experimental Results:</strong> The results of the experiments demonstrate that the Chameleon framework consistently outperforms existing models in terms of accuracy under the given constraints. For mobile platforms, the adapted ChamNet-Mobile models achieve top-1 accuracy rates of 73.8% and 75.3% on the Snapdragon 835 CPU and Hexagon v62 DSP, respectively, at 20ms latency. These rates represent significant improvements over baseline models like MobileNetV2 and MnasNet.</p>
<p>On server platforms, the ChamNet-Res models also show superior performance, with top-1 accuracy rates improving from 72.7% to 78.5% on an Intel CPU under 400ms latency constraint, and from 70.8% to 78.6% on an Nvidia GPU under 15ms latency constraint.</p>
<p>In the energy-driven adaptation experiments, ChamNet-Mobile demonstrates a notable improvement in the accuracy-energy trade-off. For instance, the model achieves 60.0% accuracy at only 14mJ per run, a 26% reduction in energy consumption compared to the MobileNetV2 0.75x baseline.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330165220972.png" alt="image-20240330165220972"></p>
<p>The paper also compares Chameleon with alternative adaptation and compression approaches, such as MNAS, AMC, NetAdapt, and MorphNet. The comparisons show that Chameleon offers the most favorable accuracy-latency trade-offs and reduces the computational cost from O(m · n · k) to O(m + n), where m, n, and k represent the number of network models, distinct platforms, and use scenarios, respectively.</p>
<p>In conclusion, the experimental results validate the Chameleon framework’s capability to efficiently adapt neural network architectures to meet specific performance and resource constraints. The consistent improvements in accuracy and efficiency across various platforms and scenarios highlight the practicality and potential of the Chameleon approach for real-world applications.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>CodeBear: Your Personal AI Coding Assitant</title>
    <url>/2024/02/28/CodeBear/</url>
    <content><![CDATA[<p>Repo: <a class="link" href="https://github.com/hjchen-thu/codebear">https://github.com/hjchen-thu/codebear <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>Demo: <a class="link" href="http://106.55.182.221:8080/">http://106.55.182.221:8080/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> (May be offline due to VPN issues)</p>
<h2 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h2><p>2024-04-05 added normal sampling args on web-ui</p>
<p>2024-03-29 changed to deepseek 33b &amp; 6b as base model</p>
<p>2024-03-26 changed to new web-ui developed by gradio</p>
<p>2024-03-10 fused mlp triton kernel</p>
<h2 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h2><ul>
<li>Special thanks to <a class="link" href="https://huggingface.co/deepseek-ai">Deepseek <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> team for releasing DeepSeek-Coder models with outstanding performance</li>
<li>Special thanks to <a class="link" href="https://github.com/feifeibear">feifeibear <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for releasing the implemention of speculative decoding with both Google’s and Deepmind’s versions(<a class="link" href="https://github.com/feifeibear/LLMSpeculativeSampling">LLMSpeculativeSampling <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>).</li>
<li>Special thanks to <a class="link" href="https://github.com/AutoGPTQ/">AutoGPTQ team <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for implementing GPTQ algorithm and open source the code.</li>
</ul>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>This repository combines <strong>GPTQ 4-bit quantization</strong> and <strong>Speculative Decoding</strong> to accelerate Large Language Models’ (LLM) inference for code completion tasks in <strong>personal usage scenarios</strong> (where GPU resources are limited yet there’s a pursuit for better performance and faster speed with larger models).</p>
<p><a class="link" href="https://arxiv.org/abs/2210.17323">GPTQ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> is a one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly efficient. And <a class="link" href="https://arxiv.org/abs/2302.01318">Speculative Decoding <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> is a innovative sampling strategy by using a small approximation model to propose sequences of tokens that will later be checked by a larger model.</p>
<p>By combining these two techniques, one can even deploy multiple LLMs in a single GPU with limited HBM memory usage. While benefiting from the improved performance brought by larger models, it also helps to accelerate inference speed to some extent.</p>
<p>The flowing figures are tested in a single V100(32GB) by deploying <a class="link" href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct">34B <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> and <a class="link" href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct">7B <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> models, with triton-based QuantLinear backend.</p>
<table>
<thead>
<tr>
<th>3 prefill + 200th decoding</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>Memory Usage(GB)</td>
<td>27.7</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>3 prefill + 200th decoding</th>
<th>7B(FP16)</th>
<th>7B(4Bit)</th>
<th>34B(4Bit)</th>
<th>Speculative 7B+34B(4Bit)</th>
</tr>
</thead>
<tbody><tr>
<td>Inference Speed(Tokens/sec)</td>
<td>14.3</td>
<td>34.1</td>
<td>7.9</td>
<td>9.4</td>
</tr>
</tbody></table>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240501174643087.png" alt="image-20240501174643087"></p>
<h3 id="Quick-Tour"><a href="#Quick-Tour" class="headerlink" title="Quick Tour"></a>Quick Tour</h3><h4 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">triton==2.1.0</span><br><span class="line">auto_gptq==0.7.0</span><br><span class="line">transformers==4.37.2</span><br></pre></td></tr></table></figure></div>
<h4 id="Step1-Quantize"><a href="#Step1-Quantize" class="headerlink" title="Step1: Quantize"></a>Step1: Quantize</h4><p>Download the float model from official(<a class="link" href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct">7B <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>and <a class="link" href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct">34B <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>, then quantize them.</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">#quntize the 7b model</span><br><span class="line">./make_quant.sh -f /PATH/TO/7B/FLOAT/MODEL -q /PATH/TO/7B/QUANT/MODEL</span><br><span class="line">#quntize the 34b model</span><br><span class="line">./make_quant.sh -f /PATH/TO/34B/FLOAT/MODEL -q /PATH/TO/34B/QUANT/MODEL</span><br></pre></td></tr></table></figure></div>

<p>The basic config of quantization is set to bits = 4, group_num = 128 (can be changed in ./scripts/quantize.py).</p>
<h4 id="Step2-Serving"><a href="#Step2-Serving" class="headerlink" title="Step2: Serving"></a>Step2: Serving</h4><p>Start serving</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">./start_server.sh -s /PATH/TO/7B/QUANT/MODEL -l /PATH/TO/34B/QUANT/MODEL -t /PATH/TO/7B/FLOAT/MODEL</span><br></pre></td></tr></table></figure></div>
<p>Default sampling params are set to max_tokens = 500, top_k = 10, top_p = 0.9 (can be changed in ./scripts/serving.py).</p>
<p>Send request (<strong>the model is specially trained for code completion with python</strong>)</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -X POST -H "Content-Type: application/json" -d '{"prompt": "def quicksort("}' http://127.0.0.1:5000/codebear</span><br></pre></td></tr></table></figure></div>

<p><a href="https://github.com/hjchen-thu/codebear/blob/main/images/request.png"><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/request.png" alt="alt text"></a></p>
<h3 id="Future-plans"><a href="#Future-plans" class="headerlink" title="Future plans"></a>Future plans</h3><table>
<thead>
<tr>
<th>Progress</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>fused_flash_attn_MHA triton implemention</td>
<td>todo</td>
</tr>
<tr>
<td>fused_flash_attn_GQA triton implemention</td>
<td>todo</td>
</tr>
<tr>
<td>INT8 KV cache</td>
<td>todo</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>LLM</tag>
        <tag>Transformer</tag>
        <tag>Quantization</tag>
        <tag>Triton</tag>
      </tags>
  </entry>
  <entry>
    <title>Analyzing Transformer Models: Parameters and Memory usage</title>
    <url>/2022/09/28/ATM_parameter/</url>
    <content><![CDATA[<p>Recently, OpenAI’s release of ChatGPT has demonstrated outstanding performance, sparking a research frenzy in the field of Large Language Models (LLMs). The “large” in Large Language Models is reflected in two aspects: the scale of model parameters and the scale of training data. Taking GPT-3 as an example, it boasts a staggering 175 billion parameters and was trained on a colossal dataset of 570 GB. Consequently, training Large Language Models faces two primary challenges: GPU memory efficiency and computational efficiency.</p>
<p>In the current landscape of the industry, large language models are predominantly built upon the Transformer architecture. These models can be broadly categorized into two main types: encoder-decoder models (with T5 as a representative) and decoder-only models. Specifically, decoder-only structures can be further divided into Causal LM (exemplified by the GPT series) and Prefix LM (represented by models like GLM). Due to the tremendous success achieved by the GPT series, the majority of mainstream large language models opt for the Causal LM structure. As a result, this series of articles delve into an analysis of model parameters, computational complexity, intermediate activation values, and KV cache to gain a deeper understanding of the GPU memory efficiency and computational efficiency involved in training large language models.</p>
<p>For the purpose of analysis, let’s define some mathematical symbols in advance. Let the number of layers in the Transformer model be denoted as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>, the hidden layer dimension as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container>, the number of attention heads as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>. The vocabulary size is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>, the batch size for training data is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container> and the sequence length is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/bWnx0.png" alt="Overview of transformer architecture"></p>
<h1 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h1><p>The Transformer model consists of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container> layers, with each layer comprising two parts: a self-attention block and an MLP block.</p>
<p>The self-attention block of the model has parameters including <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="7.553ex" height="2.032ex" role="img" focusable="false" viewBox="0 -704 3338.3 898"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(791,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1235.7,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(2124.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2569.3,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>weight matrices <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="12.901ex" height="2.195ex" role="img" focusable="false" viewBox="0 -683 5702 970.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g><g data-mml-node="mo" transform="translate(1586.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2031,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mo" transform="translate(3686.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4131.3,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></g></svg></mjx-container> and biases, output weight matrices <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.544ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1566.5 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g></g></g></g></svg></mjx-container>, and biases. The shapes of the four weight matrices are <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.87ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2152.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(854,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1298.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(1874.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>, and the shapes of the four biases are [<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>]. The number of parameters in the self-attention block is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="8.622ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 3811 915.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="msup" transform="translate(500,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1734.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2735,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(3235,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>.</p>
<p>The MLP block consists of two linear layers. Typically, the first linear layer maps the dimension from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.434ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1076 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>, and the second linear layer maps the dimension from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.434ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1076 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container> to <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>. The shape of the weight matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container> for the first linear layer is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.002ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2652.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(854,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(1298.7,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(1798.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(2374.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>, and the shape of its bias is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.692ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1632 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mn" transform="translate(278,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(1354,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>. The shape of the weight matrix <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container> for the second linear layer is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.002ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2652.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mn" transform="translate(278,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(778,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(1354,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1798.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(2374.7,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>, and the shape of its bias is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.561ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1132 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(854,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>. The total number of parameters in the MLP block is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="8.622ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 3811 915.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g><g data-mml-node="msup" transform="translate(500,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1734.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2735,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g><g data-mml-node="mi" transform="translate(3235,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>.</p>
<p>The self-attention block and the MLP block each have one layer normalization, which includes two trainable model parameters: the scaling parameter <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container> and the shifting parameter <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container>, both of which have the shape <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="2.561ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1132 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(854,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>. The total number of parameters for the two layer normalizations is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.434ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 1076 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>.</p>
<p>In total, the parameter count for each transformer layer is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="10.885ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 4811 915.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="msup" transform="translate(1000,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2234.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3235,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(4235,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>.</p>
<p>Moreover, the parameter count of the word embedding matrix is also substantial. The word vector dimension is usually equal to the hidden layer dimension <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>, and the parameter count of the word embedding matrix is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.043ex" height="1.62ex" role="img" focusable="false" viewBox="0 -694 1345 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(769,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>. The weight matrix of the final output layer is often parameter-shared with the word embedding matrix.</p>
<p>Regarding positional encoding, if trainable positional encoding is adopted, there will be some trainable model parameters, but the number is relatively small. If relative positional encoding is adopted, such as RoPE and ALiBi, it does not contain trainable model parameters. We will ignore these parameters.</p>
<p>In summary, the trainable parameter count for a transformer model with  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container> layers is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="19.128ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 8454.4 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(298,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(687,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="msup" transform="translate(1687,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2921.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3922,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(4922,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(5498,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6109.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7109.4,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(7878.4,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>. When the hidden dimension <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container> is relatively large, the linear term can be ignored, and the model parameter count is approximately <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="5.227ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 2310.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="msup" transform="translate(1298,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>.</p>
<p>Next, we estimate the parameter count for different versions of the LLaMA model.</p>
<table>
<thead>
<tr>
<th align="left">Parameters</th>
<th>Hidden dimension <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container></th>
<th>Layers <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container></th>
<th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="5.227ex" height="1.912ex" role="img" focusable="false" viewBox="0 -833.9 2310.6 844.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="msup" transform="translate(1298,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></th>
</tr>
</thead>
<tbody><tr>
<td align="left">6.7B</td>
<td>4096</td>
<td>32</td>
<td>6,442,450,944</td>
</tr>
<tr>
<td align="left">13.0B13.0B</td>
<td>5120</td>
<td>40</td>
<td>12,582,912,000</td>
</tr>
<tr>
<td align="left">32.5B32.5B</td>
<td>6656</td>
<td>60</td>
<td>31,897,681,920</td>
</tr>
<tr>
<td align="left">65.2B</td>
<td>8192</td>
<td>80</td>
<td>64,424,509,440</td>
</tr>
</tbody></table>
<h1 id="Memory-Usage"><a href="#Memory-Usage" class="headerlink" title="Memory Usage"></a>Memory Usage</h1><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><h3 id="Weights"><a href="#Weights" class="headerlink" title="Weights"></a>Weights</h3><p>In the inference phase of neural networks, there are neither optimizer states nor gradients, and it is not necessary to store intermediate activations. With the absence of gradients, optimizer states, and intermediate activations, the GPU memory usage during the model inference stage is much less than during the training stage. During the model inference phase, the primary occupier of GPU memory is the model parameters. If float16 is used for inference, the GPU memory occupied by the model parameters during the inference stage is approximately <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="7.776ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 3437 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g><g data-mml-node="mi" transform="translate(1222,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(1651,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2141,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(2502,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2968,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>. If KV cache is used to accelerate the inference process, it will also occupy GPU memory, and the memory usage of the KV cache will be detailed in the following sections. Additionally, input data needs to be placed on the GPU, and there are some intermediate results (intermediate results during the inference process are released as soon as possible), but the memory occupied by this part is very small and can be ignored.</p>
<h3 id="Activatons"><a href="#Activatons" class="headerlink" title="Activatons"></a>Activatons</h3><p>During the inference phase of a large deep learning model, the memory usage for activation values is typically smaller than during the training phase. This is because during inference, the model does not need to compute gradients, and therefore does not need to store the intermediate information required for backpropagation. However, activation values still need to be stored for use in the forward pass.</p>
<p>Memory pooling is a technique that allows models to use memory more efficiently during inference by reusing the space that has already been allocated for computed activation values. This method is particularly useful for model structures where activation values can be reused.</p>
<p>Here are some considerations for the memory usage of activation values:</p>
<ol>
<li><p><strong>Model Size</strong>: The scale of the model (number of parameters) affects the number of activation values, which in turn affects memory usage.</p>
</li>
<li><p><strong>Sequence Length</strong>: For models that process sequential data, such as natural language processing (NLP) models, the longer the sequence, the more memory is used for activation values.</p>
</li>
<li><p><strong>Batch Size</strong>: An increase in batch size results in a greater number of samples being processed simultaneously, which increases the memory usage for activation values.</p>
</li>
<li><p><strong>Memory Pooling</strong>: Through memory pooling techniques, the memory usage for activation values can be reduced, as storage space can be reused.</p>
</li>
<li><p><strong>Optimization Techniques</strong>: Using specific optimization techniques, such as mixed precision training or quantization, can also reduce memory usage.</p>
</li>
<li><p><strong>Model Architecture</strong>: Some model architectures may inherently have less memory usage for activation values, such as attention mechanisms that use fewer parameters.</p>
</li>
</ol>
<p>In practice, the memory usage will vary depending on the specific model architecture, the size of the input data, and whether memory optimization techniques are employed. For large models, such as GPT-3, memory pooling can significantly reduce the memory usage during the inference phase, allowing the model to run with limited hardware resources.</p>
<h3 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV cache"></a>KV cache</h3><p>Assuming the length of the input sequence is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.023ex" role="img" focusable="false" viewBox="0 -442 469 452"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container>, the output sequence length is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>, and the KV cache is saved in float16, then the peak memory usage of the KV cache is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="16.428ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7261.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(1444.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(2166.7,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(2888.9,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(3317.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3706.9,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(4398.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(5398.3,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(5998.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(6387.3,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(6963.3,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container> bytes. Here, the first 2 represents the K/V cache, and the second 2 indicates that float16 occupies 2 bytes.</p>
<p>Taking GPT-3 as an example, let’s compare the memory usage of the KV cache with the model parameters. The GPT-3 model occupies a memory size of 350GB. Assume batch size <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.25ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2762.6 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(706.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1762.6,0)"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container>, input sequence <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="7.472ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 3302.6 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(746.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1802.6,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path></g></g></g></svg></mjx-container>, output sequence <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.637ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2933.6 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(877.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1933.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container>, then the KV cache occupies <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="6.889ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 3045 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(500,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1000,0)"></path></g><g data-mml-node="mi" transform="translate(1500,0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(2286,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g></svg></mjx-container> memory, which is approximately 0.5 times the memory of the model parameters.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Weights-1"><a href="#Weights-1" class="headerlink" title="Weights"></a>Weights</h3><p>In the process of training neural networks, the major components that occupy GPU memory can be primarily divided into four parts: model parameters, intermediate activations generated during forward computation, gradients computed during backpropagation, and optimizer state. This discussion specifically analyzes the GPU memory occupation of parameters, gradients, and optimizer state, with a detailed introduction to the memory occupation of intermediate activations to follow later. When training large models, the AdamW optimizer is commonly used, and mixed-precision training is employed to accelerate the training. The analysis of GPU memory usage is based on this premise.</p>
<p>In a single training iteration, each trainable model parameter corresponds to one gradient and two optimizer states (first and second momentum of the gradient in the Adam optimizer). Assuming the number of model parameters is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.633ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 722 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g></g></g></svg></mjx-container>, the number of elements in the gradient is also <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.633ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 722 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g></g></g></svg></mjx-container>, and the number of elements in the AdamW optimizer is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.765ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1222 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g></g></g></svg></mjx-container>. An element of the float16 data type occupies 2 bytes, while an element of the float32 data type occupies 4 bytes. In mixed-precision training, float16 model parameters are used for forward and backward propagation, yielding float16 gradients; when the optimizer updates the model parameters, float32 optimizer states, float32 gradients, and float32 model parameters are used to update the model parameters. Therefore, for each trainable model parameter, a memory of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="36.187ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 15994.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(389,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(1111.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2111.4,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mo" transform="translate(2611.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3222.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(4222.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(4611.9,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(5334.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(6334.3,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mo" transform="translate(6834.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7445.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(8445.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(8834.8,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mo" transform="translate(9557,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(10557.2,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mo" transform="translate(11057.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(11724,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(12779.8,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(13779.8,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(14208.8,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(14698.8,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(15059.8,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(15525.8,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g></svg></mjx-container> is occupied. When training a large model with parameter size <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.633ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 722 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g></g></g></svg></mjx-container> using the AdamW optimizer and mixed-precision training, the GPU memory size occupied by the model parameters, gradients, and optimizer states is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.896ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1722 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="3A6" d="M312 622Q310 623 307 625T303 629T297 631T286 634T270 635T246 636T211 637H184V683H196Q220 680 361 680T526 683H538V637H511Q468 637 447 635T422 631T411 622V533L425 531Q525 519 595 466T665 342Q665 301 642 267T583 209T506 172T425 152L411 150V61Q417 55 421 53T447 48T511 46H538V0H526Q502 3 361 3T196 0H184V46H211Q231 46 245 46T270 47T286 48T297 51T303 54T307 57T312 61V150H310Q309 151 289 153T232 166T160 195Q149 201 136 210T103 238T69 284T56 342Q56 414 128 467T294 530Q309 532 310 533H312V622ZM170 342Q170 207 307 188H312V495H309Q301 495 282 491T231 469T186 423Q170 389 170 342ZM415 188Q487 199 519 236T551 342Q551 384 539 414T507 459T470 481T434 491T415 495H410V188H415Z"></path></g></g></g></svg></mjx-container>.</p>
<h3 id="Activations"><a href="#Activations" class="headerlink" title="Activations"></a>Activations</h3><p><a class="link" href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>Note</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Direct federated neural architecture search</title>
    <url>/2022/01/01/DFNAs/</url>
    <content><![CDATA[<p><a class="link" href="https://dp-ml.github.io/2021-workshop-ICLR/files/32.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Federated Learning (FL) has emerged as a leading approach for training machine learning models in a privacy-preserving manner, by allowing multiple clients to collaboratively train a shared model without directly sharing their data. However, the effectiveness of FL is often limited by the use of static, predefined neural network architectures that do not account for the diverse and heterogeneous nature of client-held data.</p>
<p>In the realm of neural network design, Neural Architecture Search (NAS) has demonstrated significant potential in automating the discovery of high-performing architectures tailored to specific tasks and datasets. Yet, the integration of NAS with FL has been hindered by the computational demands of existing methods and their inability to adapt to the non-IID data distribution common in FL scenarios.</p>
<p>This discrepancy between the potential benefits of NAS and the practical challenges of its application in FL environments underscores the need for a more efficient and adaptable approach. The goal is to develop an end-to-end NAS technique that can seamlessly operate within the constraints of FL, dynamically searching for and optimizing neural network architectures that not only enhance model accuracy but also minimize resource usage.</p>
<p>Such an approach would be pivotal in unlocking the full potential of FL, enabling it to deliver more accurate and personalized models at scale, while upholding the privacy and regulatory requirements of various domains. By overcoming the computational and data distribution barriers, this research aims to propel FL towards broader adoption and more impactful applications.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>This research introduces a groundbreaking method that significantly advances the field of Federated Learning (FL) by seamlessly integrating Neural Architecture Search (NAS) into the FL framework. The primary contributions of this work are threefold:</p>
<ol>
<li><p>It pioneers the concept of FedNAS for both cross-silo and cross-device environments, where data is partitioned by examples, and the architecture is learned in tandem with model parameters. This innovative approach allows for the discovery of optimal neural network architectures that are tailored to the specific characteristics of each client’s data.</p>
</li>
<li><p>The proposed method offers an efficient, plug-and-play solution for searching and deploying neural network architectures within the federated setting. By eliminating the need for retraining the derived network on the clients, it simplifies the process and reduces the computational overhead, making it suitable for low-resource devices.</p>
</li>
<li><p>Through extensive experiments, the research demonstrates a substantial reduction in communication rounds and computational resources compared to existing methods. For the same number of communication rounds, the method achieves an average test accuracy improvement of up to 10% compared to predefined models with Federated Averaging (FedAvg).</p>
</li>
</ol>
<p>These contributions collectively pave the way for more efficient, accurate, and resource-conscious FL systems, capable of handling the intricacies of real-world data distributions and computational constraints. The work sets a new standard for the application of NAS in FL, pushing the boundaries of what is possible in the pursuit of optimized and privacy-preserving machine learning models.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330173119893.png" alt="image-20240330173119893"></p>
<p>The methodology presented in this research is a novel approach to addressing the challenges of applying Neural Architecture Search (NAS) within the context of Federated Learning (FL). The core of this approach is a direct federated NAS technique that is designed to identify and optimize neural network architectures in a one-stage process, specifically tailored for the heterogeneous and distributed nature of FL environments.</p>
<p>The method begins by defining the problem within the federated learning paradigm, where multiple clients, each with their own local data, collaborate to train a shared model without exchanging data. The key innovation is the inclusion of the network architecture as a variable parameter, alongside the traditional model weights. This allows for the simultaneous optimization of both the architecture and the model parameters, aiming to maximize the predictive performance on the given task.</p>
<p>To achieve this, the method employs a parent network, which is a directed acyclic graph (DAG) representing the search space of possible neural network architectures. Each node in the DAG corresponds to an intermediate representation of the data, and the edges represent transformations such as pooling or convolution. Multiple edges between nodes represent the different architectural choices available.</p>
<p>The algorithm then samples a set of architecture parameters, which are used to construct a child network from the parent network. This child network is trained on the local data of each client using a standard optimization process, with gradients being backpropagated not only through the weights but also through the architecture parameters. This allows the algorithm to learn which architectural choices are most beneficial for the given data.</p>
<p>A key aspect of this method is its hardware-agnostic nature, meaning it can be applied to any type of client hardware, from multi-GPU clusters in data centers to edge devices like smartphones. The parent network is constructed based on the hardware configuration, ensuring that the search process is tailored to the specific computational capabilities of the target devices.</p>
<p>The federated aspect of the method is orchestrated by a central server, which selects a subset of clients to participate in each training round. The server aggregates the updates from the clients, updating the global architecture and weight parameters accordingly. This process is repeated over multiple rounds until convergence or a predetermined stopping criterion is met.</p>
<p>The method’s one-stage nature is a significant departure from traditional two-stage NAS methods, which often require a separate retraining phase after the architecture has been discovered. By integrating the search and training processes, this approach reduces the overall computational cost and communication overhead, making it more suitable for the resource-constrained FL setting.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330173241086.png" alt="image-20240330173241086"></p>
<p>The experimental framework of this research is meticulously designed to evaluate the efficacy of the proposed direct federated Neural Architecture Search (NAS) method within the context of Federated Learning (FL). The experiments are structured to assess the computational efficiency, communication rounds, and test accuracy of the method in comparison to existing approaches.</p>
<p><strong>Experimental Setup:</strong></p>
<p>The experiments are conducted using the FedML research library, which provides a comprehensive platform for FL experimentation. The target task for evaluation is image classification, a widely recognized benchmark for assessing machine learning models. Two primary datasets are utilized: the CIFAR-10 dataset for cross-silo settings and the CINIC-10 dataset for cross-device settings. These datasets are manipulated to create both Independent and Identically Distributed (IID) and non-IID data distributions to simulate various real-world scenarios.</p>
<p>The search space for the parent network is constructed using shuffle blocks, with different configurations for cross-silo and cross-device experiments to accommodate the diversity of client devices. The method’s performance is compared against the Federated Averaging (FedAvg) algorithm and other federated NAS methods, such as FedNAS and MiLeNAS.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330173312878.png" alt="image-20240330173312878"></p>
<p><strong>Experimental Results:</strong></p>
<p>The results of the experiments demonstrate a significant improvement in computational efficiency and test accuracy. The proposed method achieves an order of magnitude reduction in resource consumption compared to prior art, with a notable reduction in communication rounds required for training. This reduction is crucial for FL, as it minimizes the data transmitted between clients and the server, thereby preserving privacy and reducing energy consumption.</p>
<p>In terms of test accuracy, the method outperforms predefined models with FedAvg, showing an average improvement of up to 10% for the same number of communication rounds. This improvement is attributed to the method’s ability to adapt the network architecture to the specific characteristics of the non-IID data, leading to more accurate and robust models.</p>
<p>The experiments also reveal that the method is highly scalable and robust across different client configurations. The performance of the method remains consistent even as the number of clients and the distribution of data vary, showcasing its adaptability to diverse FL environments.</p>
<p>Furthermore, the memory consumption during the search process is significantly lower than other methods, making the approach viable for deployment on low-memory hardware such as smartphones and IoT devices. This is a critical aspect for real-world FL applications, where client devices can have significantly varying computational capabilities.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
        <tag>FL</tag>
      </tags>
  </entry>
  <entry>
    <title>Edd: Efficient differentiable dnn architecture and implementation co-search for embedded ai solutions</title>
    <url>/2021/08/28/Edd/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/2005.02563.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In the rapidly evolving field of Artificial Intelligence (AI), the quest for high-quality AI solutions has led to a growing interest in the joint optimization of AI algorithms and their hardware implementations. Deep Neural Networks (DNNs), the workhorse of modern AI, have demonstrated remarkable achievements but are often limited by the efficiency of their hardware deployment. Traditional approaches to enhance algorithm quality, such as neural architecture search (NAS), have focused on developing DNNs that surpass human-designed architectures. However, these methods often overlook the simultaneous optimization of hardware implementations, which is crucial for improving performance metrics like latency, throughput, and energy efficiency.</p>
<p>Recent advancements in hardware-aware NAS and hardware/software co-design have begun to address this gap by considering hardware features in DNN design. Yet, there remains a significant opportunity to further optimize by integrating the hardware implementation search within the NAS process. This integration can provide more accurate performance evaluations and instant guidance for hardware-aware DNN design.</p>
<p>The motivation behind this research is to bridge this gap by proposing a novel methodology that performs a fully simultaneous and efficient co-search for DNN architecture and hardware implementation. This approach aims to maximize both algorithm accuracy and hardware implementation quality, offering a unified solution applicable to various devices and performance objectives. By fusing DNN search variables with hardware implementation variables into a single solution space and leveraging differentiable formulations, the methodology enables the use of gradient descent algorithms, significantly reducing search time and optimizing AI solutions for embedded AI applications.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181325138.png" alt="image-20240330181325138"></p>
<p>This research introduces a groundbreaking methodology that addresses the critical need for a simultaneous and efficient approach to co-search for DNN architecture and hardware implementation. The primary contributions of the study are manifold:</p>
<ol>
<li><p>It pioneers a mathematical formulation that unifies the search space for DNN architecture and hardware implementation, aiming to maximize DNN accuracy and implementation performance concurrently. This formulation is differentiable, allowing for the application of gradient descent algorithms and significantly expediting the search process.</p>
</li>
<li><p>The proposed formulation is versatile, applicable across various hardware platforms such as GPUs, FPGAs, and dedicated accelerators. It is designed to target diverse performance objectives, including latency, throughput, and energy efficiency, while also considering resource usage and sharing constraints.</p>
</li>
<li><p>The methodology demonstrates its effectiveness through experiments that search for representative DNNs tailored for low-latency GPU implementation and FPGA implementations with both recursive and pipelined architectures. Each model produced achieves comparable accuracy to state-of-the-art NAS methods but with superior performance within a short search timeframe.</p>
</li>
<li><p>The study also presents a novel approach to differentiable quantization and performance/resource formulation, which is crucial for the co-search process. This allows for the optimization of data precision in DNNs, leading to more efficient computation and resource utilization.</p>
</li>
</ol>
<p>These contributions collectively push the boundaries of AI research by providing a comprehensive framework that enhances the quality and efficiency of AI solutions, particularly for embedded systems where performance and resource constraints are paramount.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The methodology at the core of this research is an innovative approach to the co-search problem, which integrates the search for optimal DNN architectures with the search for efficient hardware implementations. This is achieved through a differentiable framework that allows for a unified optimization process.</p>
<p>At the outset, the research introduces a mathematical formulation that combines DNN search variables, which define the structure and parameters of the network, with hardware implementation variables, which pertain to the physical realization of the DNN on various devices. This fusion creates a unified solution space where both algorithm accuracy and hardware performance are optimized simultaneously.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181352045.png" alt="image-20240330181352045"></p>
<p>The key innovation lies in the differentiability of the formulation. By ensuring that the objective function, which includes both accuracy loss and performance loss, is differentiable with respect to the fused variables, gradient descent algorithms can be applied. This enables efficient optimization, significantly reducing the search time compared to traditional exhaustive search methods.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181403203.png" alt="image-20240330181403203"></p>
<p>The methodology also introduces a unique approach to differentiable quantization, a critical aspect of hardware implementation. Quantization involves reducing the precision of data to increase computational efficiency, but it must be carefully balanced with the impact on DNN accuracy. The proposed method uses a Gumbel-Softmax function to convert the discrete, non-differentiable sampling of quantization into a continuous, differentiable process. This allows for the optimization of quantization levels in tandem with the DNN architecture.</p>
<p>Furthermore, the methodology provides a comprehensive performance and resource formulation that captures the intermediate performance and resource usage of each DNN operation and block. This includes the latency of operations under different quantization schemes and the resource requirements in terms of hardware-specific parameters, such as the number of DSPs on an FPGA.</p>
<p>The co-search process is carried out in a bilevel fashion. In the first level, the DNN weights are optimized on the training dataset to minimize the training loss. In the second level, the DNN architecture and implementation variables are updated by minimizing the objective function on the validation set. This iterative process continues until convergence or a fixed number of epochs is reached.</p>
<p>The research demonstrates the effectiveness of the methodology through extensive experiments on three different hardware platforms: GPUs, recursive FPGA accelerators, and pipelined FPGA accelerators. For each platform, a representative DNN model is searched and optimized, achieving competitive accuracy while significantly improving performance metrics such as latency and throughput.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p>The experimental setup of this research is meticulously designed to validate the effectiveness of the proposed co-search methodology. The experiments are conducted on a subset of the ImageNet dataset, which is a standard benchmark for evaluating DNN models. The models searched by the methodology are trained from scratch on the entire ImageNet dataset, which consists of 1000 classes, to ensure a comprehensive assessment of their performance.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181427801.png" alt="image-20240330181427801"></p>
<p>Three distinct DNN models, referred to as EDD-Nets, are targeted in the experiments: EDD-Net-1 for low-latency GPU implementation, EDD-Net-2 for recursive FPGA architecture, and EDD-Net-3 for pipelined FPGA architecture. The initial DNN for the search consists of 20 MBConv blocks, with each block offering a variety of operation candidates defined by different filter sizes and channel expansion ratios. The DNN weights and activations are optimized for each hardware platform, considering the specific constraints and capabilities of GPUs and FPGAs.</p>
<p>The experiments utilize a fixed number of epochs for the EDD search process, with the DNN weights updated through stochastic gradient descent. The architecture and implementation variables are optimized using a bilevel approach, where the DNN weights are first fixed and the objective function is minimized over the validation set. This process is repeated until the training converges or the predefined number of epochs is completed.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181441030.png" alt="image-20240330181441030"></p>
<p>The results of the experiments demonstrate the efficacy of the co-search methodology. EDD-Net-1 achieves a top-1 test error of 25.3% and a latency of 11.17 ms on the Titan RTX GPU, outperforming existing hardware-aware NAS approaches in terms of inference speed. When re-trained and fine-tuned on an Nvidia 1080 Ti GPU, EDD-Net-1 shows a latency reduction across different data precisions, with the shortest latency of 1.74 ms achieved using 8-bit integer precision.</p>
<p>For FPGA implementations, EDD-Net-2 and EDD-Net-3 are compared with state-of-the-art solutions. EDD-Net-2, optimized for a recursive FPGA accelerator, delivers a latency of 7.96 ms on the Xilinx ZCU102 FPGA, outperforming other models by a significant margin. EDD-Net-3, designed for a pipelined FPGA architecture, achieves a throughput of 40.2 frames per second on the ZC706 FPGA, indicating a 1.45倍 increase in performance over the previous best solution.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Efficient Neural Architecture Search via Parameter Sharing</title>
    <url>/2020/01/12/Enas./</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1802.03268.pdf">https://arxiv.org/pdf/1802.03268.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Neural Architecture Search (NAS) has emerged as a revolutionary approach to automate the design of neural network architectures, offering the potential to discover high-performing models without the need for manual trial and error. However, the computational cost and time consumption associated with NAS have been prohibitive, often requiring hundreds of GPUs and days of training to find optimal architectures.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144354863.png" alt="image-20240330144354863"></p>
<p>The motivation behind Efficient Neural Architecture Search (ENAS) stems from the need to address these challenges and make NAS more accessible and efficient. By introducing a paradigm where a controller discovers neural network architectures within a large computational graph through parameter sharing, ENAS dramatically reduces the resources required for the search process. This innovative method not only maintains the empirical performance of traditional NAS but also achieves it with a fraction of the computational expense, using only a single GPU and a matter of hours.</p>
<p>ENAS’s ability to share parameters among child models during the search process is a key factor in its efficiency. This design choice allows for a more stable and effective learning process for the controller, which in turn can explore a vast search space of architectures more thoroughly and rapidly than its counterparts. The result is a method that not only discovers state-of-the-art architectures for tasks like image classification and language modeling but also does so in a way that is practical and scalable.</p>
<p>By democratizing access to advanced neural network design through ENAS, researchers and practitioners can now focus more on innovation and less on the resource-intensive process of architecture search. This shift has the potential to accelerate advancements in machine learning and artificial intelligence, unlocking new possibilities and applications that were previously out of reach due to the computational barriers of traditional NAS methods.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>Efficient Neural Architecture Search (ENAS) makes several significant contributions to the field of automated machine learning (AutoML). Firstly, it introduces a novel approach to neural architecture search by leveraging parameter sharing among a multitude of child models. This innovative strategy allows ENAS to explore a vast design space with remarkable efficiency, using a single GPU and within a few hours, which is a stark contrast to the extensive computational resources required by traditional NAS methods.</p>
<p>Secondly, ENAS demonstrates that parameter sharing not only reduces computational costs but also leads to competitive model performance. The method achieves state-of-the-art results on established benchmarks such as the Penn Treebank dataset for language modeling and the CIFAR-10 dataset for image classification, matching or even surpassing the performance of architectures found by more expensive NAS methods.</p>
<p>Thirdly, ENAS presents a flexible framework that can be applied to various types of neural networks, from recurrent neural networks (RNNs) for sequential data to convolutional neural networks (CNNs) for image data. This versatility is a testament to the general applicability of ENAS and its potential to impact a wide range of domains within machine learning.</p>
<p>Lastly, ENAS’s contributions extend to the methodology of neural architecture search itself. By improving the efficiency of the search process, ENAS opens up new possibilities for research and experimentation. It enables more researchers to engage in neural architecture research, fostering innovation and potentially leading to the discovery of even more efficient and effective methods in the future. The impact of ENAS is thus both immediate, through its superior performance and efficiency, and long-term, as it paves the way for further advancements in the field of AutoML.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>In the pursuit of enhancing the efficiency of Neural Architecture Search (NAS), the paper introduces Efficient Neural Architecture Search (ENAS), a groundbreaking method that stands out for its innovative use of parameter sharing and computational graph optimization. The specific methodologies employed in ENAS can be summarized as follows:</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144735504.png" alt="image-20240330144735504"></p>
<ol>
<li><p><strong>Controller and Child Models</strong>: At the heart of ENAS lies a controller, typically an LSTM RNN, which is responsible for sampling decisions to construct child models. These child models are subgraphs within a larger computational graph, and they share the same set of parameters, which drastically reduces the computational resources needed to search for and train new architectures.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144757765.png"></p>
</li>
<li><p><strong>Parameter Sharing</strong>: A pivotal concept in ENAS is the sharing of parameters across all child models. This is achieved by defining the search space as a directed acyclic graph (DAG) where nodes represent local computations and edges represent the flow of information. Each node in the DAG has its own set of parameters, which are used when the node is active in a child model. By sharing parameters, ENAS minimizes redundancy and maximizes the efficiency of the search process.</p>
</li>
<li><p><strong>Policy Gradient for Controller Training</strong>: The controller is trained using policy gradient methods, specifically the REINFORCE algorithm. It selects subgraphs (child models) that are expected to yield high rewards on a validation set. The reward function is designed to encourage the selection of models that generalize well, steering the search towards architectures that perform well on unseen data.</p>
</li>
<li><p><strong>Training Procedure</strong>: ENAS employs a two-phase training process. The first phase involves training the shared parameters (ω) of the child models on the entire training dataset. The second phase focuses on training the controller’s parameters (θ) to optimize the policy that selects child models. These phases are interleaved, with the controller learning from the performance of the child models it generates.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144826654.png" alt="image-20240330144826654"></p>
</li>
<li><p><strong>Designing Recurrent and Convolutional Cells</strong>: ENAS provides a flexible framework for designing both recurrent and convolutional architectures. For recurrent cells, the controller decides on the connections and operations within a DAG, allowing for the creation of complex RNN structures. Similarly, for convolutional architectures, the controller makes decisions on connecting previous nodes and choosing computation operations, facilitating the formation of intricate CNN structures.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144855741.png" alt="image-20240330144855741"></p>
</li>
<li><p><strong>Efficient Search Space Representation</strong>: The search space in ENAS is represented as a DAG, where each possible architecture is a subgraph of this larger structure. This representation allows for an exponential number of configurations, making the search space comprehensive and diverse, yet tractable due to parameter sharing.</p>
</li>
</ol>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330144921006.png" alt="image-20240330144921006"></p>
<ol start="7">
<li><strong>Deriving Final Architectures</strong>: Once the controller is trained, ENAS derives final architectures by sampling from the policy and selecting the model with the highest expected reward for further training. This process can be repeated with different samples to explore the search space more thoroughly, although the paper demonstrates that even a single sample can yield competitive results.</li>
</ol>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330145132477.png" alt="image-20240330145132477"></p>
<p>The experimental results presented in the paper on Efficient Neural Architecture Search (ENAS) are both compelling and insightful, showcasing the method’s effectiveness in discovering high-performing neural network architectures while significantly reducing computational costs. Here’s a summary of the key findings:</p>
<ol>
<li><p><strong>Penn Treebank Dataset</strong>: ENAS was applied to design recurrent neural network (RNN) cells for language modeling on the Penn Treebank dataset. The method successfully discovered a novel architecture that achieved a test perplexity of 55.8, setting a new state-of-the-art among methods without post-training processing. This result was particularly impressive given that ENAS required only 1000x less GPU hours compared to standard NAS approaches.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330145308528.png"></p>
</li>
<li><p><strong>CIFAR-10 Dataset</strong>: For image classification, ENAS was utilized to find novel convolutional network architectures on the CIFAR-10 dataset. The method identified an architecture that achieved a test error of 2.89%, which is comparable to the 2.65% test error of NASNet, a highly-regarded architecture discovered through extensive computational search.</p>
</li>
<li><p><strong>Efficiency and Performance</strong>: A notable aspect of ENAS’s experimental results is the balance between efficiency and performance. The method demonstrated that it is possible to conduct a thorough search for neural network architectures without incurring the high computational costs associated with traditional NAS. This was evidenced by the fact that ENAS could discover competitive architectures using a single Nvidia GTX 1080Ti GPU in under 16 hours.</p>
</li>
<li><p><strong>Comparison with Other Methods</strong>: The paper also includes an ablation study and comparison with other approaches, such as DenseNet and NAS, to validate ENAS’s effectiveness. The results consistently show that ENAS can match or exceed the performance of other methods while using a fraction of the resources.</p>
</li>
<li><p><strong>Search Space Complexity</strong>: ENAS’s ability to handle complex search spaces was demonstrated by its application to both RNN cells and convolutional networks. The method’s flexibility in designing different types of neural networks highlights its potential for widespread use in various machine learning tasks.</p>
</li>
</ol>
<p>In conclusion, the experiments conducted in the paper provide strong evidence of ENAS’s capabilities. They not only validate the method’s efficiency in terms of computational resource usage but also confirm its ability to discover state-of-the-art neural network architectures that are competitive with those found by more expensive search methods. These results position ENAS as a promising approach for democratizing access to advanced neural architecture search and accelerating innovation in the field of machine learning.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
    <url>/2020/02/03/Fbnet/</url>
    <content><![CDATA[<p><a class="link" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.pdf">paper <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The rapid advancement of mobile devices has led to an increasing demand for efficient and accurate neural networks that can operate on limited computational resources. Convolutional Neural Networks (ConvNets) are the backbone of many computer vision applications, but their deployment on mobile devices is often hindered by their computational complexity and latency. Traditional neural architecture search (NAS) methods are computationally expensive and do not always translate to practical efficiency gains on actual hardware.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330161109207.png" alt="image-20240330161109207"></p>
<p>FBNet addresses this challenge by introducing a novel differentiable neural architecture search (DNAS) framework, which is hardware-aware and focuses on optimizing ConvNets for mobile devices. The key motivation behind this research is to bridge the gap between the theoretical efficiency of NAS methods and their practical application on real-world devices. By considering the hardware-specific characteristics, such as input resolution and processing power, FBNet aims to discover efficient ConvNet architectures that not only reduce the number of floating-point operations (FLOPs) but also improve latency, which is critical for mobile applications.</p>
<p>The paper proposes a layer-wise search space, where each layer of the network can choose from a set of predefined blocks, allowing for a more tailored architecture that adapts to the target device’s capabilities. The use of gradient-based optimization and the Gumbel Softmax technique enables the efficient exploration of this vast search space, leading to the discovery of FBNets that outperform existing state-of-the-art models in terms of accuracy and efficiency. This research represents a significant step forward in the design of mobile-friendly ConvNets and has important implications for the future of on-device machine learning applications.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>The paper presents several groundbreaking contributions to the field of mobile neural network design. Firstly, it introduces FBNet, a family of efficient ConvNets that are meticulously optimized for mobile devices using a hardware-aware approach. Through the Differentiable Neural Architecture Search (DNAS) framework, the authors are able to sidestep the computationally intensive nature of traditional NAS methods, significantly reducing the search cost while achieving superior performance.</p>
<p>A key contribution is the layer-wise search space that allows for individual optimization of each network layer, leading to architectures that are not only computationally efficient but also tailored to the specific constraints of mobile hardware. The use of the Gumbel Softmax technique enables gradient-based optimization of the architecture distribution, making the search process for optimal network configurations extremely fast and scalable.</p>
<p>Moreover, the paper proposes a latency-aware loss function that directly incorporates the actual latency on target devices into the training objective. This novel approach ensures that the discovered architectures are not only accurate but also operate with minimal delay, which is crucial for real-time mobile applications.</p>
<p>Lastly, the authors demonstrate the versatility of their approach by searching for and optimizing FBNets across different input resolutions and channel sizes, as well as different target devices, showcasing the adaptability and robustness of their method. The open-sourcing of FBNet models further enhances the paper’s contribution, making these state-of-the-art, efficient ConvNets accessible to the broader research community and promoting further advancements in mobile deep learning.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The methodology at the core of the FBNet paper revolves around the Differentiable Neural Architecture Search (DNAS) framework, which is designed to efficiently discover hardware-aware ConvNet architectures for mobile devices. The key innovation lies in the layer-wise search space and the use of gradient-based optimization techniques, which allow for a much faster and more targeted search process compared to traditional reinforcement learning-based NAS methods.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162531306.png" alt="image-20240330162531306"></p>
<p><strong>Layer-Wise Search Space:</strong> The authors propose a search space where each layer of the ConvNet can independently select a different building block or operator. This approach is in contrast to prior works that often reuse the same cell structure across all layers. The layer-wise search space is defined by a macro-architecture that specifies the number of layers and their input/output dimensions, while the type of block for each layer is determined through the search process. This flexibility enables the discovery of architectures that can adapt to the varying computational demands at different stages of the network.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162617949.png" alt="image-20240330162617949"></p>
<p><strong>Block Structures and Operators:</strong> The building blocks within the search space are inspired by successful architectures like MobileNetV2 and ShiftNet. They consist of a series of point-wise (1x1) convolutions, depthwise (KxK) convolutions with varying kernel sizes (K), and expansion ratios (e), which control the growth of the network’s channel dimensions. The blocks also incorporate group convolutions and channel shuffle operations to further enhance efficiency. A unique “skip” block is included, which allows the network to skip computation-heavy layers and effectively control depth.</p>
<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162627505.png" alt="image-20240330162627505" style="zoom:50%;">

<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162658077.png" alt="image-20240330162658077" style="zoom:50%;">

<p><strong>Latency-Aware Loss Function:</strong> A critical component of the methodology is the loss function used during the search and training process. The authors define a loss function that includes both cross-entropy loss for accuracy and a latency loss term that penalizes architectures based on their runtime on the target device. This latency is measured in microseconds and incorporated into the loss through a lookup table model, which estimates the overall latency by summing the latencies of individual operators. This model ensures that the latency term is differentiable, allowing for gradient-based optimizations.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162745106.png" alt="image-20240330162745106"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162755782.png" alt="image-20240330162755782"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330162806615.png" alt="image-20240330162806615"></p>
<p><strong>Gumbel Softmax and Gradient-Based Optimization:</strong> To efficiently explore the vast search space, the authors utilize the Gumbel Softmax distribution to relax the discrete sampling of architectures into a continuous relaxation. This technique enables the use of stochastic gradient descent (SGD) to optimize the architecture distribution parameters, turning the search for optimal architectures into a training problem. The Gumbel Softmax function, controlled by a temperature parameter, allows for the gradients to flow through the sampling process, effectively training the network to preferentially sample architectures that balance accuracy and latency.</p>
<p><strong>Efficient Super Net Training:</strong> The DNAS framework represents the search space as a “super net” with multiple parallel blocks at each layer. During training, the super net is optimized by updating the weights of each operator and the sampling probabilities for each block. The latter is achieved by updating the parameters of the Gumbel Softmax distribution. Once the training is complete, the optimal architectures are sampled from the learned distribution.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330163209255.png" alt="image-20240330163209255"></p>
<p><strong>Experimental Setup:</strong> The authors conduct a series of experiments using the ImageNet 2012 classification dataset, a standard benchmark for evaluating the performance of ConvNets. The search process begins with the construction of a latency lookup table for the target device, a Samsung Galaxy S8 with a Qualcomm Snapdragon 835 platform. This table is crucial for the latency-aware loss function, providing accurate measurements of operator runtimes.</p>
<p>The search space is defined by a macro-architecture with 22 layers, where each layer can choose from 9 candidate blocks, resulting in approximately 10^21 possible architectures. The DNAS framework is then used to train a stochastic super net, which contains parallel instances of these blocks at each layer. The training process involves optimizing the weights of each operator and the architecture distribution parameters using stochastic gradient descent (SGD) and the Adam optimizer.</p>
<p>To reduce training time and computational cost, the authors randomly select 100 classes from the 1000-class dataset and train the super net for 90 epochs. The input resolution is set to 224x224, and the training is performed with Caffe2’s int8 inference engine, which is optimized for mobile devices.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330163309240.png" alt="image-20240330163309240"></p>
<p><strong>Experimental Results:</strong> The results of the experiments are highly promising. FBNet-B, one of the models discovered by DNAS, achieves a top-1 accuracy of 74.1% on ImageNet with 295M FLOPs and a latency of 23.1 ms on a Samsung S8 phone. This represents a significant improvement over MobileNetV2-1.3, which has similar accuracy but requires 2.4x more FLOPs and 1.5x more latency. Furthermore, FBNet-B’s search cost is estimated to be 420x smaller than that of MnasNet, a state-of-the-art model designed using a different NAS approach.</p>
<p>The paper also presents results for different input resolutions and channel scalings, showing that FBNets can achieve 1.5% to 6.4% higher accuracy than MobileNetV2 under these conditions. The smallest FBNet model, optimized for a Samsung S8, achieves 50.2% accuracy with a latency of just 2.9 ms (345 frames per second), demonstrating the potential for real-time applications.</p>
<p>In addition, the authors perform searches targeting two different mobile devices, the Samsung Galaxy S8 and the iPhone X, and compare the performance of the resulting models. The results highlight the importance of device-specific optimizations, as the FBNet model optimized for the iPhone X shows a 1.4x speedup on that device compared to the Samsung-optimized model.</p>
<p>Overall, the experimental results validate the effectiveness of the DNAS framework in discovering efficient and accurate ConvNets for mobile devices. The significant improvements in both accuracy and latency, along with the low search costs, make FBNets a compelling choice for deploying computer vision models on mobile platforms. The paper’s findings pave the way for future research in efficient neural network design and its practical application in resource-constrained environments.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking</title>
    <url>/2021/09/03/FedMask/</url>
    <content><![CDATA[<p><a class="link" href="https://dl.acm.org/doi/pdf/10.1145/3485730.3485929">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181940888.png" alt="image-20240330181940888"></p>
<p>In the realm of mobile deep learning applications, the challenge of training sophisticated models on devices with limited data and computational resources is a significant hurdle. Traditional approaches that rely on centralizing data for model training often compromise privacy and consume substantial communication bandwidth. Federated Learning (FL) emerges as a promising solution, enabling decentralized training without sharing raw data, thus upholding privacy. However, non-IID data distribution across devices and limited communication bandwidth present major obstacles to practical FL deployment. Moreover, the computational efficiency of training and running Deep Neural Networks (DNNs) on mobile devices is critical due to their restricted resources. Addressing these challenges, this work introduces a novel framework that focuses on joint computation and communication efficiency in personalized FL. By leveraging heterogeneous binary masking, the framework allows each device to learn a tailored, sparse DNN model, enhancing inference accuracy while significantly reducing the communication and computation overheads. This innovation aims to unlock the potential of on-device deep learning applications, offering a more efficient and user-centric approach to FL.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330181950985.png" alt="image-20240330181950985"></p>
<p>This research pioneers a new federated learning paradigm that simultaneously enhances communication and computational efficiency while preserving personalization for mobile devices. The key contribution lies in the introduction of a novel framework that employs heterogeneous binary masking. This innovative approach allows for the learning of a personalized and structured sparse Deep Neural Network (DNN) on each device, which can operate efficiently despite the limited resources typically found on mobile devices.</p>
<p>By communicating only binary masks, rather than entire model parameters, the framework drastically reduces the communication burden. The masks, once learned, are optimized through a structured sparsity regularization method, which not only maintains the personalized aspects of each device’s local data but also leads to a compact model that demands less computational power during training and inference.</p>
<p>The framework’s ability to improve inference accuracy by a significant margin, coupled with its substantial reductions in both communication and computation costs, marks a significant advancement in the field of federated learning. It provides a unified solution that addresses the critical challenges of statistical heterogeneity, communication bandwidth limitations, and computational constraints, setting a new standard for deploying on-device deep learning applications efficiently and effectively.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182001287.png" alt="image-20240330182001287"></p>
<p>The methodology at the core of this research revolves around a novel federated learning (FL) framework that efficiently addresses the challenges of data heterogeneity, communication bandwidth constraints, and computational limitations on mobile devices. The framework’s unique approach is to leverage heterogeneous binary masking, which allows each device to learn a personalized and structured sparse Deep Neural Network (DNN).</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182059100.png" alt="image-20240330182059100"></p>
<p>The process begins with a one-shot pruning method that initializes a heterogeneous binary mask for each device, based on its local data. This mask determines the structure of the sparse DNN that will be learned by the device. During subsequent communication rounds, instead of transmitting the entire model update, each device only communicates the optimized binary mask to the central server. This mask is applied to a fixed set of model parameters, which remain unchanged throughout the learning process.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182011609.png" alt="image-20240330182011609"></p>
<p>The key innovation lies in the structured sparsity regularization applied to the binary mask optimization. This regularization encourages the learning of a sparse model that is not only personalized to the local data distribution but also computationally efficient. The regularization promotes channel-wise and filter-wise sparsity in convolutional layers and row-wise or column-wise sparsity in fully connected layers, resulting in a compact model that requires less computation during both training and inference.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182028594.png" alt="image-20240330182028594"></p>
<p>The aggregation strategy on the central server is carefully designed to preserve the personalized information embedded in the heterogeneous binary masks. The server only aggregates elements that are common across the binary masks from different devices, ensuring that the unique structures learned by each device are not lost.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182036543.png" alt="image-20240330182036543"></p>
<p>The framework’s approach to handling different types of layers, such as convolutional and fully connected layers, within the DNN is crucial for maintaining the model’s expressiveness and performance. For instance, the framework adapts the pruning method to the specific needs of layers like Long Short-Term Memory (LSTM) networks, which have a chain-like structure, to prevent significant performance drops.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182047779.png" alt="image-20240330182047779"></p>
<p>The experimental evaluation of the framework demonstrates its effectiveness in improving inference accuracy by up to 28.47% while reducing communication costs by up to 34.48× and computation costs by up to 2.44× compared to existing methods. Additionally, the framework achieves up to 1.56× inference speedup and reduces energy consumption by up to 1.78×, showcasing its potential for practical deployment in on-device deep learning applications.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182110813.png" alt="image-20240330182110813"></p>
<p>The experimental setup of this research is meticulously designed to evaluate the efficacy of the proposed federated learning framework across a variety of mobile AI applications. The experiments are conducted using three representative applications: image classification, human activity recognition, and next-character prediction. These applications are built upon non-IID datasets, reflecting the real-world scenario where data across devices can be diverse and unbalanced.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182120111.png" alt="image-20240330182120111"></p>
<p>The image classification application is developed using the VGG16 model on datasets such as EMNIST and CIFAR10, with devices holding a limited number of classes and samples. The human activity recognition application employs a 3-layer fully connected neural network, utilizing the HAR dataset which includes accelerometer and gyroscope data from 30 individuals. Lastly, the next-character prediction application is based on an RNN model constructed from an 8-D encoder, using dialogues from The Complete Works of William Shakespeare.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182131270.png" alt="image-20240330182131270"></p>
<p>The central server and mobile devices used in the experiments are equipped with hardware that reflects realistic mobile AI deployment scenarios, including NVIDIA Jetson TX2, Raspberry Pi 4, and an Intel Xeon <a class="link" href="mailto:E5-2630@2.6GHz">E5-2630@2.6GHz <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> server. Power consumption is measured using the Monsoon power monitor to accurately assess energy efficiency.</p>
<p>The experimental results are benchmarked against several state-of-the-art federated learning methods, including FedAvg, Top-k, and Per-FedAvg, among others. The proposed framework outperforms these baselines in terms of inference accuracy, communication cost, computation cost, memory footprint, inference latency, and energy consumption.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330182143940.png" alt="image-20240330182143940"></p>
<p>In terms of inference accuracy, the framework demonstrates a substantial improvement, with gains ranging from 1.09% to 28.47% across different applications. Communication cost is significantly reduced, with savings of up to 34.48×, a result of transmitting only binary masks instead of entire model parameters. Computation cost is also drastically decreased, with reductions of up to 2.44×, attributed to the structured sparsity of the learned models.</p>
<p>The memory footprint is substantially lower in models trained by the proposed framework, saving up to 32.2% compared to baseline models. This reduction is crucial for mobile devices with limited storage capacity. Inference speed is improved, with the framework achieving up to 1.56× speedup, making it more efficient for real-time applications. Lastly, energy consumption per inference is reduced by up to 1.78×, highlighting the framework’s energy efficiency.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
        <tag>FL</tag>
      </tags>
  </entry>
  <entry>
    <title>Fednas: Federated deep learning via neural architecture search</title>
    <url>/2021/09/28/FedNas/</url>
    <content><![CDATA[<p><a class="link" href="https://openreview.net/pdf?id=1OHZX4YDqhT">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Federated Learning (FL) has emerged as a promising solution for decentralized machine learning, addressing privacy and communication constraints by avoiding centralized data storage. However, FL faces significant challenges due to the non-identical and independent distribution (non-IID) of user data, which often leads to suboptimal performance of models trained using predefined architectures designed for centralized data. This data heterogeneity necessitates a more tailored approach to model development within the FL framework.</p>
<p>The paper introduces FedNAS, a novel approach to Federated Neural Architecture Search, aiming to automate and enhance model design for FL by collaboratively searching for optimal neural architectures among distributed workers. By personalizing not only the model weights but also the architecture itself, FedNAS seeks to overcome the limitations imposed by data heterogeneity and achieve higher accuracy than traditional methods.</p>
<p>The motivation behind FedNAS stems from the observation that existing FL algorithms, which focus on optimization strategies, often overlook the importance of model selection and design. By leveraging the power of Neural Architecture Search (NAS), FedNAS offers a complementary solution that adapts to the unique data distributions present at each FL user, leading to more efficient and effective learning models. This research explores the integration of NAS into the FL paradigm, presenting a new direction for advancing the state-of-the-art in decentralized machine learning.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>The paper presents several key contributions to the field of Federated Learning (FL) through the introduction of FedNAS, a novel framework for Federated Neural Architecture Search. The primary contributions are threefold:</p>
<ol>
<li><strong>Collaborative Model Personalization</strong>: FedNAS pioneers a method for collaboratively searching for both global and personalized model architectures among edge devices in FL settings. This approach allows for the co-optimization of model weights and architecture parameters, leading to models that are better suited to the specific data distributions of individual users.</li>
<li><strong>Addressing Data Heterogeneity</strong>: Recognizing the challenge of non-IID data in FL, FedNAS employs NAS to find architectures that can adapt to users’ unique data characteristics more effectively than existing local adaptation and personalization techniques. The experimental results demonstrate that FedNAS can achieve superior performance, showcasing its ability to handle data heterogeneity and improve model accuracy.</li>
<li><strong>State-of-the-Art Performance</strong>: FedNAS is shown to achieve state-of-the-art performance in both cross-silo and cross-device FL settings. The architecture searched by FedNAS outperforms manually predefined architectures and existing personalized FL methods, such as Ditto and perFedAvg, in terms of validation accuracy. This highlights the potential of FedNAS as a leading approach for model development in decentralized learning environments.</li>
</ol>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330170025813.png" alt="image-20240330170025813"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330170035786.png" alt="image-20240330170035786"></p>
<p>The methodology of the paper revolves around the Federated Neural Architecture Search (FedNAS) framework, which is designed to automate and optimize the model design process in a federated learning (FL) environment. The core of FedNAS lies in its ability to collaboratively search for neural architectures that are better suited for the non-IID (non-identical and independently distributed) data present at FL users. Here’s an in-depth look at the key components and steps involved in the FedNAS approach:</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330170133782.png" alt="image-20240330170133782"></p>
<p><strong>1. Problem Definition and Search Space:</strong> The paper begins by defining the FL setting, where there are multiple nodes (clients), each with its own non-IID dataset. The objective is to minimize the loss function across all clients by optimizing both the network weights and the neural architecture. The search space for FedNAS is inspired by the mixed-operation search space used in prior NAS works like DARTS and MiLeNAS, which allows for the exploration of shared convolutional cells that form the basis of the eventual model architecture.</p>
<p><strong>2. Local Search with MiLeNAS:</strong> Each client performs a local search using an improved variant of the gradient-based method, MiLeNAS, which is computationally efficient and suitable for resource-constrained edge devices. This local search involves optimizing the network weights and architecture parameters simultaneously using a mixed-level optimization technique, where the loss with respect to both training and validation data is considered.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330170149421.png" alt="image-20240330170149421"></p>
<p><strong>3. Federated Architecture Search (FedNAS):</strong> The FedNAS algorithm operates in a multi-step process. Initially, clients search locally for optimal architecture parameters (α) and weights (w). These are then sent to a central server, which aggregates the gradients from all clients to obtain a global set of α and w. These global parameters are subsequently synced back to the clients for further local searches. This iterative process continues until convergence, with the goal of finding a personalized architecture that fits the scattered data distributions effectively.</p>
<p><strong>4. Personalized FedNAS:</strong> In addition to global model search, FedNAS also supports personalization. After the initial search process, clients can further fine-tune the received global model locally by alternating the optimization of their local architecture and model weights. This local adaptation step enhances the model’s robustness against local data heterogeneity.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330170227810.png" alt="image-20240330170227810"></p>
<p><strong>5. System Efficiency and Experiments:</strong> The paper also addresses the efficiency of the FedNAS framework by comparing its search time and parameter size with those of FedAvg, a widely used FL algorithm. FedNAS is shown to find better architectures with fewer parameters in less time than FedAvg. Extensive experiments are conducted on non-IID datasets, including CIFAR-10 and GLD-23K, to evaluate the performance of FedNAS in both global model training and personalized model training scenarios. The results consistently demonstrate FedNAS’s superiority over other personalization methods and its ability to achieve state-of-the-art performance.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><strong>Experimental Setup:</strong></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330171026249.png" alt="image-20240330171026249"></p>
<ol>
<li><p><strong>Datasets and Non-IID Data Distributions:</strong> The experiments utilize the CIFAR-10 dataset, which consists of 60,000 32x32 color images across 10 classes, and the GLD-23K dataset, which contains 23,000 images. To simulate non-IID data, the training images are split among clients in an unbalanced manner, creating label-skewed and latent Dirichlet allocation (LDA) based distributions. This ensures that the models must learn to generalize well despite the variability in data across clients.</p>
</li>
<li><p><strong>Model and Algorithm Configuration:</strong> The experiments employ ResNet18 as the baseline architecture for comparison, which has 11 million parameters. FedNAS, on the other hand, uses an 8-layer DARTS cell structure with 4 million parameters. The local search algorithm, MiLeNAS, is utilized with a mixed-level optimization technique to efficiently search for optimal architectures and weights.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330171042700.png" alt="image-20240330171042700"></p>
</li>
<li><p><strong>Training and Evaluation:</strong> The experiments are conducted in both cross-silo and cross-device settings, with different numbers of clients and rounds. The server and clients communicate through a distributed training system built for FedNAS. The performance of the models is evaluated based on the average validation accuracy across all clients, and the results are compared with local adaptation via FedAvg, Ditto, and perFedAvg.</p>
</li>
</ol>
<p><strong>Experimental Results:</strong></p>
<ol>
<li><p><strong>Personalized Model Search:</strong> FedNAS demonstrates superior performance in the personalized model search experiments. It achieves an average validation accuracy of 91.3% for label skew distribution, which is 5% higher than local adaptation and 2% higher than the Ditto method. The results also show that FedNAS can outperform other methods in most client setups, with some improvements as high as 15% compared to perFedAvg.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330171257843.png"></p>
</li>
<li><p><strong>Global Model Search:</strong> When searching for a global model, FedNAS outperforms the FedAvg algorithm running on DenseNet. The architecture searched by FedNAS achieves a 4% higher test accuracy than FedAvg on the non-IID dataset, confirming the efficacy of FedNAS in finding models that are better suited for heterogeneous data.</p>
</li>
<li><p><strong>System Efficiency:</strong> The efficiency comparison reveals that FedNAS can find better architectures with fewer parameters in less time compared to FedAvg. The distributed version of FedNAS takes less than 5 hours to complete, while the single-process version takes 33 hours, highlighting the practicality of FedNAS for real-world deployment.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330171336454.png" alt="image-20240330171336454"></p>
</li>
<li><p><strong>Visualization and Analysis:</strong> The paper includes detailed visualizations of validation accuracies across clients, average validation accuracy distribution, and average validation accuracy improvement distribution. These visualizations provide a clear picture of FedNAS’s consistent performance advantage over other methods.</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
        <tag>FL</tag>
      </tags>
  </entry>
  <entry>
    <title>Mixture of Experts(MOEs): The Versatile Framework for Scalable AI Solutions</title>
    <url>/2023/03/28/MOE/</url>
    <content><![CDATA[<h2 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h2><p>Due to the recent surge in popularity of Mixture of Experts (MOE) in LLMs, this blog  was revised  on 2024-04-24.</p>
<p>2024-04-24: LLM relevant update</p>
<p>2023-03-28: Summary for Internship in Tencent AI Lab </p>
<p>TensorRT CUDA plugin source code: <a class="link" href="https://github.com/hjchen-thu/moe_simple">https://github.com/hjchen-thu/moe_simple <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>MoE has become so prevalent that it’s now challenging to find a new, large language model (LLM) that isn’t an MoE. Models such as GPT-4, Gemini 1.5, Mixtral 8x7B, and Jamba are all examples of MoE architectures.</p>
<p>The advent of this technology can be attributed to three main aspects:</p>
<h3 id="Sparsity-of-Neural-Networks"><a href="#Sparsity-of-Neural-Networks" class="headerlink" title="Sparsity of Neural Networks:"></a><strong>Sparsity of Neural Networks:</strong></h3><p>In specific layers, neural networks can become extremely sparse, meaning that the activation frequency of certain neurons is significantly lower than that of others. In other words, not all neurons are utilized every time, which is analogous to the neurons in the human brain.</p>
<p>Many are unaware that neural networks are often excessively large for the majority of the predictions they make.</p>
<p>Take, for example, the task of summarizing an article. The model’s trained parameters not only encompass the data for this capability but also encompass knowledge in physics, mathematics, astronomy, and more. This means that for every prediction, we run the entire network, yet only a small fraction of the model is actually utilized.</p>
<p>Take ChatGPT, for instance. Despite being compelled to run the entire vast network to predict each new word, this demands a significant computational workload. However, for the user’s current question, only a very specific part of the network is activated to assist in predicting the new word.</p>
<p>[1] highlighted the issue of sparse activation in the Transformer model’s Feed-Forward Networks (FFNs). In other words, for a single output, only a small subset of the FFN’s neurons are activated.</p>
<p>In the study, the authors found that after the application of activation functions like ReLU, a majority of the activation values are zero, meaning that only a minority of neurons have non-zero activations, leading to a highly sparse activation pattern in FFNs.</p>
<p>Furthermore, the larger the model, the greater its sparsity. Large models activate a smaller proportion of neurons relative to their total number when processing inputs. For example, in large models, 80% of inputs activate less than 3% of neurons. This phenomenon mirrors the sparse activation patterns observed in the human brain.</p>
<p>In practical use, even with large models, the parameters of their FFNs are not fully utilized, with most neurons corresponding to parameters that remain inactive in the majority of cases.</p>
<p>To address the issue of sparse activation, the paper proposed the MoEfication method, which involves segmenting the FFNs into multiple experts and constructing expert routers to determine which experts to use for each input. This approach enhances the efficiency and performance of the model.</p>
<h3 id="Expertise-of-Neurons"><a href="#Expertise-of-Neurons" class="headerlink" title="Expertise of Neurons:"></a><strong>Expertise of Neurons:</strong></h3><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/202404301926150.png"></p>
<p>The brain is an exceedingly complex organ, composed of various regions, each with its specific area of expertise. Below are examples of some brain regions and the domains of knowledge they excel in:</p>
<p><strong>Frontal Lobes</strong> - These are associated with decision-making, planning, problem-solving, personality, and social behavior. Individuals with damage to the frontal lobes may exhibit changes in personality and a decline in decision-making abilities.</p>
<p><strong>Parietal Lobes</strong> - Primarily responsible for processing sensory information, such as touch and spatial awareness. Damage to the parietal lobes can lead to an inability to accurately perceive the position or size of objects.</p>
<p><strong>Temporal Lobes</strong> - Involved in auditory processing, language comprehension, and memory formation. Damage to the temporal lobes may affect one’s language skills and memory.</p>
<p>For instance, consider a single neuron among the billions in a neural network that may be activated every time an input topic involves “apples,” and yet, this same neuron might also be activated when the input topic involves “telephones.”</p>
<p>One might wonder, what is the connection between the two?</p>
<p>This not only makes neural networks difficult to interpret but is also far from an ideal scenario. A single neuron is expected to be an expert in a variety of topics that are almost entirely unrelated to each other. Imagine being required to be an expert in both neuroscience and geology simultaneously; it would be a daunting task.</p>
<p>As a result, due to the vast scope of knowledge required, these neurons struggle to specialize. What’s worse, the learning curves may conflict with each other, where gaining more knowledge in one subject could impair the neuron’s ability to acquire knowledge in another.</p>
<p>Picture being an expert in mutually exclusive theories, such as materialism and idealism. A significant amount of information from one theory would contradict the other, potentially leading to a collapse in knowledge, rendering you essentially unable to discuss one of the theories coherently.</p>
<p>So, what if we could employ a technique to divide, eliminate, or at least reduce these two issues? This is the problem that the Mixture-of-Experts (MoE) aims to solve.</p>
<p>The MoE approach introduces a modular architecture where a large network is composed of smaller, specialized networks, or “experts,” each responsible for a specific domain of knowledge. By routing inputs to the appropriate expert, the MoE model can focus computational resources and learning efforts on relevant areas, thereby improving interpretability and efficiency.</p>
<h3 id="Limited-Computational-Resources"><a href="#Limited-Computational-Resources" class="headerlink" title="Limited Computational Resources:"></a><strong>Limited Computational Resources:</strong></h3><p>The scale of a model is one of the key factors in enhancing its performance. However, resources are invariably finite at any given stage. Under a constrained computational budget, training a larger model with fewer training steps often yields better results than training a smaller model with more steps.</p>
<h2 id="Workload"><a href="#Workload" class="headerlink" title="Workload"></a>Workload</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240430201035849.png" alt="image-20240430201035849"></p>
<p><strong>Sparse Mixture-of-Experts Layers:</strong> These layers are substitutes for the traditional Feed-Forward Networks (FFNs) found in Transformer models. An MoE layer consists of several “experts” (for example, eight experts), with each expert being an independent neural network. In practice, these experts are often FFNs, but they can also be more complex network structures or even MoE layers themselves, creating a hierarchical MoE structure.</p>
<p><strong>Gating Networks or Routing:</strong> This component is responsible for determining which tokens are sent to which expert. For instance, in the figure below, the token “More” might be directed to the second expert, while the token “Parameters” could be sent to the first expert. Sometimes, a single token can even be routed to multiple experts. The routing of tokens is a critical aspect of MoE utilization because the router is composed of learned parameters and is pre-trained alongside the rest of the network.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/Screenshot-2024-01-15-at-12.36.15-PM.png" alt="What is Mixture of Experts Approach of LLM Development ..."></p>
<h3 id="Slow-version"><a href="#Slow-version" class="headerlink" title="Slow version"></a>Slow version</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Expert</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Expert, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_dim, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MoE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, num_experts</span>):</span><br><span class="line">        <span class="built_in">super</span>(MoE, self).__init__()</span><br><span class="line">        self.experts = nn.ModuleList([Expert(input_dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_experts)])</span><br><span class="line">        <span class="comment"># gating的组成</span></span><br><span class="line">        self.gating = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, num_experts),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 各个expert做forward前向推理</span></span><br><span class="line">        expert_outputs = [expert(x) <span class="keyword">for</span> expert <span class="keyword">in</span> self.experts]</span><br><span class="line">        expert_outputs = torch.stack(expert_outputs, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 加权组合各个expert的输出</span></span><br><span class="line">        gating_weights = self.gating(x)</span><br><span class="line">        final_output = torch.<span class="built_in">sum</span>(expert_outputs * gating_weights.unsqueeze(<span class="number">2</span>), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure></div>



<h3 id="Fast-version-Sparse-to-Compact-GEMM"><a href="#Fast-version-Sparse-to-Compact-GEMM" class="headerlink" title="Fast version: Sparse to Compact GEMM"></a>Fast version: Sparse to Compact GEMM</h3><p>By packing tokens assigned to the same expert together, the originally sparse matrix multiplications can be transformed into dense matrix multiplications. </p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MixtralBLockSparseTop2MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ffn_dim = <span class="number">256</span></span><br><span class="line">        self.hidden_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.act_fn = nn.SiLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>): </span><br><span class="line">        y = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)</span><br><span class="line">        y = self.w2(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MixtralSparseMoeBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_dim = <span class="number">128</span></span><br><span class="line">        self.ffn_dim = <span class="number">256</span></span><br><span class="line">        self.num_experts = <span class="number">8</span> </span><br><span class="line">        self.top_k = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=<span class="literal">False</span>)</span><br><span class="line">        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP() \</span><br><span class="line">                                      <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_experts)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        bs, seql, dim = x.shape</span><br><span class="line">        x = x.view(-<span class="number">1</span>, dim)</span><br><span class="line">        </span><br><span class="line">        router_logits = self.gate(x)</span><br><span class="line">        routing_weights = F.softmax(router_logits, dim=<span class="number">1</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-<span class="number">1</span>)</span><br><span class="line">        routing_weights /= routing_weights.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        expert_mask = F.one_hot(selected_experts, num_classes=self.num_experts).permute(<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        final_hidden_states = torch.zeros((bs * seql, dim), dtype=x.dtype, device=x.device)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> expert_idx <span class="keyword">in</span> <span class="built_in">range</span>(self.num_experts):</span><br><span class="line">            expert_layer = self.experts[expert_idx]</span><br><span class="line">            </span><br><span class="line">            idx, top_x = torch.where(expert_mask[expert_idx])</span><br><span class="line">  </span><br><span class="line">            top_x_list = top_x.tolist()</span><br><span class="line">            idx_list = idx.tolist()</span><br><span class="line"></span><br><span class="line">            current_state = x[<span class="literal">None</span>, top_x_list].reshape(-<span class="number">1</span>, dim)</span><br><span class="line"></span><br><span class="line">            current_hidden_states = expert_layer(current_state)  \</span><br><span class="line">                                    * routing_weights[top_x_list, idx_list, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">            final_hidden_states.index_add_(<span class="number">0</span>, top_x, current_hidden_states.to(x.dtype))</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h2 id="Accelerating-Inference-A-simple-attempt"><a href="#Accelerating-Inference-A-simple-attempt" class="headerlink" title="Accelerating Inference (A simple attempt)"></a>Accelerating Inference (A simple attempt)</h2><p>This section serves as a summary of my internship at Tencent AI Lab, where my primary focus was on enhancing the inference speed of the Mixture of Experts (MoE) module within Conformer models.</p>
<p>I develop a TensorRT plugin to accelerate Mixture of Experts (MOEs) module by leveraging mixed quantization methods. Token reordering is employed to reduce unnecessary inference in expert networks. And in scenarios with fewer tokens, multi-streaming is also applied to further improve SM occupancy. As a result, we achieve a 25% end- to-end performance improvement on a single NVIDIA T4 with a negligible 0.01% word error rate (WER) increase.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240501110101766-20240501110457426.png"></p>
<h3 id="Main-code"><a href="#Main-code" class="headerlink" title="Main code"></a>Main code</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ComputeFmoeExpertInt8</span><span class="params">(<span class="type">const</span> T* input, <span class="type">const</span> <span class="type">int</span>* gate_idx, <span class="type">const</span> <span class="type">int</span> input_volume, <span class="type">const</span> <span class="type">int</span> weight1_volume,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> weight2_volume, <span class="type">const</span> <span class="type">int</span> S, <span class="type">const</span> <span class="type">int</span> num_expert, <span class="type">const</span> <span class="type">int</span> idim,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> hidden_units, <span class="type">const</span> T* w1_bias_ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">char</span>* weight1_int8, <span class="type">const</span> T* weight1_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> T* w2_bias_ptr, <span class="type">const</span> <span class="type">char</span>* weight2_int8, <span class="type">const</span> T* weight2_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                             std::vector&lt;<span class="type">int</span>&gt;&amp; v_acc_his, <span class="type">void</span>* workspace, T* output, cudaStream_t stream,</span></span></span><br><span class="line"><span class="params"><span class="function">                             std::shared_ptr&lt;CudaStreamManager&gt; csm_ptr)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> input_buffer_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(input_volume, kAlignment);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// caculate sizes</span></span><br><span class="line">  <span class="keyword">auto</span> mapping_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S, kAlignment);</span><br><span class="line">  <span class="keyword">auto</span> inverse_mapping_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S, kAlignment);</span><br><span class="line">  <span class="keyword">auto</span> his_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(num_expert + <span class="number">1</span>, kAlignment);</span><br><span class="line">  <span class="keyword">auto</span> Layer1_In_buffer_int8_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(input_volume, kAlignment);        <span class="comment">// Input reorder &amp; quantize</span></span><br><span class="line">  <span class="keyword">auto</span> Layer1_In_scale_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S, kAlignment);                         <span class="comment">// Layer1 input scale</span></span><br><span class="line">  <span class="keyword">auto</span> Layer1_Out_buffer_int32_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S * hidden_units, kAlignment);  <span class="comment">// MM out1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> Layer2_In_buffer_int8_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S * hidden_units, kAlignment);  <span class="comment">// Out1 scaling and silu &amp; quantize</span></span><br><span class="line">  <span class="keyword">auto</span> Layer2_In_scale_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(S, kAlignment);                       <span class="comment">// Layer2 input scale</span></span><br><span class="line">  <span class="keyword">auto</span> Layer2_Out_buffer_int32_size = <span class="built_in">alignTo</span>&lt;<span class="type">int</span>&gt;(input_volume, kAlignment);    <span class="comment">// MM out2</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// caculate address</span></span><br><span class="line">  <span class="type">int</span>* mapping = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>*&gt;(workspace);</span><br><span class="line">  <span class="type">int</span>* inverse_mapping = mapping + mapping_size;</span><br><span class="line">  <span class="type">int</span>* acc_histogram = inverse_mapping + inverse_mapping_size;</span><br><span class="line">  <span class="type">int</span>* Layer1_Out_buffer_int32 = acc_histogram + his_size;</span><br><span class="line">  <span class="type">int</span>* Layer2_Out_buffer_int32 = Layer1_Out_buffer_int32 + Layer1_Out_buffer_int32_size;</span><br><span class="line"></span><br><span class="line">  <span class="type">char</span>* Layer1_In_buffer_int8 = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span>*&gt;(Layer2_Out_buffer_int32 + Layer2_Out_buffer_int32_size);</span><br><span class="line">  <span class="type">char</span>* Layer2_In_buffer_int8 = Layer1_In_buffer_int8 + Layer1_In_buffer_int8_size;</span><br><span class="line"></span><br><span class="line">  T* Layer1_In_scale = <span class="built_in">reinterpret_cast</span>&lt;T*&gt;(Layer2_In_buffer_int8 + Layer2_In_buffer_int8_size);</span><br><span class="line">  T* Layer2_In_scale = Layer1_In_scale + Layer1_In_scale_size;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// step 1: Check all variables(for debug)</span></span><br><span class="line">  <span class="type">int</span> status = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// step 2: Compute reeordered idex: gate_idx -&gt; mapping &amp; acc_histogram</span></span><br><span class="line">  status = <span class="built_in">ComputeScatterMapping</span>(gate_idx, num_expert, S, mapping, inverse_mapping, acc_histogram, stream);</span><br><span class="line">  <span class="keyword">if</span> (status != <span class="number">0</span>) {</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">"compute_scatter_mapping error!"</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  <span class="comment">// step 3: mapping and quantize input</span></span><br><span class="line">  status = <span class="built_in">QuantizedScatterMappingCopy</span>(input, mapping, S, idim, Layer1_In_buffer_int8, Layer1_In_scale, stream);</span><br><span class="line">  <span class="keyword">if</span> (status != <span class="number">0</span>) {</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">"QuantizedScatterMappingCopy error!"</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  }</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// step 4: MOE caculation</span></span><br><span class="line">  <span class="type">int</span>* h_acc_his = v_acc_his.<span class="built_in">data</span>();</span><br><span class="line">  <span class="built_in">cudaMemcpyAsync</span>(h_acc_his, acc_histogram, <span class="built_in">sizeof</span>(<span class="type">int</span>) * (num_expert + <span class="number">1</span>), cudaMemcpyDeviceToHost, stream);</span><br><span class="line">  <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"></span><br><span class="line">  cublasOperation_t transa = CUBLAS_OP_N;</span><br><span class="line">  cublasOperation_t transb = CUBLAS_OP_T;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_expert; i++) {</span><br><span class="line">    <span class="keyword">auto</span> cur_stream = csm_ptr-&gt;<span class="built_in">Stream</span>(i);</span><br><span class="line">    <span class="keyword">auto</span> handle = csm_ptr-&gt;<span class="built_in">CublasHandle</span>(i);</span><br><span class="line">    <span class="type">int</span> m = h_acc_his[i + <span class="number">1</span>] - h_acc_his[i];</span><br><span class="line">    <span class="keyword">if</span> (m == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step 4.0: Prepare for workspace</span></span><br><span class="line">    <span class="keyword">auto</span> w_offset = i * idim * hidden_units;</span><br><span class="line">    <span class="keyword">auto</span> cur_weight1_int8_ptr = weight1_int8 + w_offset;</span><br><span class="line">    <span class="keyword">auto</span> cur_weight2_int8_ptr = weight2_int8 + w_offset;</span><br><span class="line">    <span class="keyword">auto</span> Layer1_In_buffer_int8_ptr = Layer1_In_buffer_int8 + h_acc_his[i] * idim;</span><br><span class="line">    <span class="keyword">auto</span> Layer1_Out_buffer_int32_ptr = Layer1_Out_buffer_int32 + h_acc_his[i] * hidden_units;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step 4.1: compute L1 out int32</span></span><br><span class="line">    <span class="built_in">CUBLAS_CHECK</span>(<span class="built_in">cublasGemm</span>(handle, transa, transb, m, hidden_units, idim, <span class="number">1</span>, Layer1_In_buffer_int8_ptr,</span><br><span class="line">                            cur_weight1_int8_ptr, <span class="number">0</span>, Layer1_Out_buffer_int32_ptr));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step 4.2: Dequantize + BiasSilu + Quantize</span></span><br><span class="line">    <span class="keyword">auto</span> Layer1_In_scale_ptr_expert = Layer1_In_scale + h_acc_his[i];</span><br><span class="line">    <span class="type">const</span> T* w1_weight_scale_ptr_expert = weight1_scale + i * hidden_units;</span><br><span class="line">    <span class="keyword">auto</span> w1_bias_ptr_expert = w1_bias_ptr + i * hidden_units;</span><br><span class="line">    <span class="keyword">auto</span> Layer2_In_buffer_int8_expert = Layer2_In_buffer_int8 + h_acc_his[i] * hidden_units;</span><br><span class="line">    <span class="keyword">auto</span> Layer2_In_scale_expert = Layer2_In_scale + h_acc_his[i];</span><br><span class="line"></span><br><span class="line">    status = <span class="built_in">DequantizedBiasSiluAndQuantize</span>(Layer1_Out_buffer_int32_ptr, Layer1_In_scale_ptr_expert,</span><br><span class="line">                                            w1_weight_scale_ptr_expert, w1_bias_ptr_expert, m, hidden_units,</span><br><span class="line">                                            Layer2_In_buffer_int8_expert, Layer2_In_scale_expert, cur_stream);</span><br><span class="line">    <span class="keyword">if</span> (status != <span class="number">0</span>) {</span><br><span class="line">      <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">"DequantizedBiasSilu error!"</span> &lt;&lt; endl;</span><br><span class="line">      <span class="keyword">return</span> status;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step 4.3: compute L2 out int32</span></span><br><span class="line">    <span class="keyword">auto</span> Layer2_Out_buffer_int32_ptr = Layer2_Out_buffer_int32 + h_acc_his[i] * idim;</span><br><span class="line">    <span class="type">const</span> T* w2_weight_scale_ptr_expert = weight2_scale + i * idim;</span><br><span class="line">    <span class="keyword">auto</span> w2_bias_ptr_expert = w2_bias_ptr + i * idim;</span><br><span class="line">    <span class="built_in">CUBLAS_CHECK</span>(<span class="built_in">cublasGemm</span>(handle, transa, transb, m, idim, hidden_units, <span class="number">1</span>, Layer2_In_buffer_int8_expert,</span><br><span class="line">                            cur_weight2_int8_ptr, <span class="number">0</span>, Layer2_Out_buffer_int32_ptr));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// step 4.4: Dequantize + Bias + ReverseMapping to Output</span></span><br><span class="line">    status =</span><br><span class="line">        <span class="built_in">DequantizedBiasGatherMapping</span>(Layer2_Out_buffer_int32_ptr, Layer2_In_scale_expert, w2_weight_scale_ptr_expert,</span><br><span class="line">                                     w2_bias_ptr_expert, m, idim, h_acc_his[i], inverse_mapping, output, cur_stream);</span><br><span class="line">    <span class="keyword">if</span> (status != <span class="number">0</span>) {</span><br><span class="line">      <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">"DequantizedBiasGatherMapping error!"</span> &lt;&lt; endl;</span><br><span class="line">      <span class="keyword">return</span> status;</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  csm_ptr-&gt;<span class="built_in">SyncAllStream</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="Step-By-Step"><a href="#Step-By-Step" class="headerlink" title="Step By Step"></a>Step By Step</h3><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240501123124961.png" alt="image-20240501123124961"></p>
<h4 id="1-Compute-reordered-index-gate-idx-mapping-acc-histogram"><a href="#1-Compute-reordered-index-gate-idx-mapping-acc-histogram" class="headerlink" title="1. Compute reordered index: gate_idx -> mapping & acc_histogram"></a>1. Compute reordered index: gate_idx -&gt; mapping &amp; acc_histogram</h4><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ScatterMappingKernel</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* gate_idx, <span class="type">const</span> <span class="type">int</span> num_expert, <span class="type">const</span> <span class="type">int</span> idx_num, <span class="type">int</span>* mapping,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">int</span>* inverse_mapping, <span class="type">int</span>* acc_histogram)</span> </span>{</span><br><span class="line">  <span class="type">int</span> idx = threadIdx.x;</span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">int</span> his[];</span><br><span class="line">  <span class="keyword">if</span> (idx &lt; num_expert + <span class="number">1</span>) his[idx] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = threadIdx.x; i &lt; idx_num; i += blockDim.x) {</span><br><span class="line">    <span class="comment">// calc his</span></span><br><span class="line">    <span class="keyword">auto</span> old = <span class="built_in">atomicAdd</span>(&amp;his[gate_idx[i] + <span class="number">1</span>], <span class="number">1</span>);</span><br><span class="line">    mapping[i] = old;</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// acc his</span></span><br><span class="line">  <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) {</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_expert; i++) his[i + <span class="number">1</span>] += his[i];</span><br><span class="line">  }</span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = threadIdx.x; i &lt; idx_num; i += blockDim.x) {</span><br><span class="line">    <span class="comment">// calc his</span></span><br><span class="line">    mapping[i] += his[gate_idx[i]];</span><br><span class="line">    inverse_mapping[mapping[i]] = i;</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (idx &lt; num_expert + <span class="number">1</span>) acc_histogram[idx] = his[idx];</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ComputeScatterMapping</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* gate_idx, <span class="type">const</span> <span class="type">int</span> num_expert, <span class="type">const</span> <span class="type">int</span> idx_num, <span class="type">int</span>* mapping,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span>* inverse_mapping, <span class="type">int</span>* acc_histogram, cudaStream_t stream)</span> </span>{</span><br><span class="line">  <span class="type">int</span> block_size = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (idx_num &lt; <span class="number">1024</span>)</span><br><span class="line">    block_size = <span class="number">256</span>;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (idx_num &lt; <span class="number">4096</span>)</span><br><span class="line">    block_size = <span class="number">512</span>;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    block_size = <span class="number">1024</span>;</span><br><span class="line"></span><br><span class="line">  ScatterMappingKernel&lt;&lt;&lt;<span class="number">1</span>, block_size, (num_expert + <span class="number">1</span>) * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream&gt;&gt;&gt;(</span><br><span class="line">      gate_idx, num_expert, idx_num, mapping, inverse_mapping, acc_histogram);</span><br><span class="line">  <span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaPeekAtLastError</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h4 id="2-Mapping-copy-and-quantize"><a href="#2-Mapping-copy-and-quantize" class="headerlink" title="2. Mapping copy and quantize"></a>2. Mapping copy and quantize</h4><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="type">int</span> TPB&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Trans2CharRowScatterMapping</span><span class="params">(<span class="type">const</span> T* fp_data, <span class="type">const</span> <span class="type">int</span>* mapping, <span class="type">const</span> <span class="type">int</span> idim, <span class="type">char</span>* char_data, T* scales_data)</span> </span>{</span><br><span class="line">  <span class="type">int</span> tidx = threadIdx.x;</span><br><span class="line"></span><br><span class="line">  fp_data += blockIdx.x * idim;</span><br><span class="line">  char_data += mapping[blockIdx.x] * idim;</span><br><span class="line">  </span><br><span class="line">  T s_max = (T)<span class="number">0.0</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = tidx; i &lt; idim; i += TPB) {</span><br><span class="line">    T tmp_abs = <span class="built_in">fabsf</span>(fp_data[i]);</span><br><span class="line">    <span class="keyword">if</span> (s_max &lt; tmp_abs) {</span><br><span class="line">      s_max = tmp_abs;</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  <span class="keyword">using</span> BlockReduce = cub::BlockReduce&lt;T, TPB&gt;;</span><br><span class="line">  __shared__ <span class="keyword">typename</span> BlockReduce::TempStorage temp_storage;</span><br><span class="line">  T dim_max =  <span class="built_in">BlockReduce</span>(temp_storage).<span class="built_in">Reduce</span>(s_max, cub::<span class="built_in">Max</span>());</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (tidx == <span class="number">0</span>) {</span><br><span class="line">    scales_data[mapping[blockIdx.x]] = <span class="number">255.0</span> / <span class="number">2.0</span> / dim_max;</span><br><span class="line">  }</span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// quantization</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = tidx; i &lt; idim; i += blockDim.x) {</span><br><span class="line">    <span class="type">bool</span> positive = (fp_data[i] &gt; (T)<span class="number">0.0</span>);</span><br><span class="line">    T tmp = fp_data[i] * scales_data[mapping[blockIdx.x]];</span><br><span class="line">    <span class="keyword">if</span> (tmp &lt;= <span class="number">-128</span>)</span><br><span class="line">      char_data[i] = <span class="number">-128</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (tmp &gt;= <span class="number">127</span>)</span><br><span class="line">      char_data[i] = <span class="number">127</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      char_data[i] = (<span class="type">signed</span> <span class="type">char</span>)(tmp + (positive ? <span class="number">1</span> : <span class="number">-1</span>) * <span class="number">0.5</span>);</span><br><span class="line">  }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">QuantizedScatterMappingCopy</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* input, <span class="type">const</span> <span class="type">int</span>* mapping, <span class="type">const</span> <span class="type">int</span> S, <span class="type">const</span> <span class="type">int</span> idim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">char</span>* input_buffer_int8, <span class="type">float</span>* scale, cudaStream_t stream)</span> </span>{</span><br><span class="line">  <span class="keyword">if</span> (input == <span class="literal">nullptr</span> || input_buffer_int8 == <span class="literal">nullptr</span> || scale == <span class="literal">nullptr</span> || mapping == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> block_1d_size = <span class="number">256</span>;</span><br><span class="line">  <span class="type">int</span> grid_1d_size = S;</span><br><span class="line"></span><br><span class="line">  Trans2CharRowScatterMapping&lt;<span class="type">float</span>, block_1d_size&gt;&lt;&lt;&lt;grid_1d_size, block_1d_size, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">      input, mapping, idim, input_buffer_int8, scale);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaPeekAtLastError</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240501122922605.png" alt="image-20240501122922605"></p>
<h5 id="3-1-First-Gemm"><a href="#3-1-First-Gemm" class="headerlink" title="3.1 First Gemm"></a>3.1 First Gemm</h5><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">CUBLAS_CHECK</span>(<span class="built_in">cublasGemm</span>(handle, transa, transb, m, hidden_units, idim, <span class="number">1</span>, Layer1_In_buffer_int8_ptr,</span><br><span class="line">                            cur_weight1_int8_ptr, <span class="number">0</span>, Layer1_Out_buffer_int32_ptr));</span><br></pre></td></tr></table></figure></div>

<h5 id="3-2-Dequantize-BiasSilu-Quantize"><a href="#3-2-Dequantize-BiasSilu-Quantize" class="headerlink" title="3.2 Dequantize + BiasSilu + Quantize"></a>3.2 Dequantize + BiasSilu + Quantize</h5><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="type">int</span> TPB&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">DequantizedBiasSiluAndQuantizeKernel</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* input_int32, <span class="type">const</span> T* input_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                     <span class="type">const</span> T* weight_scale, <span class="type">const</span> T* weight_bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                     <span class="type">const</span> <span class="type">int</span> hidden_dims, <span class="type">char</span>* char_data, T* scale)</span> </span>{</span><br><span class="line">  <span class="type">int</span> tidx = threadIdx.x;</span><br><span class="line">  T temp = <span class="number">0.0</span>;</span><br><span class="line">  T dequantized = <span class="number">0.0</span>;</span><br><span class="line">  <span class="keyword">if</span> (tidx &gt;= hidden_dims) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  input_int32 += blockIdx.x * hidden_dims;</span><br><span class="line">  char_data += blockIdx.x * hidden_dims;</span><br><span class="line"></span><br><span class="line">  T s_max = (T)<span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = tidx; i &lt; hidden_dims; i += blockDim.x) {</span><br><span class="line">    dequantized = (input_int32[i]) / input_scale[blockIdx.x] / weight_scale[i] + weight_bias[i];</span><br><span class="line">    temp = <span class="built_in">fabsf</span>(dequantized * <span class="built_in">sigmoid</span>(dequantized));</span><br><span class="line">    <span class="keyword">if</span> (s_max &lt; temp) {</span><br><span class="line">      s_max = temp;</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">using</span> BlockReduce = cub::BlockReduce&lt;T, TPB&gt;;</span><br><span class="line">  __shared__ <span class="keyword">typename</span> BlockReduce::TempStorage temp_storage;</span><br><span class="line">  T dim_max =  <span class="built_in">BlockReduce</span>(temp_storage).<span class="built_in">Reduce</span>(s_max, cub::<span class="built_in">Max</span>());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (tidx == <span class="number">0</span>) {</span><br><span class="line">    scale[blockIdx.x] = <span class="number">255.0</span> / <span class="number">2.0</span> / dim_max;</span><br><span class="line">  }</span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// quantization</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = tidx; i &lt; hidden_dims; i += blockDim.x) {</span><br><span class="line">    dequantized = input_int32[i] / input_scale[blockIdx.x] / weight_scale[i] + weight_bias[i];</span><br><span class="line">    temp = dequantized * <span class="built_in">sigmoid</span>(dequantized) * scale[blockIdx.x];</span><br><span class="line">    <span class="keyword">if</span> (temp &lt;= <span class="number">-128</span>)</span><br><span class="line">      char_data[i] = <span class="number">-128</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (temp &gt;= <span class="number">127</span>)</span><br><span class="line">      char_data[i] = <span class="number">127</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      char_data[i] = (<span class="type">signed</span> <span class="type">char</span>)(temp + (temp &gt; <span class="number">0</span> ? <span class="number">1</span> : <span class="number">-1</span>) * <span class="number">0.5f</span>);</span><br><span class="line">  }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">DequantizedBiasSiluAndQuantize</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* input_int32, <span class="type">const</span> <span class="type">float</span>* input_scale, <span class="type">const</span> <span class="type">float</span>* 			 weight_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">const</span> <span class="type">float</span>* weight_bias, <span class="type">const</span> <span class="type">int</span> m, <span class="type">const</span> <span class="type">int</span> hidden_dims,</span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">char</span>* Layer2_input_int8, <span class="type">float</span>* Layer2_scale, cudaStream_t stream)</span> </span>{</span><br><span class="line">  <span class="keyword">if</span> (input_int32 == <span class="literal">nullptr</span> || input_scale == <span class="literal">nullptr</span> || weight_scale == <span class="literal">nullptr</span> || weight_bias == <span class="literal">nullptr</span> ||</span><br><span class="line">      Layer2_input_int8 == <span class="literal">nullptr</span> || Layer2_scale == <span class="literal">nullptr</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> block_1d_size = <span class="number">256</span>;</span><br><span class="line">  <span class="type">int</span> grid_1d_size = m;</span><br><span class="line"></span><br><span class="line">  DequantizedBiasSiluAndQuantizeKernel&lt;<span class="type">float</span>, block_1d_size&gt;&lt;&lt;&lt;grid_1d_size, block_1d_size, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">      input_int32, input_scale, weight_scale, weight_bias, hidden_dims, Layer2_input_int8, Layer2_scale);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaPeekAtLastError</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h5 id="3-3-Second-Gemm"><a href="#3-3-Second-Gemm" class="headerlink" title="3.3 Second Gemm"></a>3.3 Second Gemm</h5><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">CUBLAS_CHECK</span>(<span class="built_in">cublasGemm</span>(handle, transa, transb, m, idim, hidden_units, <span class="number">1</span>, Layer2_In_buffer_int8_expert,</span><br><span class="line">                            cur_weight2_int8_ptr, <span class="number">0</span>, Layer2_Out_buffer_int32_ptr));</span><br></pre></td></tr></table></figure></div>

<h5 id="3-4-Dequantizatize-Bias-ReverseMapping-to-Output"><a href="#3-4-Dequantizatize-Bias-ReverseMapping-to-Output" class="headerlink" title="3.4 Dequantizatize + Bias + ReverseMapping to Output"></a>3.4 Dequantizatize + Bias + ReverseMapping to Output</h5><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">DequantizedBiasGatherMappingKernel</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* input_int32, <span class="type">const</span> T* input_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                   <span class="type">const</span> T* weight_scale, <span class="type">const</span> T* weight_bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                   <span class="type">const</span> <span class="type">int</span>* inverse_mapping, <span class="type">const</span> <span class="type">int</span> idim, <span class="type">const</span> <span class="type">int</span> acc_idex,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                   <span class="type">const</span> <span class="type">int</span> numel, T* output)</span> </span>{</span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &gt;= numel) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> s = idx / idim;</span><br><span class="line">  <span class="type">int</span> i = idx % idim;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> inverse_mapping_idx = inverse_mapping[s + acc_idex];</span><br><span class="line"></span><br><span class="line">  output[inverse_mapping_idx * idim + i] =</span><br><span class="line">  (input_int32[idx] / weight_scale[i]) / input_scale[s] + weight_bias[i];  </span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">DequantizedBiasGatherMapping</span><span class="params">(<span class="type">const</span> <span class="type">int</span>* input_int32, <span class="type">const</span> <span class="type">float</span>* input_scale, <span class="type">const</span> <span class="type">float</span>* weight_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">const</span> <span class="type">float</span>* weight_bias, <span class="type">const</span> <span class="type">int</span> m, <span class="type">const</span> <span class="type">int</span> idim, <span class="type">const</span> <span class="type">int</span> acc_idex,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">const</span> <span class="type">int</span>* inverse_mapping, <span class="type">float</span>* output, cudaStream_t stream)</span> </span>{</span><br><span class="line">  <span class="keyword">auto</span> numel = m * idim;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> block_size = <span class="number">256</span>;</span><br><span class="line">  <span class="type">int</span> grid_size = (numel + block_size - <span class="number">1</span>) / block_size;</span><br><span class="line"></span><br><span class="line">  DequantizedBiasGatherMappingKernel&lt;&lt;&lt;grid_size, block_size, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">      input_int32, input_scale, weight_scale, weight_bias, inverse_mapping, idim, acc_idex, numel, output);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaPeekAtLastError</span>());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>The limitation of the current scenario lies in the fact that it does not take into account the ultra-large-scale matrix multiplication of Large Language Models (LLMs). Therefore, the multi-stream acceleration within a single card cannot meet the acceleration requirements of LLMs. In addition, in the current top-k expert selection, k is only taken as 1. For cases where k is greater than 1, the computational scale and data transfer become more complex, necessitating a more refined consideration.</p>
<h2 id="MOE-parallel"><a href="#MOE-parallel" class="headerlink" title="MOE parallel"></a>MOE parallel</h2><p>To do</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a class="link" href="https://arxiv.org/pdf/2110.01786">https://arxiv.org/pdf/2110.01786 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><strong>Diverse Ensemble Evolution: Curriculum Data-Model Marriage</strong>, NeurIPS’18</li>
<li><strong>Diversity and Depth in Per-Example Routing Models</strong>, ICLR’21</li>
<li><strong>Adaptive mixtures of local experts, Neural Computation’1991</strong></li>
<li><strong>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, ICLR’17</strong></li>
</ol>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>Quantization</tag>
        <tag>MOE</tag>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
    <url>/2020/02/13/SingeNas/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1904.02877.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330173945735.png" alt="image-20240330173945735"></p>
<p>The rapid evolution of mobile technology has led to an increasing demand for sophisticated applications capable of advanced tasks such as image classification, while maintaining optimal performance and efficiency. However, the deployment of complex machine learning models like Convolutional Neural Networks (ConvNets) on mobile devices is often hindered by the limited computational resources and strict latency requirements of these platforms. This has sparked a need for hardware-efficient ConvNets that can deliver high accuracy without compromising on speed.</p>
<p>Neural Architecture Search (NAS) has emerged as a powerful tool for automating the design of ConvNets, offering the potential to explore a vast design space to find optimal network architectures. Despite its promise, traditional NAS methods are often computationally expensive, requiring significant GPU hours to search through the multitude of possible architectures, making it impractical for widespread use.</p>
<p>Recognizing these challenges, the research community has been driven to develop more efficient NAS methods that can significantly reduce the search time. The goal is to design ConvNets that not only meet the accuracy and latency criteria for mobile devices but also minimize the computational overhead of the search process. By doing so, researchers aim to democratize the use of NAS, enabling a broader range of applications and promoting further innovation in mobile artificial intelligence. This pursuit has led to the exploration of novel NAS techniques that leverage differentiable search methods, aiming to achieve a balance between efficiency and performance, and ultimately, to unlock the full potential of mobile AI.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>This research makes several pivotal contributions to the field of efficient Neural Architecture Search (NAS) for mobile devices. Firstly, it introduces a novel single-path search space that dramatically reduces the number of trainable parameters compared to traditional multi-path NAS methods. This innovation allows for a more streamlined and efficient search process, which is crucial for deploying ConvNets on hardware with limited resources.</p>
<p>Secondly, the method achieves state-of-the-art accuracy on the ImageNet dataset while maintaining a latency suitable for mobile devices. This accomplishment demonstrates that the proposed approach can effectively balance the trade-off between performance and efficiency, a common challenge in mobile AI applications.</p>
<p>Thirdly, the research significantly improves the efficiency of the NAS process, reducing the search cost by up to 5,000 times compared to prior work. By completing the search in merely eight epochs, the method makes NAS more practical and accessible, potentially enabling broader adoption in the development of mobile applications.</p>
<p>Lastly, the authors prioritize reproducibility by open-sourcing the entire codebase, which not only allows others to verify the results but also encourages further research and development in the area of hardware-aware NAS. This transparent approach fosters a collaborative environment and paves the way for future advancements in the field. Overall, these contributions represent a significant leap towards realizing the full potential of mobile AI through the efficient design of ConvNets.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330174155827.png" alt="image-20240330174155827"></p>
<p>The methodology introduced in this research is a novel approach to Neural Architecture Search (NAS) that focuses on designing hardware-efficient Convolutional Neural Networks (ConvNets) suitable for mobile devices. The core innovation is the Single-Path NAS, a differentiable NAS method that significantly reduces the search time while maintaining high accuracy and adherence to latency constraints.</p>
<p>At the heart of Single-Path NAS is the concept of a single-path search space, which contrasts with previous methods that often employed multi-path supernets. Instead of exploring numerous distinct architectures, Single-Path NAS uses a single, over-parameterized ConvNet where all architectural decisions are encoded with shared convolutional kernel parameters. This paradigm shift reduces the number of trainable parameters, leading to a substantial decrease in search cost.</p>
<p>The method operates by searching over an over-parameterized “superkernel” in each ConvNet layer. This superkernel contains the weights for all candidate convolutional operations, allowing the search to be framed as identifying the appropriate subset of kernel weights to use. By sharing the convolutional kernel weights across different architectural options, the method avoids the need to maintain separate paths for each candidate operation, which is a common practice in multi-path NAS methods.</p>
<p>A critical component of Single-Path NAS is the use of a hardware-aware objective function that incorporates an accurate inference latency model. This model predicts the runtime of the ConvNets on the target mobile platform, allowing the search to optimize for both accuracy and efficiency. The objective function is designed to be differentiable, ensuring that gradients can be computed efficiently during the search process.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330175957878.png" alt="image-20240330175957878"></p>
<p>The Single-Path NAS methodology leverages a one-shot NAS formulation, where the search is conducted over a single, compact neural network rather than a bi-level optimization problem. This formulation eliminates the need for separate gradient steps between the ConvNet weights and the NAS parameters, simplifying the training process and further reducing the search time.</p>
<p>The research also introduces a reproducibility aspect by open-sourcing the entire codebase, enabling other researchers and practitioners to reproduce the results and build upon the methodology. This openness fosters collaboration and accelerates the advancement of NAS techniques for mobile devices.</p>
<p>In summary, the Single-Path NAS method represents a significant leap forward in the field of NAS, particularly for mobile applications. By streamlining the search process and reducing the computational demands, it makes the design of hardware-efficient ConvNets more accessible and practical. The method’s focus on accuracy, efficiency, and reproducibility sets a new standard for future research in this domain.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330180022085.png" alt="image-20240330180022085"></p>
<p>The experimental setup of this research is meticulously designed to evaluate the efficacy of the proposed method in the context of mobile device deployment. The primary dataset utilized is the ImageNet dataset, a large-scale collection of images widely recognized for benchmarking image classification models. The target platform for the ConvNets designed through the method is the Pixel 1 smartphone, representative of real-world mobile devices.</p>
<p>The method’s implementation leverages TensorFlow and TPUs (Tensor Processing Units), which are specialized hardware accelerators for machine learning workloads. All models are deployed using TensorFlow TFLite, ensuring compatibility with the mobile platform. Runtime profiling is conducted using the Facebook AI Performance Evaluation Platform (FAI-PEP), which provides detailed per-layer runtime breakdowns, essential for latency-sensitive applications.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330180835451.png" alt="image-20240330180835451"></p>
<p>The experimental results are nothing short of impressive. The proposed method achieves a top-1 accuracy of 74.96% on ImageNet, with an inference latency of just 79ms on the Pixel 1 phone. This performance is not only state-of-the-art among hardware-efficient NAS methods but also maintains a target latency of 80ms, as per the experimental constraints.</p>
<p>In terms of search efficiency, the method requires only eight epochs to reach its peak performance, which translates to a mere 3.75 hours of wall-clock time on a TPUv2. This represents a staggering improvement over previous NAS methods, which could take weeks of computational time. Specifically, the method is up to 5,000 times faster in search cost compared to existing approaches, making it a groundbreaking advancement in the realm of NAS.</p>
<p>The research also includes an ablation study to understand the impact of kernel-weight subsets on the accuracy and efficiency trade-off. The findings suggest that training over subsets of kernel weights can effectively capture similar accuracy-runtime trade-offs as individually trained kernels, highlighting the flexibility and adaptability of the proposed method.</p>
<p>Furthermore, the method’s runtime model is validated for accuracy, with an average prediction error of just 1.76%, indicating its reliability in estimating the performance of the designed ConvNets.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Transferable Architectures forScalable Image Recognition</title>
    <url>/2020/01/12/Nasnet/</url>
    <content><![CDATA[<p><a class="link" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In the realm of deep learning, designing efficient and effective neural network architectures for image classification has been a challenging task that often requires extensive manual tuning and expertise. Traditional approaches to architecture engineering are not only time-consuming but also limited by human creativity and intuition. With the increasing size and complexity of datasets like ImageNet, the need for automated architecture design becomes more pressing to harness the full potential of available data.</p>
<p>The motivation behind this research is to address the computational constraints and manual effort involved in architecture engineering by introducing an automated approach to learning transferable neural network architectures. The goal is to develop a method that can discover efficient convolutional building blocks from a smaller dataset and transfer them to larger datasets, such as ImageNet, with minimal modifications. This transferability is crucial as it allows the learned architectures to adapt and perform well across various image recognition tasks without extensive retraining or redesign.</p>
<p>The research leverages the concept of Neural Architecture Search (NAS) to explore a vast space of possible architectures and identify those that generalize well beyond the dataset they were discovered on. By focusing on the design of a novel search space and a new regularization technique, the study aims to achieve state-of-the-art accuracy while reducing computational demands, thus making neural networks more accessible and efficient for a broader range of applications. The ultimate aim is to push the boundaries of what is possible in image classification by automating the process of architecture design and demonstrating the potential for significant performance improvements over human-engineered models.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190801124.png" alt="image-20240330190801124"></p>
<p>This research makes several significant contributions to the field of neural network architecture design. Firstly, it introduces a novel search space, known as the “NASNet search space,” which is specifically designed to enable transferability of learned architectures across different datasets and computational scales. This space is crafted to ensure that the complexity of the architecture is independent of the network’s depth and input image size, allowing for scalability and adaptability.</p>
<p>Secondly, the paper presents a method for learning convolutional cells, or “building blocks,” on a small dataset and transferring them to larger datasets, such as ImageNet. This approach significantly accelerates the search process and demonstrates that architectures discovered on smaller datasets can achieve state-of-the-art accuracy when transferred to larger ones, without extensive re-engineering.</p>
<p>Additionally, the research introduces a new regularization technique, “ScheduledDropPath,” which improves the generalization capabilities of the models. This technique involves dropping out paths in the network with a probability that increases linearly over the training period, leading to better performance and regularization compared to existing methods.</p>
<p>Finally, the study shows that the learned features from the image classification models are generically useful and can be transferred to other computer vision tasks, such as object detection. By combining the learned features with existing frameworks like Faster-RCNN, the research achieves state-of-the-art results in object detection tasks, further highlighting the versatility and utility of the proposed methods. Overall, these contributions pave the way for more efficient and effective neural network design, with potential applications across various domains of artificial intelligence.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The methodology at the core of this research revolves around the concept of Neural Architecture Search (NAS), which is used to automate the process of discovering efficient and effective convolutional neural network (CNN) architectures. The process is divided into several key steps that culminate in the development of a transferable and scalable CNN architecture.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190822005.png" alt="image-20240330190822005"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190839919.png" alt="image-20240330190839919"></p>
<p><strong>1. Designing the NASNet Search Space:</strong><br>The initial step involves the creation of a novel search space, termed the NASNet search space. This space is designed to be flexible and scalable, allowing the complexity of the architecture to be decoupled from the network’s depth and the size of the input images. The search space is composed of convolutional layers, referred to as “cells,” which have identical structures but different weights. The goal is to find the optimal cell structure that can be stacked together to form a complete network, with each cell having its own set of parameters.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190829414.png" alt="image-20240330190829414"></p>
<p><strong>2. Searching for Optimal Cells on a Small Dataset:</strong><br>The next phase is the search for the best convolutional cell on a smaller dataset, such as CIFAR-10. This is done using a controller recurrent neural network (RNN) that samples child networks with different architectures. These child networks are trained to convergence on the validation set, and their accuracies are used to update the controller. The controller RNN is optimized using policy gradient methods, which adjust the probabilities of sampling certain architectures over time to favor those that perform well.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190851048.png" alt="image-20240330190851048"></p>
<p><strong>3. Transferring the Learned Architecture to Larger Datasets:</strong><br>Once the best cell is identified on the small dataset, it is transferred to a larger dataset like ImageNet. This transfer is made possible by the design of the NASNet search space, which ensures that the complexity of the architecture is independent of the input image size. The best cell is simply stacked multiple times, with each copy having its own parameters, to create a deep convolutional architecture suitable for ImageNet.</p>
<p><strong>4. Introduction of ScheduledDropPath Regularization:</strong><br>To improve the generalization of the models, the paper introduces ScheduledDropPath, a regularization technique that involves stochastically dropping out paths within the cells during training. Unlike traditional dropout methods, ScheduledDropPath increases the probability of dropping out a path linearly over the course of training. This dynamic scheduling helps the model to learn more robust features that are less likely to overfit to the training data.</p>
<p><strong>5. Training and Evaluation:</strong><br>The final step involves training the complete architecture on the target dataset using standard practices such as data augmentation, cosine learning rate scheduling, and optimization techniques like SGD with momentum. The model is evaluated on the validation set to determine its accuracy and compared to existing state-of-the-art models.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p>The experimental setup in this research is meticulously designed to validate the effectiveness of the proposed method and to benchmark its performance against existing state-of-the-art models. The experiments are conducted in a structured manner, ensuring a comprehensive evaluation of the newly introduced NASNet architecture.</p>
<p><strong>Experimental Setup:</strong> The initial phase of the experiments involves searching for the optimal convolutional cell on the CIFAR-10 dataset, a smaller yet challenging image classification dataset. This search is performed using a controller RNN with a one-layer LSTM configuration, which is responsible for sampling child networks with varying architectures. The child networks are trained using standard training procedures, and their performance on a validation set is utilized to update the controller RNN. The search process is optimized using Proximal Policy Optimization (PPO), a reinforcement learning algorithm that encourages exploration and expedites convergence.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190917473.png" alt="image-20240330190917473"></p>
<p>To ensure the robustness of the discovered architectures, the experiments incorporate ScheduledDropPath, a novel regularization technique. This method stochastically drops out paths in the convolutional cells during training, with the dropout probability increasing linearly over time. The overall architecture is then evaluated on the CIFAR-10 test set to identify the top-performing cells.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190934116.png" alt="image-20240330190934116"></p>
<p><strong>Transfer to Larger Datasets:</strong> Once the best convolutional cell is identified on CIFAR-10, the researchers transfer this architecture to the larger ImageNet dataset. This transfer is facilitated by the scalable nature of the NASNet search space, which allows the convolutional cell to be stacked multiple times to form a deep network suitable for ImageNet. The transferred architecture is trained from scratch on ImageNet using similar procedures as before, with the addition of data augmentation techniques to further enhance generalization.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190942986.png" alt="image-20240330190942986"></p>
<p><strong>Experimental Results:</strong> The experimental results are highly promising and demonstrate the effectiveness of the proposed method. On the CIFAR-10 dataset, the NASNet architecture achieves a state-of-the-art error rate of 2.4%, showcasing its ability to generalize from a smaller dataset to a more complex one.</p>
<p>The results on ImageNet are equally impressive. Despite not being searched directly on ImageNet, the NASNet architecture, when transferred, achieves a top-1 accuracy of 82.7% and a top-5 accuracy of 96.2%, outperforming human-engineered architectures. This performance is achieved while using 9 billion fewer FLOPS, representing a 28% reduction in computational demand compared to the previous state-of-the-art model.</p>
<p>Furthermore, the NASNet architecture exhibits excellent scalability. By varying the number of convolutional cells and filters, the researchers create a family of NASNet models that outperform equivalent human-designed models at various computational budgets. Even a smaller version of NASNet achieves a top-1 accuracy of 74% on ImageNet, which is 3.1% better than similarly-sized state-of-the-art models.</p>
<p>In addition to image classification, the learned features from NASNet are shown to be generically useful and transferable to other computer vision tasks. When combined with the Faster-RCNN framework for object detection, NASNet surpasses state-of-the-art results by a significant 4.0%, achieving a mean Average Precision (mAP) of 43.1% on the COCO dataset.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>BlockQNN: Efficient Block-wise Neural Network Architecture Generation</title>
    <url>/2020/01/30/blockqnn/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1808.05584.pdf">https://arxiv.org/pdf/1808.05584.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In the realm of computer vision, Convolutional Neural Networks (CNNs) have achieved remarkable success, with hand-crafted architectures like AlexNet, VGG, Inception, and ResNet significantly advancing the state-of-the-art. However, the design of these networks relies heavily on expert knowledge and is often a labor-intensive process. The exponential growth of network configurations due to the increasing depth and complexity has made manual design not only challenging but also infeasible, as it involves a tremendous search space and computational costs.</p>
<p>The paper “BlockQNN: Efficient Block-wise Neural Network Architecture Generation” addresses this challenge by proposing an automated approach to network architecture design. The authors introduce BlockQNN, a method that leverages the Q-Learning paradigm with an epsilon-greedy exploration strategy to efficiently generate high-performance network blocks. These blocks are then stacked to construct the entire network, which not only reduces the search space but also accelerates the generation process.</p>
<p>The motivation behind BlockQNN is to democratize the design of neural network architectures by eliminating the need for extensive manual tuning and expertise. By automating the process, BlockQNN aims to make it accessible for a broader range of researchers and practitioners to develop state-of-the-art models, even with limited computational resources. The paper highlights the potential of automated network design to discover novel architectures that are both efficient and effective, paving the way for future advancements in deep learning.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330152346495.png" alt="image-20240330152346495"></p>
<p>The paper makes several significant contributions to the field of automated neural network design. Firstly, it introduces a novel block-wise network generation approach that constructs high-performance networks by sequentially selecting component layers through a learning agent trained with Q-Learning. This method yields state-of-the-art results, achieving a 2.35% top-1 error rate on the CIFAR-10 dataset, and competitive accuracy on the larger-scale ImageNet dataset.</p>
<p>Secondly, BlockQNN dramatically reduces the search space and computational costs associated with network design. The paper demonstrates that a high-performing network can be generated with only 32 GPUs in 3 days, which is a significant improvement over previous methods that required hundreds of GPUs and weeks of training time.</p>
<p>Additionally, the paper presents a distributed asynchronous framework and an early stop strategy to further accelerate the generation process. This allows for efficient search with fast convergence, making the method more practical for researchers with limited resources.</p>
<p>Another key contribution is the introduction of a reward function that correlates the accuracy of early-stopped networks with their final performance, enabling the selection of good blocks in reduced training time.</p>
<p>Lastly, the paper explores the transferability of the generated networks, showing that a network optimized for CIFAR can be transferred to ImageNet with minimal modifications and still achieve outstanding performance. This highlights the generalizability of BlockQNN and its potential for application across various datasets and tasks. Overall, BlockQNN represents a significant step forward in making neural network architecture generation more accessible and efficient.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The BlockQNN framework is built upon the concept of generating networks in a block-wise fashion. Instead of designing each layer individually, the authors propose constructing networks by stacking optimized blocks, which are generated by the learning agent through a process known as epsilon-greedy exploration. This strategy starts with a high probability of exploration (epsilon) and gradually reduces it to favor exploitation of known, high-performing block structures.</p>
<p>To further enhance the efficiency of the network generation process, the authors introduce a distributed asynchronous framework. This setup allows for parallel training of multiple network blocks across different compute nodes, significantly speeding up the search for optimal block structures.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330154805440.png" alt="image-20240330154805440"></p>
<p>An early stopping strategy is also implemented to curtail training once a certain performance threshold is met, saving computational resources and time. This strategy is underpinned by a carefully designed reward function that positively correlates the early-stopped network’s accuracy with its final performance, ensuring that the agent can identify effective blocks even when training is halted prematurely.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330154818103.png" alt="image-20240330154818103"></p>
<p>Additionally, the paper explores the use of layer embedding and recurrent networks, specifically Long Short-Term Memory (LSTM) units, to aggregate information from individual layers into an overall network representation. This representation is then fed into a Multi-Layer Perceptron (MLP) to predict network performance before extensive training, further optimizing the search process.</p>
<p>The authors also experiment with different block connection styles, finding that simply stacking blocks sequentially may not be optimal. They propose an automated method for determining the best way to connect blocks, which can enhance network performance by combining features across multiple resolutions.</p>
<p>Overall, the methods employed in the BlockQNN paper represent a significant advancement in the field of neural architecture generation, offering a more efficient and effective approach to designing high-performing neural networks.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330155122058.png" alt="image-20240330155122058"></p>
<p>One of the primary achievements of BlockQNN is its ability to generate networks that rival the performance of hand-crafted architectures. On the CIFAR-10 dataset, the best network generated by BlockQNN achieved a top-1 error rate of 2.35%, which is on par with, if not better than, several state-of-the-art hand-crafted networks. This demonstrates that the block-wise generation strategy and the learning agent’s ability to select optimal components layers are highly effective.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330155132787.png" alt="image-20240330155132787"></p>
<p>The efficiency of BlockQNN was also a key focus, with the authors reporting that the entire process of network generation, from random exploration to exploitation, could be completed in just 3 days using 32 GPUs. This is a significant reduction in time and resources compared to other methods such as Neural Architecture Search (NAS), which can take weeks and hundreds of GPUs.</p>
<p>The transferability of the networks generated by BlockQNN was another aspect explored in the experiments. The authors transferred the best block structure found on the CIFAR-10 dataset to the more challenging ImageNet dataset. Despite the increase in complexity and the difference in input image sizes, the transferred network still achieved competitive accuracy, with a top-1 error rate of 18.00% and a top-5 error rate of 4.58% when trained on 224x224 images and tested on 320x320 images. This underscores the generalization capabilities of the block structures discovered by BlockQNN.</p>
<p>The paper also presents results from the Faster BlockQNN approach, which aims to predict network performance before training. This method was able to achieve comparable results with only 1 GPU in 20 hours on the CIFAR-10 dataset, making it an extremely cost-effective solution for researchers with limited computational resources.</p>
<p>In addition to these benchmarks, the authors analyzed the evolutionary process of the auto-generated blocks, observing a transition from simpler structures during the exploration phase to more complex, multi-branch structures in the exploitation phase. This suggests that the learning agent is effectively learning to construct more sophisticated network blocks as it gains more experience.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>[CUDA]element-wise</title>
    <url>/2021/07/06/cuda_elementwise/</url>
    <content><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<h2 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h2><p>Element-wise operations represent the most fundamental and straightforward category of kernel functions. Their computational characteristics align seamlessly with the working style of GPUs: they perform individual arithmetic operations on each element and then output the results directly. Despite their simplicity, many operators in the field of deep learning fall into this category. Common examples include Add, Mul, Concat, various activation functions like Sigmoid and ReLU along with their variants, and normalization techniques such as BatchNorm.</p>
<h2 id="Sum"><a href="#Sum" class="headerlink" title="Sum"></a>Sum</h2><h3 id="Naive"><a href="#Naive" class="headerlink" title="Naive"></a>Naive</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">__global__ void elementwise_add(<span class="built_in">float</span>* a, <span class="built_in">float</span>* b, <span class="built_in">float</span>* c, <span class="built_in">int</span> N) {</span><br><span class="line">  <span class="built_in">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (idx &lt; N) c[idx] = a[idx] + b[idx];</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="Vectorize-LD-ST"><a href="#Vectorize-LD-ST" class="headerlink" title="Vectorize LD/ST"></a>Vectorize LD/ST</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">// ElementWise Add + float4</span><br><span class="line">// 不要这么写，会影响occupancy</span><br><span class="line">// elementwise_add_float4&lt;&lt;&lt;CeilDiv(N, block_size), block_size/<span class="number">4</span>&gt;&gt;&gt;(d_A, d_B, d_C, N)</span><br><span class="line">// 应该在grid的维度上除以<span class="number">4</span></span><br><span class="line">// elementwise_add_float4&lt;&lt;&lt;CeilDiv(N/<span class="number">4</span>, block_size), block_size&gt;&gt;&gt;(d_A, d_B, d_C, N)</span><br><span class="line">// a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)</span><br><span class="line">//float4向量化访存只对数据规模大的时候有加速效果，数据规模小的时候没有加速效果</span><br><span class="line">//<span class="comment">#define FLOAT4(value)  *(float4*)(&amp;(value))</span></span><br><span class="line">//<span class="comment">#define FLOAT4(value) (reinterpret_cast&lt;float4*&gt;(&amp;(value))[0])</span></span><br><span class="line">__global__ void elementwise_add_float4(<span class="built_in">float</span>* a, <span class="built_in">float</span>* b, <span class="built_in">float</span> *c, <span class="built_in">int</span> N)</span><br><span class="line">{</span><br><span class="line">    <span class="built_in">int</span> idx = (blockDim.x * blockIdx.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(idx &lt; N ){</span><br><span class="line">        float4 tmp_a = FLOAT4(a[idx]);</span><br><span class="line">        float4 tmp_b = FLOAT4(b[idx]);</span><br><span class="line">        float4 tmp_c;</span><br><span class="line">        tmp_c.x = tmp_a.x + tmp_b.x;</span><br><span class="line">        tmp_c.y = tmp_a.y + tmp_b.y;</span><br><span class="line">        tmp_c.z = tmp_a.z + tmp_b.z;</span><br><span class="line">        tmp_c.w = tmp_a.w + tmp_b.w;</span><br><span class="line">        FLOAT4(c[idx]) = tmp_c;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<p>Here, it’s important to note that if vectorized memory access with float4 is used, the number of threads needs to be reduced to a quarter of the original. Whether the block size or the grid size should be reduced to a quarter depends on the actual data scale and should be determined through empirical testing.</p>
<p>Notes:</p>
<ol>
<li>When using cuda events to measure elapsed time, run 1000 iterations and take the average. When running with nsight compute, run it only once.</li>
<li>If the data significantly deviates from theoretical expectations, consider whether other people/processes are also using the GPU, which could lead to inaccurate timing measurements.</li>
<li>For all experiments below, set the block size to 128.</li>
</ol>
<p>For a data scale of N=32×1024×1024 <em>N</em>=32×1024×1024, the following experimental data is obtained (data may fluctuate; if the discrepancy is particularly large, rerun ncu):</p>
<p>For N = 32 * 1024  * 1024</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240331184335404.png" alt="image-20240331184335404"></p>
<p>For N = 1* 1024</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240331184407625.png" alt="image-20240331184407625"></p>
<p>It can be observed that although all three can calculate the correct results, the approach of vectorizing memory access with float4 by reducing the block size to a quarter of the original for data scales such as <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="20.612ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 9110.4 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(3443.8,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(4166,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1500,0)"></path></g><g data-mml-node="mo" transform="translate(6388.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(7110.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1500,0)"></path></g></g></g></svg></mjx-container> actually takes more time (or the time difference is not significant compared to the version without vectorized memory access). Where lies the issue?</p>
<p>Phenomenon:</p>
<ol>
<li>For data scales like <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="20.612ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 9110.4 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(3443.8,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(4166,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1500,0)"></path></g><g data-mml-node="mo" transform="translate(6388.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(7110.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1500,0)"></path></g></g></g></svg></mjx-container>, after using float4, the theoretical and measured Occupancy of elementwise_add_f4_grid is very close to that of elementwise_add without vectorized memory access, but its Memory usage rate has increased.</li>
<li>For relatively smaller data scales such as <mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="12.819ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 5666 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2221.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2943.8,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mn" transform="translate(3666,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000,0)"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(1500,0)"></path></g></g></g></svg></mjx-container>, the GPU’s Compute and Memory usage rates are both very low (latency bound), making the difference between using vectorized memory access or not negligible.</li>
</ol>
<p>Conclusion: In the case of small data scales, the optimization method of vectorized memory access does not need to be considered. For large data scales, it is advisable to consider using vectorized memory access to avoid affecting Occupancy. The same approach is taken in the following official blog:</p>
<p><a class="link" href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">CUDA Pro Tip: Increase Performance with Vectorized Memory Access | NVIDIA Technical Blog <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>In situations where the performance is not limited by latency, the principle behind the performance enhancement of float4 vectorized memory access is as follows:</p>
<p>GPUs do not have features like AVX-512 in CPUs, which can process multiple data elements with a single instruction. The performance improvement of float4 mainly lies in the reduction of memory access instructions (for the same data scale, it previously required four instructions, but now only a quarter of the instructions are needed). This allows more instructions to be stored in the instruction cache, thereby increasing the hit rate of the instruction cache.</p>
<p>To determine whether vectorized memory access has been utilized, one should look at the generated SASS code to see if there are instructions like LDG.E.128 Rx, [Rx.64] or STG.E.128 [R6.64], Rx present. The existence of these instructions indicates successful vectorization; their absence indicates a failure in vectorization.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240331190852561.png" alt="image-20240331190852561"></p>
<h2 id="Other-element-wise-kernel"><a href="#Other-element-wise-kernel" class="headerlink" title="Other element-wise kernel:"></a>Other element-wise kernel:</h2><h3 id="Sigmoid-float4"><a href="#Sigmoid-float4" class="headerlink" title="Sigmoid + float4"></a>Sigmoid + float4</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">// Sigmoid x: N, y: N y=<span class="number">1</span>/(<span class="number">1</span>+exp(-x)) float4</span><br><span class="line">//nvcc -o sigmoid sigmoid.cu &amp;&amp; ./sigmoid</span><br><span class="line">//sigmoid_float4&lt;&lt;&lt;CeilDiv(N/<span class="number">4</span>,block_size), block_size&gt;&gt;&gt;(d_A, d_B, N)</span><br><span class="line">__global__ void sigmoid_vec4(float4* x, float4* y, <span class="built_in">int</span> N) {</span><br><span class="line">  <span class="built_in">int</span> idx = (blockIdx.x * blockDim.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line">  <span class="keyword">if</span> (idx &lt; N) {</span><br><span class="line">    float4 tmp_x = FLOAT4(x[idx]);</span><br><span class="line">    float4 tmp_y;</span><br><span class="line">    tmp_y.x = <span class="number">1.0</span>f / (<span class="number">1.0</span>f + expf(-tmp_x.x));</span><br><span class="line">    tmp_y.y = <span class="number">1.0</span>f / (<span class="number">1.0</span>f + expf(-tmp_x.y));</span><br><span class="line">    tmp_y.z = <span class="number">1.0</span>f / (<span class="number">1.0</span>f + expf(-tmp_x.z));</span><br><span class="line">    tmp_y.w = <span class="number">1.0</span>f / (<span class="number">1.0</span>f + expf(-tmp_x.w));</span><br><span class="line">    FLOAT4(y[idx]) = tmp_y;</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="Relu-float4"><a href="#Relu-float4" class="headerlink" title="Relu + float4"></a>Relu + float4</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line"> // Relu x: N, y: N y=<span class="built_in">max</span>(<span class="number">0</span>,x) float4</span><br><span class="line">// relu_float4&lt;&lt;&lt;CeilDiv(N/<span class="number">4</span>, block_size), block_size&gt;&gt;&gt;(d_A, d_B, N);</span><br><span class="line">__global__ void relu_float4(<span class="built_in">float</span>* x, <span class="built_in">float</span>* y, <span class="built_in">int</span> N) {</span><br><span class="line">  <span class="built_in">int</span> idx = (blockIdx.x * blockDim.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line">  <span class="keyword">if</span> (idx &lt; N) {</span><br><span class="line">    float4 tmp_x = FLOAT4(x[idx]);</span><br><span class="line">    float4 tmp_y;</span><br><span class="line">    tmp_y.x = fmaxf(<span class="number">0.0</span>f, tmp_x.x);</span><br><span class="line">    tmp_y.y = fmaxf(<span class="number">0.0</span>f, tmp_x.y);</span><br><span class="line">    tmp_y.z = fmaxf(<span class="number">0.0</span>f, tmp_x.z);</span><br><span class="line">    tmp_y.w = fmaxf(<span class="number">0.0</span>f, tmp_x.w);</span><br><span class="line">    FLOAT4(y[idx]) = tmp_y;</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="Histogram-float4"><a href="#Histogram-float4" class="headerlink" title="Histogram + float4"></a>Histogram + float4</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">// Histogram + float4</span><br><span class="line">// x: Nx1, y: count histogram</span><br><span class="line">//nvcc -o histogram histogram.cu &amp;&amp; ./histogram</span><br><span class="line">//sudo /usr/local/cuda-<span class="number">11.7</span>/<span class="built_in">bin</span>/ncu --<span class="built_in">set</span> full -f -o histogram ./histogram</span><br><span class="line">//histogram_float4&lt;&lt;&lt;CeilDiv(N/<span class="number">4</span>, block_size), block_size&gt;&gt;&gt;(d_A, d_B, N);</span><br><span class="line">__global__ void histogram_float4(<span class="built_in">int</span>* x, <span class="built_in">int</span>* y, <span class="built_in">int</span> N) {</span><br><span class="line">  <span class="built_in">int</span> idx = <span class="number">4</span> * (blockIdx.x * blockDim.x + threadIdx.x);</span><br><span class="line">  <span class="keyword">if</span> (idx &lt; N) {</span><br><span class="line">    int4 tmp_y = INT4(x[idx]);</span><br><span class="line">    atomicAdd(&amp;(y[tmp_y.x]), <span class="number">1</span>);</span><br><span class="line">    atomicAdd(&amp;(y[tmp_y.y]), <span class="number">1</span>);</span><br><span class="line">    atomicAdd(&amp;(y[tmp_y.z]), <span class="number">1</span>);</span><br><span class="line">    atomicAdd(&amp;(y[tmp_y.w]), <span class="number">1</span>);</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>Work hard</category>
        <category>Note</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>[CUDA]reduce</title>
    <url>/2021/07/20/cuda_reduce/</url>
    <content><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://zhuanlan.zhihu.com/p/426978026">https://zhuanlan.zhihu.com/p/426978026 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>Reduction problems are a common category of questions encountered in interviews. Compared to elementwise problems, where each thread independently performs tasks, reduction problems become more complex due to the involvement of synchronization and communication between threads. Additionally, these problems can leverage the previously introduced optimization techniques for vectorized memory access, which adds a layer of complexity in terms of thought process.<br>The <code>reductions.cu</code> file primarily implements the following three kernel functions for reduction summation:</p>
<ol>
<li>The most basic <code>atomic_reduction()</code>, which extensively utilizes the <code>atomicAdd()</code> function.</li>
<li>The sweep style reduction sum <code>reduction_a()</code>, which calls <code>atomicAdd</code> once at the end.</li>
<li>The warp shuffle reduction <code>reduction_ws()</code>, which also calls <code>atomicAdd</code> once at the end.</li>
</ol>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240331211607423.png" alt="image-20240331211607423"></p>
<p>Theory: The <code>atomic_reduction()</code> is the slowest because it heavily relies on the <code>atomicAdd()</code> function. Compared to <code>reduction_a</code>, <code>reduction_ws</code> tends to perform better due to its minimal use of shared memory.</p>
<p>Observed Phenomena:</p>
<ol>
<li>As the data size increases, the performance advantage of <code>reduce_ws</code> over <code>reduction_a</code> diminishes.</li>
<li>The use of the <code>atomicAdd</code> function affects the Memory utilization rate, and this impact does not vary with the scale of the data.</li>
</ol>
<p>Regarding the first point: Both <code>reduce_ws</code> and <code>reduction_a</code> require loading data within a grid-stride loop initially. As the data size grows, the time taken to load the data becomes the dominant factor in the overall time consumption. At this juncture, the specific method of performing reduction matters less, hence the time difference narrows with increasing data sizes. In theory, warp shuffle should offer better performance than the classic reduction sum, but in practice, it still depends on the specific data scale of the problem at hand and requires a detailed analysis.</p>
<p>Regarding the second point: The impact of the <code>atomicAdd</code> function on performance is significant because it can reduce the Memory utilization rate to a very low level, and this is difficult to improve by simply changing the data scale.</p>
<h2 id="Usefull-basic-reduction-functions"><a href="#Usefull-basic-reduction-functions" class="headerlink" title="Usefull basic reduction functions"></a>Usefull basic reduction functions</h2><p>Here is a collection of some fundamental reduction kernel functions:</p>
<h3 id="warp-reduce-sum"><a href="#warp-reduce-sum" class="headerlink" title="warp reduce sum"></a>warp reduce sum</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//__shfl_down_sync performs a tree-reduction to compute the sum of the val variable held by </span></span><br><span class="line"><span class="comment">//each thread in a warp. At the end of the loop, val of the first thread in the warp </span></span><br><span class="line"><span class="comment">//contains the sum</span></span><br><span class="line"><span class="comment">//https://developer.nvidia.com/blog/using-cuda-warp-level-primitives</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">float</span> <span class="title">warp_reduce_sum</span><span class="params">(<span class="type">float</span> val)</span> </span>{</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">32</span>)val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>); <span class="comment">// 0-16, 1-17, 2-18, etc.</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">16</span>)val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">8</span>);<span class="comment">// 0-8, 1-9, 2-10, etc.</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">8</span>)val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">4</span>);<span class="comment">// 0-4, 1-5, 2-6, etc.</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">4</span>)val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">2</span>);<span class="comment">// 0-2, 1-3, 4-6, 5-7, etc.</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">2</span>)val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">1</span>);<span class="comment">// 0-1, 2-3, 4-5, etc.</span></span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">}</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15</span></span><br><span class="line"><span class="comment">16 17 18 19 20 21 22 23 24 25 26  27  28  29  30  31</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></div>

<h3 id="warp-reduce-max"><a href="#warp-reduce-max" class="headerlink" title="warp reduce max"></a>warp reduce max</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">template &lt;unsigned int blockSize&gt;</span><br><span class="line">__device__ __forceinline__ float warp_reduce_max(float val) {</span><br><span class="line">    if (blockSize &gt;= 32)val = fmaxf(val, __shfl_down_sync(0xffffffff, val, 16)); // 0-16, 1-17, 2-18, etc.</span><br><span class="line">    if (blockSize &gt;= 16)val = fmaxf(val, __shfl_down_sync(0xffffffff, val, 8));// 0-8, 1-9, 2-10, etc.</span><br><span class="line">    if (blockSize &gt;= 8)val = fmaxf(val, __shfl_down_sync(0xffffffff, val, 4));// 0-4, 1-5, 2-6, etc.</span><br><span class="line">    if (blockSize &gt;= 4)val = fmaxf(val, __shfl_down_sync(0xffffffff, val, 2));// 0-2, 1-3, 4-6, 5-7, etc.</span><br><span class="line">    if (blockSize &gt;= 2)val = fmaxf(val, __shfl_down_sync(0xffffffff, val, 1));// 0-1, 2-3, 4-5, etc.</span><br><span class="line">    return val;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="block-reduce-sum"><a href="#block-reduce-sum" class="headerlink" title="block reduce sum"></a>block reduce sum</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//在一个block里面求和，先每个warp内部求和，结果保存在每个warp第一个线程的寄存器val里</span></span><br><span class="line"><span class="comment">//然后将val暂存在共享内存sdata里</span></span><br><span class="line"><span class="comment">//最后在第一个warp里，把val从共享内存里再读取出来，对warp之间再求和</span></span><br><span class="line"><span class="function">__device__  <span class="type">float</span> <span class="title">block_reduce_sum</span><span class="params">(<span class="type">float</span> val)</span> </span>{</span><br><span class="line">    <span class="comment">// 一个block里最多32个warps，因为最多1024个线程</span></span><br><span class="line">    __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line">    <span class="type">int</span> lane = threadIdx.x % warpSize;<span class="comment">//0~31</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//每个warp内部求和</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">          val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val;<span class="comment">//每个warp里第一个线程的寄存器保存这个warp内部求和的结果</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>){</span><br><span class="line">        val = (lane &lt; blockDim.x / warpSize) ? sdata[lane] : <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">            val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="block-reduce-max"><a href="#block-reduce-max" class="headerlink" title="block reduce max"></a>block reduce max</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">block_reduce_max</span><span class="params">(<span class="type">float</span> val)</span> </span>{</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];</span><br><span class="line">    <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line">    <span class="type">int</span> lane = threadIdx.x % warpSize;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">        val = <span class="built_in">fmaxf</span>(val, __shfl_xor_sync(<span class="number">0xffffffff</span>, val, offset));</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>){</span><br><span class="line">        val = (lane &lt; blockDim.x / warpSize) ? sdata[lane] : -FLT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">            val = <span class="built_in">fmaxf</span>(val, __shfl_xor_sync(<span class="number">0xffffffff</span>, val, offset));<span class="comment">//求最大值的时候必须得是__shfl_xor_sync</span></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> val;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="All-reduce-sum"><a href="#All-reduce-sum" class="headerlink" title="All reduce sum"></a>All reduce sum</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Block All Reduce Sum</span></span><br><span class="line"><span class="comment">// grid(N/256), block(256)</span></span><br><span class="line"><span class="comment">// input: Nx1, output=sum(input)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce_ws</span><span class="params">(<span class="type">float</span> *gdata, <span class="type">float</span> *out)</span></span>{</span><br><span class="line">     __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];<span class="comment">//最多就32个warp，因为线程数最多是1024</span></span><br><span class="line">     <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">     <span class="type">int</span> idx = threadIdx.x+blockDim.x*blockIdx.x;<span class="comment">//&lt;&lt;&lt;640,256&gt;&gt;&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="type">float</span> val = <span class="number">0.0f</span>;<span class="comment">//这是每个线程独有的本地变量</span></span><br><span class="line">     <span class="type">unsigned</span> mask = <span class="number">0xFFFFFFFF</span>U;</span><br><span class="line"></span><br><span class="line">     <span class="type">int</span> lane = threadIdx.x % warpSize;<span class="comment">//0~31</span></span><br><span class="line">     <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">while</span> (idx &lt; N) {  <span class="comment">// grid stride loop to load </span></span><br><span class="line">        val += gdata[idx];</span><br><span class="line">        idx += gridDim.x*blockDim.x;  </span><br><span class="line">      }</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 1st warp-shuffle reduction</span></span><br><span class="line">   <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) </span><br><span class="line">       val += __shfl_xor_sync(mask, val, offset);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val; <span class="comment">//warp内部求和, 每个线程都把warp内部求和的结果放在自己本地的val变量上，但是只有lane==0的val被保存进shared_memory[warpID]里</span></span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (warpID == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line"> <span class="comment">// reload val from shared mem if warp existed</span></span><br><span class="line">       val = (tid &lt; blockDim.x/warpSize)?sdata[lane]:<span class="number">0</span>;<span class="comment">//为什么用lane而不是warpID去索引？ 因为warpID是0才会进来</span></span><br><span class="line">       <span class="comment">//tid 小于8，那么lane也是0~7之间，可以用来索引sdata</span></span><br><span class="line"> <span class="comment">// final warp-shuffle reduction</span></span><br><span class="line">       <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">          val += __shfl_down_sync(mask, val, offset);</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span>(tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(out, val);<span class="comment">//将所有block内部求和的结果再一次汇总</span></span><br><span class="line">     }</span><br><span class="line">  }</span><br></pre></td></tr></table></figure></div>



<h3 id="All-reduce-sum-float4"><a href="#All-reduce-sum-float4" class="headerlink" title="All reduce sum float4"></a>All reduce sum float4</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Block All Reduce Sum + float4</span></span><br><span class="line"><span class="comment">// grid(N/256/4), block(256)</span></span><br><span class="line"><span class="comment">// input: Nx1, output=sum(input)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> warpSize 32</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="type">const</span> <span class="type">int</span> block_size </span>= <span class="number">256</span>/<span class="number">4</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce_ws_float4</span><span class="params">(<span class="type">float</span> *gdata, <span class="type">float</span> *out)</span></span>{</span><br><span class="line"></span><br><span class="line">   __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];<span class="comment">//最多就32个warp，因为线程数最多是1024</span></span><br><span class="line">   <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">   <span class="type">int</span> idx = (blockIdx.x * blockDim.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">   <span class="type">float</span> val = <span class="number">0.0f</span>;<span class="comment">//这是每个线程独有的本地变量,不用担心读写同步的问题</span></span><br><span class="line">   <span class="type">unsigned</span> mask = <span class="number">0xFFFFFFFF</span>U;</span><br><span class="line">   <span class="type">int</span> lane = threadIdx.x % warpSize;<span class="comment">//0~31</span></span><br><span class="line">   <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">while</span> (idx &lt; N) {  <span class="comment">// grid stride loop to load </span></span><br><span class="line">      float4 tmp_input = <span class="built_in">FLOAT4</span>(gdata[idx]); </span><br><span class="line">      val += tmp_input.x;</span><br><span class="line">      val += tmp_input.y;</span><br><span class="line">      val += tmp_input.z;</span><br><span class="line">      val += tmp_input.w;</span><br><span class="line">      idx += (gridDim.x*blockDim.x) * <span class="number">4</span>;  </span><br><span class="line">   }</span><br><span class="line">   <span class="comment">// 1st warp-shuffle reduction</span></span><br><span class="line">   <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">      val += __shfl_xor_sync(mask, val, offset);</span><br><span class="line">       </span><br><span class="line">   <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val; <span class="comment">//warp内部求和, 每个线程都把warp内部求和的结果放在自己本地的val变量上，但是只有lane==0的val被保存进shared_memory[warpID]里</span></span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span>(warpID == <span class="number">0</span>){</span><br><span class="line">      val = (tid &lt; blockDim.x/warpSize) ? sdata[lane] : <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)  </span><br><span class="line">          val += __shfl_xor_sync(mask, val, offset);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(out, val);</span><br><span class="line">   }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h2 id="Complex-scenario"><a href="#Complex-scenario" class="headerlink" title="Complex scenario"></a>Complex scenario</h2><h3 id="dot-product"><a href="#dot-product" class="headerlink" title="dot product"></a>dot product</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Dot Product</span></span><br><span class="line"><span class="comment">// grid(N/256), block(256)</span></span><br><span class="line"><span class="comment">// a: Nx1, b: Nx1, out=sum(elementwise_mul(a,b))</span></span><br><span class="line"><span class="comment">// dot_product&lt;&lt;&lt;CeilDiv(N, block_size), block_size&gt;&gt;&gt;(d_A, d_B, d_C, N);</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">dot_product</span><span class="params">(<span class="type">float</span>* a, <span class="type">float</span>* b, <span class="type">float</span>* c, <span class="type">int</span> N)</span></span>{</span><br><span class="line">    </span><br><span class="line">    __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + tid;</span><br><span class="line">  </span><br><span class="line">    <span class="type">float</span> val = <span class="number">0.0f</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line">    <span class="type">int</span> lane   = threadIdx.x % warpSize; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(idx &lt; N)</span><br><span class="line">    {</span><br><span class="line">        val += a[idx] * b[idx]; </span><br><span class="line">        idx += gridDim.x * blockDim.x;</span><br><span class="line">    }</span><br><span class="line">    __syncthreads();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) {</span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);<span class="comment">//__shfl_xor_sync(0xffffffff, val,offset);</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">      val = (tid &lt; blockDim.x / warpSize) ? sdata[lane] : <span class="number">0.0f</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);<span class="comment">//__shfl_xor_sync(0xffffffff, val,offset);</span></span><br><span class="line">      <span class="keyword">if</span>(tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(c, val);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="dot-product-float4"><a href="#dot-product-float4" class="headerlink" title="dot product float4"></a>dot product float4</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Dot Product</span></span><br><span class="line"><span class="comment">// grid(N/256), block(256)</span></span><br><span class="line"><span class="comment">// a: Nx1, b: Nx1, out=sum(elementwise_mul(a,b))</span></span><br><span class="line"><span class="comment">// dot_product&lt;&lt;&lt;CeilDiv(N, block_size), block_size&gt;&gt;&gt;(d_A, d_B, d_C, N);</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">dot_product</span><span class="params">(<span class="type">float</span>* a, <span class="type">float</span>* b, <span class="type">float</span>* c, <span class="type">int</span> N)</span></span>{</span><br><span class="line">    </span><br><span class="line">    __shared__ <span class="type">float</span> sdata[<span class="number">32</span>];</span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + tid;</span><br><span class="line">  </span><br><span class="line">    <span class="type">float</span> val = <span class="number">0.0f</span>;</span><br><span class="line">  </span><br><span class="line">    <span class="type">int</span> warpID = threadIdx.x / warpSize;</span><br><span class="line">    <span class="type">int</span> lane   = threadIdx.x % warpSize; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(idx &lt; N)</span><br><span class="line">    {</span><br><span class="line">        val += a[idx] * b[idx]; </span><br><span class="line">        idx += gridDim.x * blockDim.x;</span><br><span class="line">    }</span><br><span class="line">    __syncthreads();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) {</span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);<span class="comment">//__shfl_xor_sync(0xffffffff, val,offset);</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lane == <span class="number">0</span>) sdata[warpID] = val;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">      val = (tid &lt; blockDim.x / warpSize) ? sdata[lane] : <span class="number">0.0f</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> offset = warpSize/<span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>)</span><br><span class="line">        val += __shfl_down_sync(<span class="number">0xffffffff</span>, val, offset);<span class="comment">//__shfl_xor_sync(0xffffffff, val,offset);</span></span><br><span class="line">      <span class="keyword">if</span>(tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(c, val);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//softmax_v2&lt;&lt;&lt;CeilDiv((int)N, block_size), block_size&gt;&gt;&gt;(d_A, d_B, d_sum);</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">softmax_v2</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span>* total)</span> </span>{</span><br><span class="line">  </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> idx = blockIdx.x * blockDim.x + tid;</span><br><span class="line">    <span class="type">float</span> sum = (idx &lt; N) ? <span class="built_in">expf</span>(x[idx]) : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">    sum = <span class="built_in">block_reduce_sum</span>(sum);</span><br><span class="line">    <span class="type">int</span> warpID = tid / warpSize;</span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>){</span><br><span class="line">        <span class="keyword">if</span> (tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(total, sum);</span><br><span class="line">        __threadfence();  </span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N) y[idx] = <span class="built_in">expf</span>(x[idx]) / (*total);</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="safe-softmax"><a href="#safe-softmax" class="headerlink" title="safe softmax"></a>safe softmax</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Safe Softmax x: Nx1, y: Nx1</span></span><br><span class="line"><span class="comment">// softmax_safe&lt;&lt;&lt;CeilDiv((int)N, block_size), block_size&gt;&gt;&gt;(d_A, d_B, d_sum)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">softmax_safe</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span>* total)</span> </span>{</span><br><span class="line">  </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> idx = blockIdx.x * blockDim.x + tid; </span><br><span class="line">  </span><br><span class="line">    <span class="type">float</span> ori_val = (idx &lt; N) ? x[idx] : (-FLT_MAX);<span class="comment">//FLT_MIN是正数</span></span><br><span class="line">    <span class="type">float</span> max_val = <span class="built_in">block_reduce_max</span>(ori_val);</span><br><span class="line">    <span class="type">float</span> exp_val = (idx &lt; N) ? <span class="built_in">expf</span>(ori_val - max_val) : <span class="number">0.0f</span>;</span><br><span class="line">    <span class="type">float</span> sum = <span class="built_in">block_reduce_sum</span>(exp_val);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> warpID = tid / warpSize;</span><br><span class="line">    <span class="keyword">if</span>(warpID == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (tid == <span class="number">0</span>) <span class="built_in">atomicAdd</span>(total, sum);</span><br><span class="line">        __threadfence(); </span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N) y[idx] = exp_val / (*total);</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="layernorm"><a href="#layernorm" class="headerlink" title="layernorm"></a>layernorm</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Layer Norm: x: NxK(K=256&lt;1024), y': NxK, y'=x-mean(x)/std(x) each row</span></span><br><span class="line"><span class="comment">// mean(x) = sum(x)/K, 1/std(x) = rsqrtf( sum( (x-mean(x))^2 )/K ) each row</span></span><br><span class="line"><span class="comment">// grid(N), block(K&lt;1024) N=batch_size*seq_len, K=hidden_size</span></span><br><span class="line"><span class="comment">// y=y'*g + b (g: scale, b: bias)</span></span><br><span class="line"><span class="comment">//layer_norm&lt;&lt;&lt;CeilDiv((int)N, block_size),block_size&gt;&gt;&gt;(d_A, d_B, g, b, row, block_size)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">layer_norm</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span> g, <span class="type">float</span> b, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>{</span><br><span class="line">  </span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x; <span class="comment">// 0..col-1</span></span><br><span class="line">    <span class="type">int</span> bid = blockIdx.x; <span class="comment">// 0..row-1</span></span><br><span class="line">    <span class="type">int</span> idx = bid * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> epsilon = <span class="number">1e-5</span>f;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_mean; <span class="comment">// 一个block计算一行的均值，s_mean被这个block里所有线程共享</span></span><br><span class="line">    __shared__ <span class="type">float</span> s_variance; <span class="comment">// 一个block计算一行的方差，s_variance被这个block里所有线程共享</span></span><br><span class="line">    <span class="type">float</span> value = (idx &lt; row * col) ? x[idx] : <span class="number">0.0f</span>; <span class="comment">// 只加载一次</span></span><br><span class="line">    <span class="type">float</span> sum = <span class="built_in">block_reduce_sum</span>(value);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_mean = sum / (<span class="type">float</span>) col;</span><br><span class="line">    <span class="comment">//一个block里所有线程同步</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="type">float</span> variance = (value - s_mean) * (value - s_mean);</span><br><span class="line">    variance = <span class="built_in">block_reduce_sum</span>(variance);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_variance = <span class="built_in">rsqrtf</span>(variance / (<span class="type">float</span>) col + epsilon);</span><br><span class="line">    <span class="comment">//一个block里所有线程同步</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; row * col) y[idx] = ((value - s_mean) * s_variance) * g + b;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="layernorm-float4"><a href="#layernorm-float4" class="headerlink" title="layernorm float4"></a>layernorm float4</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Layer Norm Vec4: x: NxK(K=256&lt;1024), y': NxK, y'=x-mean(x)/std(x) each row</span></span><br><span class="line"><span class="comment">// mean(x) = sum(x)/K, 1/std(x) = rsqrtf( sum( (x-mean(x))^2 )/K ) each row</span></span><br><span class="line"><span class="comment">// grid(N*K/K), block(K/4&lt;1024) N=batch_size*seq_len, K=hidden_size</span></span><br><span class="line"><span class="comment">// y=y'*g + b (g: scale, b: bias)</span></span><br><span class="line"><span class="comment">//layer_norm_float4_block&lt;&lt;&lt;CeilDiv((int)N, block_size),block_size/4&gt;&gt;&gt;(d_A, d_B, g, b, row, block_size)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">layer_norm_float4_block</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span> g, <span class="type">float</span> b, <span class="type">int</span> N, <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x; <span class="comment">// 0..K-1</span></span><br><span class="line">    <span class="type">int</span> bid = blockIdx.x; <span class="comment">// 0..N-1</span></span><br><span class="line">    <span class="type">int</span> idx = (bid * blockDim.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> epsilon = <span class="number">1e-5</span>f;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_mean; <span class="comment">// shared within block</span></span><br><span class="line">    __shared__ <span class="type">float</span> s_variance; <span class="comment">// shared within block</span></span><br><span class="line"></span><br><span class="line">    float4 tmp_x = <span class="built_in">FLOAT4</span>(x[idx]);</span><br><span class="line">    <span class="type">float</span> value = (idx &lt; N * K) ? (tmp_x.x + tmp_x.y </span><br><span class="line">                                + tmp_x.z + tmp_x.w) : <span class="number">0.0f</span>;</span><br><span class="line">    <span class="type">float</span> sum = <span class="built_in">block_reduce_sum</span>(value);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_mean = sum / (<span class="type">float</span>) K;</span><br><span class="line">    <span class="comment">// wait for s_mean in shared memory to be ready for all threads</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    float4 tmp_x_hat;</span><br><span class="line">    tmp_x_hat.x = tmp_x.x - s_mean;</span><br><span class="line">    tmp_x_hat.y = tmp_x.y - s_mean;</span><br><span class="line">    tmp_x_hat.z = tmp_x.z - s_mean;</span><br><span class="line">    tmp_x_hat.w = tmp_x.w - s_mean;</span><br><span class="line">    <span class="type">float</span> variance = tmp_x_hat.x * tmp_x_hat.x </span><br><span class="line">                    + tmp_x_hat.y * tmp_x_hat.y </span><br><span class="line">                    + tmp_x_hat.z * tmp_x_hat.z </span><br><span class="line">                    + tmp_x_hat.w * tmp_x_hat.w;</span><br><span class="line">    variance = <span class="built_in">block_reduce_sum</span>(variance);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_variance = <span class="built_in">rsqrtf</span>(variance / (<span class="type">float</span>) K + epsilon);</span><br><span class="line">    <span class="comment">// wait for s_variance in shared memory to be ready for all threads</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    float4 tmp_y;</span><br><span class="line">    tmp_y.x = tmp_x_hat.x * s_variance * g + b;</span><br><span class="line">    tmp_y.y = tmp_x_hat.y * s_variance * g + b;</span><br><span class="line">    tmp_y.z = tmp_x_hat.z * s_variance * g + b;</span><br><span class="line">    tmp_y.w = tmp_x_hat.w * s_variance * g + b;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N * K) <span class="built_in">FLOAT4</span>(y[idx]) = tmp_y;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="rmsnorm"><a href="#rmsnorm" class="headerlink" title="rmsnorm"></a>rmsnorm</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RMS Norm: x: NxK(K=256&lt;1024), y': NxK, y'=x/rms(x) each row</span></span><br><span class="line"><span class="comment">// 1/rms(x) = rsqrtf( sum(x^2)/K ) each row</span></span><br><span class="line"><span class="comment">// grid(N), block(K&lt;1024) N=batch_size*seq_len, K=hidden_size</span></span><br><span class="line"><span class="comment">// y=y'*g (g: scale)</span></span><br><span class="line"><span class="comment">//rms_norm&lt;&lt;&lt;CeilDiv((int)N, block_size),block_size&gt;&gt;&gt;(d_A, d_B, g, b, row, block_size)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">rms_norm</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span> g, <span class="type">float</span> b, <span class="type">int</span> N, <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x; <span class="comment">// 0..K-1</span></span><br><span class="line">    <span class="type">int</span> bid = blockIdx.x; <span class="comment">// 0..N-1</span></span><br><span class="line">    <span class="type">int</span> idx = bid * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> epsilon = <span class="number">1e-5</span>f;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_variance; <span class="comment">// 一个block里共享的变量</span></span><br><span class="line">    <span class="type">float</span> value = (idx &lt; N * K) ? x[idx] : <span class="number">0.0f</span>; <span class="comment">// 只加载一次</span></span><br><span class="line">    <span class="type">float</span> variance = value * value;</span><br><span class="line">    variance = <span class="built_in">block_reduce_sum</span>(variance);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_variance = <span class="built_in">rsqrtf</span>(variance / (<span class="type">float</span>) K + epsilon);</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span>(idx == <span class="number">0</span>){</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"idx :%d, value :%f, s_variance :%f\n"</span>, idx, value, s_variance);</span><br><span class="line">    }  </span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N * K) y[idx] = (value * s_variance) * g + b;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="rmsnorm-float4"><a href="#rmsnorm-float4" class="headerlink" title="rmsnorm float4"></a>rmsnorm float4</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RMS Norm Vec4: x: NxK(K=256&lt;1024), y': NxK, y'=x/rms(x) each row</span></span><br><span class="line"><span class="comment">// 1/rms(x) = rsqrtf( sum(x^2)/K ) each row</span></span><br><span class="line"><span class="comment">// grid(N), block(K/4&lt;1024) N=batch_size*seq_len, K=hidden_size</span></span><br><span class="line"><span class="comment">// y=y'*g (g: scale)</span></span><br><span class="line"><span class="comment">//rms_norm_float4_block&lt;&lt;&lt;CeilDiv((int)N, block_size),block_size/4&gt;&gt;&gt;(d_A, d_B, g, b, row, block_size)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">rms_norm_float4_block</span><span class="params">(<span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">float</span> g, <span class="type">float</span> b, <span class="type">int</span> N, <span class="type">int</span> K)</span> </span>{</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> tid = threadIdx.x; <span class="comment">// 0..K-1</span></span><br><span class="line">    <span class="type">int</span> bid = blockIdx.x; <span class="comment">// 0..N-1</span></span><br><span class="line">    <span class="type">int</span> idx = (bid * blockDim.x + threadIdx.x) * <span class="number">4</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> epsilon = <span class="number">1e-5</span>f;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_variance; <span class="comment">//一个block里共享的变量</span></span><br><span class="line">    float4 tmp_x = <span class="built_in">FLOAT4</span>(x[idx]);</span><br><span class="line">    <span class="type">float</span> variance = (idx &lt; N * K) ? (tmp_x.x * tmp_x.x </span><br><span class="line">                                    + tmp_x.y * tmp_x.y </span><br><span class="line">                                    + tmp_x.z * tmp_x.z </span><br><span class="line">                                    + tmp_x.w * tmp_x.w) : <span class="number">0.0f</span>;</span><br><span class="line">    variance = <span class="built_in">block_reduce_sum</span>(variance);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) s_variance = <span class="built_in">rsqrtf</span>(variance / (<span class="type">float</span>) K + epsilon);</span><br><span class="line">    </span><br><span class="line">    __syncthreads(); </span><br><span class="line">    float4 tmp_y;</span><br><span class="line">    tmp_y.x = tmp_x.x * s_variance * g;</span><br><span class="line">    tmp_y.y = tmp_x.y * s_variance * g;</span><br><span class="line">    tmp_y.z = tmp_x.z * s_variance * g;</span><br><span class="line">    tmp_y.w = tmp_x.w * s_variance * g;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; N * K) <span class="built_in">FLOAT4</span>(y[idx]) = tmp_y;</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>Work hard</category>
        <category>Note</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>[CUDA]sgemm</title>
    <url>/2021/08/01/cuda_sgemm/</url>
    <content><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://zhuanlan.zhihu.com/p/435908830">https://zhuanlan.zhihu.com/p/435908830 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://zhuanlan.zhihu.com/p/442930482">https://zhuanlan.zhihu.com/p/442930482 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://zhuanlan.zhihu.com/p/481600052">https://zhuanlan.zhihu.com/p/481600052 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://siboehm.com/articles/22/CUDA-MMM">https://siboehm.com/articles/22/CUDA-MMM <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<h2 id="naieve"><a href="#naieve" class="headerlink" title="naieve"></a>naieve</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//dim3 blockDim(32, 32);</span></span><br><span class="line"><span class="comment">//dim3 gridDim(CEIL_DIV(N, 32), CEIL_DIV(M, 32));</span></span><br><span class="line"><span class="comment">//mysgemm_v1&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(M, N, K, alpha, dA, dB, beta, dC_gpu);</span></span><br><span class="line">__global__ __launch_bounds__(<span class="number">1024</span>) <span class="function"><span class="type">void</span></span></span><br><span class="line"><span class="function"><span class="title">mysgemm_v1</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N, <span class="type">int</span> K, <span class="type">float</span> alpha, <span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> beta, <span class="type">float</span> *C)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> gx = blockIdx.x * blockDim.x + threadIdx.x; <span class="comment">// 全局x</span></span><br><span class="line">    <span class="type">int</span> gy = blockIdx.y * blockDim.y + threadIdx.y; <span class="comment">// 全局y</span></span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> tmp = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; K; i++) {</span><br><span class="line">        tmp += A[gy * K + i] * B[i * N + gx]; <span class="comment">// 两次全局内存访问和一次FMA（累加乘）</span></span><br><span class="line">    }</span><br><span class="line">    C[gy * N + gx] = alpha * tmp + beta * C[gy * N + gx];</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="share-mem-1-thread-process-N-points"><a href="#share-mem-1-thread-process-N-points" class="headerlink" title="share_mem + 1 thread process N points"></a>share_mem + 1 thread process N points</h3><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> OFFSET(row, col, ld) ((row) * (ld) + (col))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FLOAT4(pointer) (reinterpret_cast<span class="string">&lt;float4*&gt;</span>(&amp;(pointer))[0])</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mySgemmV1Aligned</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[BM][BK];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = {<span class="number">0.0</span>};</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; (K + BK - <span class="number">1</span>) / BK; bk++) {</span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_a[load_a_smem_m][load_a_smem_k]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; BK; k++) {</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; TM; m++) {</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> n = <span class="number">0</span>; n &lt; TN; n++) {</span><br><span class="line">                    <span class="type">int</span> comp_a_smem_m = ty * TM + m;</span><br><span class="line">                    <span class="type">int</span> comp_b_smem_n = tx * TN + n;</span><br><span class="line">                    r_c[m][n] += s_a[comp_a_smem_m][k] * s_b[k][comp_b_smem_n];</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM; i++) {</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM + i;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; TN; j += <span class="number">4</span>) {</span><br><span class="line">            <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN + j;</span><br><span class="line">            <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][j]);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h2 id="Fix-bank-conflict"><a href="#Fix-bank-conflict" class="headerlink" title="Fix bank conflict"></a>Fix bank conflict</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mySgemmV2Aligned</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[BK][BM];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_load_a[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_load_b[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_comp_a[TM];</span><br><span class="line">    <span class="type">float</span> r_comp_b[TN];</span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = {<span class="number">0.0</span>};</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> bk = <span class="number">0</span>; bk &lt; (K + BK - <span class="number">1</span>) / BK; bk++) {</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        s_a[load_a_smem_k    ][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> tk = <span class="number">0</span>; tk &lt; BK; tk++) {</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[tk][ty * TM / <span class="number">2</span>         ]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[tk][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[tk][tx * TN / <span class="number">2</span>         ]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[tk][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> tm = <span class="number">0</span>; tm &lt; TM; tm++) {</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> tn = <span class="number">0</span>; tn &lt; TN; tn++) {</span><br><span class="line">                    r_c[tm][tn] += r_comp_a[tm] * r_comp_b[tn];</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; i++) {</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">4</span>]);</span><br><span class="line">    }</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; i++) {</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + BM / <span class="number">2</span> + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">4</span>]);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h2 id="double-buffer"><a href="#double-buffer" class="headerlink" title="double buffer"></a>double buffer</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mySgemmV3Aligned</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ a, <span class="type">float</span> * __restrict__ b, <span class="type">float</span> * __restrict__ c,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BM = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BN = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BK = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TM = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> TN = <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * blockDim.x + tx;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_a[<span class="number">2</span>][BK][BM];</span><br><span class="line">    __shared__ <span class="type">float</span> s_b[<span class="number">2</span>][BK][BN];</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> r_load_a[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_load_b[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> r_comp_a[TM];</span><br><span class="line">    <span class="type">float</span> r_comp_b[TN];</span><br><span class="line">    <span class="type">float</span> r_c[TM][TN] = {<span class="number">0.0</span>};</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_smem_m = tid &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> load_a_smem_k = (tid &amp; <span class="number">1</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_k = tid &gt;&gt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> load_b_smem_n = (tid &amp; <span class="number">31</span>) &lt;&lt; <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> load_a_gmem_m = by * BM + load_a_smem_m;</span><br><span class="line">    <span class="type">int</span> load_b_gmem_n = bx * BN + load_b_smem_n;</span><br><span class="line"></span><br><span class="line">    {</span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k    ][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[<span class="number">0</span>][load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[<span class="number">0</span>][load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> bk = <span class="number">1</span>; bk &lt; (K + BK - <span class="number">1</span>) / BK; bk++) {</span><br><span class="line">        __syncthreads();</span><br><span class="line">        <span class="type">int</span> smem_sel = (bk - <span class="number">1</span>) &amp; <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> smem_sel_next = bk &amp; <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> load_a_gmem_k = bk * BK + load_a_smem_k;</span><br><span class="line">        <span class="type">int</span> load_a_gmem_addr = <span class="built_in">OFFSET</span>(load_a_gmem_m, load_a_gmem_k, K);</span><br><span class="line">        <span class="type">int</span> load_b_gmem_k = bk * BK + load_b_smem_k;</span><br><span class="line">        <span class="type">int</span> load_b_gmem_addr = <span class="built_in">OFFSET</span>(load_b_gmem_k, load_b_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(a[load_a_gmem_addr]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(b[load_b_gmem_addr]);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> tk = <span class="number">0</span>; tk &lt; BK; tk++) {</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel][tk][ty * TM / <span class="number">2</span>         ]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel][tk][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel][tk][tx * TN / <span class="number">2</span>         ]);</span><br><span class="line">            <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel][tk][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> tm = <span class="number">0</span>; tm &lt; TM; tm++) {</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> tn = <span class="number">0</span>; tn &lt; TN; tn++) {</span><br><span class="line">                    r_c[tm][tn] += r_comp_a[tm] * r_comp_b[tn];</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k    ][load_a_smem_m] = r_load_a[<span class="number">0</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">1</span>][load_a_smem_m] = r_load_a[<span class="number">1</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">2</span>][load_a_smem_m] = r_load_a[<span class="number">2</span>];</span><br><span class="line">        s_a[smem_sel_next][load_a_smem_k + <span class="number">3</span>][load_a_smem_m] = r_load_a[<span class="number">3</span>];</span><br><span class="line">        <span class="built_in">FLOAT4</span>(s_b[smem_sel_next][load_b_smem_k][load_b_smem_n]) = <span class="built_in">FLOAT4</span>(r_load_b[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    }</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> tk = <span class="number">0</span>; tk &lt; BK; tk++) {</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel_next][tk][ty * TM / <span class="number">2</span>         ]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_a[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_a[smem_sel_next][tk][ty * TM / <span class="number">2</span> + BM / <span class="number">2</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">0</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel_next][tk][tx * TN / <span class="number">2</span>         ]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(r_comp_b[<span class="number">4</span>]) = <span class="built_in">FLOAT4</span>(s_b[smem_sel_next][tk][tx * TN / <span class="number">2</span> + BN / <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> tm = <span class="number">0</span>; tm &lt; TM; tm++) {</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> tn = <span class="number">0</span>; tn &lt; TN; tn++) {</span><br><span class="line">                r_c[tm][tn] += r_comp_a[tm] * r_comp_b[tn];</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; i++) {</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i][<span class="number">4</span>]);</span><br><span class="line">    }</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; TM / <span class="number">2</span>; i++) {</span><br><span class="line">        <span class="type">int</span> store_c_gmem_m = by * BM + BM / <span class="number">2</span> + ty * TM / <span class="number">2</span> + i;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_n = bx * BN + tx * TN / <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> store_c_gmem_addr = <span class="built_in">OFFSET</span>(store_c_gmem_m, store_c_gmem_n, N);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">0</span>]);</span><br><span class="line">        <span class="built_in">FLOAT4</span>(c[store_c_gmem_addr + BN / <span class="number">2</span>]) = <span class="built_in">FLOAT4</span>(r_c[i + TM / <span class="number">2</span>][<span class="number">4</span>]);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>





]]></content>
      <categories>
        <category>Work hard</category>
        <category>Note</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>[CUDA]transpose</title>
    <url>/2021/08/16/cuda_transpose/</url>
    <content><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://code.hitori.moe/post/cuda-transpose-optimization/">https://code.hitori.moe/post/cuda-transpose-optimization/ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<h2 id="Naieve"><a href="#Naieve" class="headerlink" title="Naieve"></a>Naieve</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> BLOCK_DIM_X, <span class="type">int</span> BLOCK_DIM_Y&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_v1</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N,                  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">float</span> *__restrict__ iA,  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">float</span> *__restrict__ oA)</span> </span>{</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> y = blockIdx.y * BLOCK_DIM_Y + threadIdx.y;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> x = blockIdx.x * BLOCK_DIM_X + threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span> (y &gt;= N || x &gt;= M) {</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  }</span><br><span class="line">  oA[y * M + x] = iA[x * N + y];</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h2 id="Shared-mem"><a href="#Shared-mem" class="headerlink" title="Shared mem"></a>Shared mem</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> BLOCK_DIM_X, <span class="type">int</span> BLOCK_DIM_Y&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_v2_32x32</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N,                  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">const</span> <span class="type">float</span> *__restrict__ iA,  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">float</span> *__restrict__ oA)</span> </span>{</span><br><span class="line">  __shared__ <span class="type">float</span> smem[<span class="number">32</span>][<span class="number">32</span>];</span><br><span class="line">  <span class="comment">// Global Memory -&gt; Shared Memory</span></span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = blockIdx.x * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = blockIdx.y * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; M &amp;&amp; gx &lt; N) {</span><br><span class="line">          smem[lx][ly] = iA[gy * N + gx];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Shared Memory -&gt; Global Memory</span></span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = blockIdx.y * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = blockIdx.x * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; N &amp;&amp; gx &lt; M) {</span><br><span class="line">          oA[gy * M + gx] = smem[ly][lx];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h2 id="Fix-Bank-conflict"><a href="#Fix-Bank-conflict" class="headerlink" title="Fix Bank conflict"></a>Fix Bank conflict</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> BLOCK_DIM_X, <span class="type">int</span> BLOCK_DIM_Y&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_v3_32x32</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N,                  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">const</span> <span class="type">float</span> *__restrict__ iA,  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">float</span> *__restrict__ oA)</span> </span>{</span><br><span class="line">  __shared__ <span class="type">float</span> smem[<span class="number">32</span>][<span class="number">32</span>];</span><br><span class="line">  <span class="comment">// Global Memory -&gt; Shared Memory</span></span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = blockIdx.x * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = blockIdx.y * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; M &amp;&amp; gx &lt; N) {</span><br><span class="line">          smem[lx][(lx + ly) % <span class="number">32</span>] = iA[gy * N + gx];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">  __syncthreads();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Shared Memory -&gt; Global Memory</span></span><br><span class="line">  {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = blockIdx.y * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = blockIdx.x * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; N &amp;&amp; gx &lt; M) {</span><br><span class="line">          oA[gy * M + gx] = smem[ly][(lx + ly) % <span class="number">32</span>];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<h2 id="double-buffer"><a href="#double-buffer" class="headerlink" title="double buffer"></a>double buffer</h2><div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> ITER_BLOCK_X, <span class="type">int</span> ITER_BLOCK_Y,  <span class="comment">//</span></span><br><span class="line">          <span class="type">int</span> BLOCK_DIM_X, <span class="type">int</span> BLOCK_DIM_Y&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose_v4_32x32</span><span class="params">(<span class="type">int</span> M, <span class="type">int</span> N,                  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">const</span> <span class="type">float</span> *__restrict__ iA,  <span class="comment">//</span></span></span></span><br><span class="line"><span class="params"><span class="function">                                   <span class="type">float</span> *__restrict__ oA)</span> </span>{</span><br><span class="line">  __shared__ <span class="type">float</span> smem[<span class="number">2</span>][<span class="number">32</span>][<span class="number">32</span>];</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> MoveG2S = [&amp;](<span class="type">int</span> i) {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="type">int</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = i / ITER_BLOCK_X * gridDim.y + blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = i % ITER_BLOCK_X * gridDim.x + blockIdx.x;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = bx * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = by * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; M &amp;&amp; gx &lt; N) {</span><br><span class="line">          smem[i &amp; <span class="number">1</span>][lx][(lx + ly) % <span class="number">32</span>] = iA[gy * N + gx];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  };</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> MoveS2G = [&amp;](<span class="type">int</span> i) {</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_Y = <span class="number">32</span> / BLOCK_DIM_Y;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">const</span> <span class="keyword">auto</span> ITER_X = <span class="number">32</span> / BLOCK_DIM_X;</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_Y * BLOCK_DIM_Y == <span class="number">32</span>);</span><br><span class="line">    <span class="built_in">static_assert</span>(ITER_X * BLOCK_DIM_X == <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> by = i / ITER_BLOCK_X * gridDim.y + blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bx = i % ITER_BLOCK_X * gridDim.x + blockIdx.x;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ITER_Y; iy++) {</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> ly = iy * BLOCK_DIM_Y + threadIdx.x / BLOCK_DIM_X;</span><br><span class="line">      <span class="type">const</span> <span class="type">int</span> gy = by * <span class="number">32</span> + ly;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; ITER_X; ix++) {</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> lx = ix * BLOCK_DIM_X + threadIdx.x % BLOCK_DIM_X;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> gx = bx * <span class="number">32</span> + lx;</span><br><span class="line">        <span class="keyword">if</span> (gy &lt; N &amp;&amp; gx &lt; M) {</span><br><span class="line">          oA[gy * M + gx] = smem[i &amp; <span class="number">1</span>][ly][(lx + ly) % <span class="number">32</span>];</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  };</span><br><span class="line"></span><br><span class="line">  <span class="built_in">MoveG2S</span>(<span class="number">0</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ITER_BLOCK_Y * ITER_BLOCK_X; i++) {</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span> &lt; ITER_BLOCK_Y * ITER_BLOCK_X) {</span><br><span class="line">      <span class="built_in">MoveG2S</span>(i + <span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">MoveS2G</span>(i);</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>Play hard</category>
        <category>Note</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title>[CUDA]sgemv</title>
    <url>/2021/07/25/cuda_sgemv/</url>
    <content><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
<li><a class="link" href="https://zhuanlan.zhihu.com/p/494144694">https://zhuanlan.zhihu.com/p/494144694 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></li>
</ul>
<h2 id="Basic-concepts"><a href="#Basic-concepts" class="headerlink" title="Basic concepts"></a>Basic concepts</h2><p>The General Matrix-Vector Multiplication (GEMV) is a specialized form of General Matrix-Matrix Multiplication (GEMM), tailored for operations involving a matrix and a vector. This operation differs from GEMM in that it is optimized for scenarios where one of the operands is a vector, as opposed to another matrix. On Nvidia GPUs, the optimization methods for GEMV can be distinct from those used for GEMM. For instance, Cublas provides specific Application Programming Interfaces (APIs) such as <code>cublasSgemv</code> and <code>cublasDgemv</code> for the computation of single-precision (FP32) and double-precision (FP64) GEMV operations, respectively. However, the performance of these APIs is not always optimal. In some cases, custom implementations of GEMV can outperform the built-in Cublas functions due to the unique requirements and opportunities for optimization presented by the GEMV operation. It is often beneficial to experiment with both the provided Cublas APIs and custom implementations to determine which yields the best performance for a particular use case or hardware configuration.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/202404142156145-20240414215642471.png" alt="123"></p>
<h2 id="k-32-64-96"><a href="#k-32-64-96" class="headerlink" title="k = 32/64/96"></a>k = 32/64/96</h2><p>In this scenario, each block is set to have 128 threads, organized into 4 warps, with each warp responsible for the computation of a row of elements. During the computation, data must be read from global memory, followed by an internal reduce operation for summation within the warp. The code is as follows:</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//dim3 dimGrid(M/4);</span></span><br><span class="line"><span class="comment">//dim3 dimBlock(32,4);</span></span><br><span class="line"><span class="comment">// if K == 32</span></span><br><span class="line"><span class="comment">//Sgemv_k32&lt;&lt;&lt; dimGrid, dimBlock &gt;&gt;&gt;(d_A, d_x, d_y, M, K);</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Sgemv_k32</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">const</span> <span class="type">int</span> M,<span class="type">const</span> <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warp_size=<span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> laneId= threadIdx.x % warp_size;</span><br><span class="line">    <span class="type">int</span> current_row = blockDim.y * blockIdx.x + threadIdx.y; <span class="comment">// 1*(M/4 -1) +(0~3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(current_row &lt; M){</span><br><span class="line">        <span class="type">float</span> res=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> kIteration = K/warp_size; <span class="comment">//也可以处理k=64,96的情况，128有其它的实现方式</span></span><br><span class="line">        <span class="keyword">if</span>(kIteration==<span class="number">0</span>) kIteration=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt; kIteration; i++){</span><br><span class="line">            <span class="type">int</span> current_col = i*warp_size + laneId;</span><br><span class="line">            res += A[current_row*K + current_col] * x[current_col];</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        res = <span class="built_in">warpReduceSum</span>&lt;warp_size&gt;(res);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(laneId==<span class="number">0</span>) y[current_row]=res;</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h2 id="k-16"><a href="#k-16" class="headerlink" title="k = 16"></a>k = 16</h2><p>At this point, if a warp is still tasked with computing a single row of elements, then half of the threads within the warp would be idling. Therefore, it is more efficient to assign a warp to compute multiple rows of elements, ensuring that all 32 threads are actively engaged. The code is as follows:</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//dim3 dimGrid(M/ROW_PER_BLOCK);</span></span><br><span class="line"><span class="comment">//dim3 dimBlock(32,THREAD_PER_BLOCK/WARP_SIZE);</span></span><br><span class="line"><span class="comment">//Sgemv_k16&lt;ROW_PER_WARP&gt;&lt;&lt;&lt; dimGrid, dimBlock &gt;&gt;&gt;(d_A, d_x, d_y, M, K);</span></span><br><span class="line"><span class="comment">//dimGrid(M/8)</span></span><br><span class="line"><span class="comment">//dimgBlock(32,4)</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">const</span> <span class="type">int</span> ROW_PER_WARP&gt; </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Sgemv_k16</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> K)</span></span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warp_size=<span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> laneId= threadIdx.x % warp_size;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> current_warp_row = (blockDim.y * blockIdx.x + threadIdx.y) * ROW_PER_WARP;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> kWarp_size = warp_size / ROW_PER_WARP;</span><br><span class="line">    <span class="type">int</span> kLaneId = laneId % kWarp_size;</span><br><span class="line">    <span class="type">int</span> current_thread_row = current_warp_row + laneId / kWarp_size;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(current_thread_row &lt; M){</span><br><span class="line">        <span class="type">float</span> res=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> current_col = kLaneId;</span><br><span class="line">        res += A[current_thread_row * K + current_col] * x[current_col];</span><br><span class="line">        res = <span class="built_in">warpReduceSum</span>&lt;kWarp_size&gt;(res);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(kLaneId==<span class="number">0</span>) y[current_thread_row]=res;<span class="comment">//注意是kLaneId==0，不是laneId==0</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h2 id="k-128"><a href="#k-128" class="headerlink" title="k > 128"></a>k &gt; 128</h2><p>Similar to the case when k equals 32, where a warp is responsible for the computation of a single row of elements, the current approach takes advantage of the fact that there are now 128 elements per row. By employing the float4 vectorized memory access technique, it is possible to more efficiently access the GPU memory. The code is as follows:</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//dim3 dimGrid(M/4);</span></span><br><span class="line"><span class="comment">//dim3 dimBlock(32,4);</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Sgemv_v128</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* x, <span class="type">float</span>* y, <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> K)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warp_size=<span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> laneId= threadIdx.x % warp_size;</span><br><span class="line">    <span class="type">int</span> current_row = blockDim.y * blockIdx.x + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(current_row &lt; M){</span><br><span class="line">        </span><br><span class="line">        <span class="type">float</span> res=<span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> kIteration = (K/warp_size)/<span class="number">4</span>;</span><br><span class="line">        <span class="keyword">if</span>(kIteration==<span class="number">0</span>) kIteration=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt; kIteration; i++){</span><br><span class="line">            <span class="type">int</span> current_col_vec = (i*warp_size + laneId);</span><br><span class="line"></span><br><span class="line">            float4 current_val= <span class="built_in">reinterpret_cast</span>&lt;float4 *&gt;(&amp;A[current_row*K])[current_col_vec];</span><br><span class="line">            float4 current_x = <span class="built_in">reinterpret_cast</span>&lt;float4 *&gt;(x)[current_col_vec];</span><br><span class="line"></span><br><span class="line">            res += current_val.x*current_x.x;</span><br><span class="line">            res += current_val.y*current_x.y;</span><br><span class="line">            res += current_val.z*current_x.z;</span><br><span class="line">            res += current_val.w*current_x.w;</span><br><span class="line">        }</span><br><span class="line">        res = <span class="built_in">warpReduceSum</span>&lt;warp_size&gt;(res);</span><br><span class="line">        <span class="keyword">if</span>(laneId==<span class="number">0</span>) y[current_row]=res;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>Work hard</category>
        <category>Note</category>
        <category>Coding</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title> DARTS: Differentiable Architecture Search</title>
    <url>/2020/01/28/darts/</url>
    <content><![CDATA[<blockquote>
<p>Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search[J]. arXiv preprint arXiv:1806.09055, 2018.</p>
</blockquote>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The quest for state-of-the-art neural network architectures has traditionally demanded substantial human expertise and computational resources. The manual process of architecture design has been a bottleneck, prompting the development of automated algorithmic solutions. While these have achieved competitive performance, existing architecture search algorithms like reinforcement learning (RL) and evolution are computationally intensive, often requiring thousands of GPU days. This has led to an inherent scalability challenge, as the search is treated as a black-box optimization problem over a discrete domain, necessitating a large number of architecture evaluations. The drive to overcome these limitations and the desire to automate the design process with increased efficiency and reduced computational costs have been the key motivators behind the development of the DARTS method.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>DARTS (Differentiable ARchiTecture Search) revolutionizes the field by formulating architecture search as a differentiable problem, enabling the use of gradient descent for optimization. This novel algorithm is based on bilevel optimization and is applicable to both convolutional and recurrent architectures. DARTS stands out by matching or even surpassing the performance of state-of-the-art methods while using orders of magnitude less computation resources. It also outperforms ENAS, another efficient architecture search method, by discovering cells with comparable error rates but fewer parameters. Furthermore, DARTS demonstrates the transferability of learned architectures from CIFAR-10 and PTB to ImageNet and WikiText-2, respectively, showcasing its versatility and robustness.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330012026177.png" alt="image-20240330012026177"></p>
<p>DARTS represents the computation procedure of an architecture as a directed acyclic graph, with nodes as latent representations and edges as operations. By relaxing the categorical choice of operations to a softmax distribution, the architecture search is reduced to learning a set of continuous variables. DARTS employs a bilevel optimization strategy to jointly optimize the architecture and its weights, using gradient descent to minimize validation loss. An approximation technique is introduced to manage computational complexity, leading to an iterative process that converges to a final architecture.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330012054615.png" alt="image-20240330012054615"></p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330012253806.png" alt="image-20240330012253806"></p>
<p>The paper’s experiments are conducted on four datasets: CIFAR-10, ImageNet, Penn Treebank, and WikiText-2. The goal is to discover high-performance convolutional architectures for image classification and recurrent architectures for language modeling. The experiments are divided into two main phases: architecture search and architecture evaluation.</p>
<p>During the architecture search phase, the authors use DARTS to explore the space of possible network architectures. They define the search space based on a set of candidate operations, such as convolutions, pooling, and linear transformations, and allow the algorithm to learn the optimal combination of these operations. The search process is efficient due to the continuous relaxation of the architecture representation, which enables the use of gradient descent for optimization.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330012324666.png" alt="image-20240330012324666"></p>
<p>In the architecture evaluation phase, the authors construct larger networks using the best cells found during the search phase. These networks are trained from scratch on the test sets to evaluate their performance. The authors report that DARTS is able to identify a convolutional cell that achieves a test error of 2.76 ± 0.09% on CIFAR-10, which is competitive with state-of-the-art results obtained using significantly more computational resources. Similarly, for language modeling, DARTS discovers a recurrent cell that achieves a test perplexity of 55.7 on Penn Treebank, outperforming both manually tuned LSTMs and other automatically discovered cells.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330012701560.png" alt="image-20240330012701560"></p>
<p>One of the key findings is the transferability of the architectures learned by DARTS. The authors demonstrate that a convolutional cell learned on CIFAR-10 can be transferred to ImageNet with competitive performance, and a recurrent cell learned on Penn Treebank can be transferred to WikiText-2. This transferability is crucial as it shows the versatility of DARTS in discovering architectures that are not only optimal for a specific dataset but also generalizable to other tasks.</p>
<p>The authors also compare DARTS with existing state-of-the-art methods, such as NASNet, AmoebaNet, and ENAS. The results show that DARTS can match or even outperform these methods while using orders of magnitude fewer computational resources. For instance, while NASNet required 2000 GPU days and AmoebaNet required 3150 GPU days to achieve state-of-the-art results, DARTS achieved comparable performance in just a few GPU days.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Real-time federated evolutionary neural architecture search</title>
    <url>/2021/10/02/fedevo/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/2003.02793.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In the realm of machine learning, preserving user privacy has become paramount, leading to the development of federated learning. This distributed approach allows multiple devices to collaboratively train a shared model while keeping data local, thus enhancing privacy. However, federated learning faces challenges, particularly the high communication costs due to the transmission of large model parameters and the substantial computational resources required for training complex models like deep neural networks. These issues are further compounded when conducting neural architecture search (NAS) within a federated environment.</p>
<p>The traditional centralized NAS methods, which are often computationally intensive, are not ideal for federated learning due to their disregard for model size and computational efficiency. Moreover, techniques like transfer learning, which are common in NAS, may not be directly applicable to federated learning as they can lead to model divergence and increased communication overhead.</p>
<p>To address these challenges, there is a need for a real-time, federated NAS algorithm that can efficiently optimize model performance while minimizing communication and computational costs. This involves developing strategies that allow for the sampling and training of sub-models on a subset of clients, inheriting weights from a master model to avoid reinitialization, and aggregating these sub-models into a global model. The goal is to embed the evolutionary optimization process seamlessly within the federated learning rounds, thereby reducing the need for extensive computational resources and maintaining the privacy of user data. The proposed method aims to bridge the gap between the computational demands of NAS and the constraints of federated learning, paving the way for more efficient and privacy-aware machine learning applications.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>This work introduces a novel framework that addresses the critical challenges of integrating neural architecture search (NAS) with federated learning. The primary contributions are twofold:</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183119853.png" alt="image-20240330183119853"></p>
<ol>
<li><p>A pioneering real-time evolutionary NAS method is proposed, which not only optimizes model performance but also significantly reduces local computational burden and communication overhead. This is achieved through an innovative double-sampling technique that transmits a randomly sampled sub-model from a master model to a subset of clients for training, without the need for reinitialization. This approach ensures that each local device trains only one sub-network per generation, thereby minimizing the upload of parameters and maintaining model performance stability.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183203077.png"></p>
</li>
<li><p>An effective aggregation strategy is developed for updating the global model based on the sub-networks trained at each generation. This strategy leverages the inherited weights from the master model, accelerating convergence and avoiding performance deterioration due to random reinitialization. The proposed method is validated through extensive comparative studies, demonstrating its ability to produce models with competitive learning performance and computational efficiency, outperforming traditional offline evolutionary NAS methods by at least five times in terms of speed.</p>
</li>
</ol>
<p>These contributions represent a significant advancement in the field of federated learning and NAS, offering a practical solution for real-time, privacy-preserving optimization of neural network architectures.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The methodology presented in this work is designed to tackle the intricate problem of conducting neural architecture search (NAS) within the constraints of federated learning. The approach is centered around a real-time evolutionary framework that optimizes model performance while reducing computational and communication costs. Here’s a detailed breakdown of the methodological components:</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183231177.png" alt="image-20240330183231177"></p>
<p><strong>Double-Sampling Technique:</strong><br>The core innovation lies in the double-sampling technique, which is instrumental in minimizing the local computational payload and server-client communication. This technique involves two key steps: first, a sub-network is randomly sampled from a master model for each individual in the evolutionary population; second, a subset of clients is randomly selected for training these sub-networks. This randomization ensures that the computational load is evenly distributed and that each client trains a unique sub-network per generation, thus avoiding redundant computations.</p>
<p><strong>Master-Slave Communication Paradigm:</strong><br>The master model serves as the baseline from which sub-networks are derived. It is初始化 at the outset and then continuously updated throughout the evolutionary process. Client devices download the sub-network corresponding to their assigned individual and train it using their local data. The updated sub-networks are then uploaded back to the server, which aggregates them to refine the master model. This iterative process of downloading, training, and uploading is repeated across generations, embedding the NAS within the federated learning rounds.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183239569.png" alt="image-20240330183239569"></p>
<p><strong>Inheritance of Weights:</strong><br>A critical aspect of this method is the inheritance of weights from the master model to the sub-networks. This eliminates the need to reinitialize model parameters, which is both time-consuming and can lead to performance degradation. Instead, the sub-networks benefit from the knowledge accumulated in the master model, allowing them to start training from a more informed point and thus converge faster.</p>
<p><strong>Aggregation Strategy:</strong><br>The aggregation strategy is tailored to the federated setting, where clients may sample different sub-networks for different individuals. The server must reconcile these partial updates into a coherent global model. This is achieved by selectively combining the parameters of the trained sub-networks based on their corresponding choice keys, which guide which parts of the master model are updated.</p>
<p><strong>Objectives and Evaluation:</strong><br>The framework optimizes two main objectives: the test error of the global model, reflecting its predictive performance, and the floating-point operations per second (FLOPs), indicative of the model’s computational efficiency. A multi-objective evolutionary algorithm (MOEA), specifically NSGA-II, is employed to balance these objectives and guide the search towards a set of Pareto optimal solutions.</p>
<p><strong>Real-Time Optimization:</strong><br>The entire process is designed to be real-time, with each generation of the evolutionary algorithm aligning with a communication round in federated learning. This synchronization ensures that the NAS is not just efficient but also seamlessly integrated into the operational workflow of the federated learning system.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><strong>Experimental Setup:</strong> The experiments utilize a distributed system with varying numbers of clients to simulate a federated learning scenario. The master model is a lightweight convolutional neural network (CNN) with a maximum of 26 hidden layers, ensuring that communication costs are kept to a minimum. The dataset used for training and testing is Cifar10, a standard benchmark for image recognition tasks, which consists of 50,000 training and 10,000 testing 32x32 RGB images across 10 different classes.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183257102.png" alt="image-20240330183257102"></p>
<p>The federated learning process is configured with different client participation ratios and communication rounds, which are equivalent to the number of generations in the evolutionary algorithm. Each client’s local model parameters are updated using stochastic gradient descent (SGD), and the server aggregates these updates to form the global model. The batch normalization layers within the master model are modified to disable trainable parameters, as they can hinder convergence in federated learning settings.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183306594.png" alt="image-20240330183306594"></p>
<p><strong>Genetic Algorithm Configuration:</strong> The multi-objective evolutionary algorithm (MOEA) used in these experiments is the elitist non-dominated sorting genetic algorithm (NSGA-II), which is known for its efficiency in handling multi-objective optimization problems. The algorithm parameters, such as population size, crossover probability, mutation probability, and bit length, are carefully chosen to balance exploration and exploitation during the search process.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330183317877.png" alt="image-20240330183317877"></p>
<p><strong>Experimental Results:</strong> The results of the experiments demonstrate the proposed methodology’s ability to identify a set of Pareto optimal solutions that trade off between model performance and computational complexity. The solutions obtained after 500 generations show that the real-time federated NAS algorithm can achieve competitive test accuracies while significantly reducing the number of floating-point operations per second (FLOPs) compared to the baseline ResNet18 model.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
        <tag>FL</tag>
      </tags>
  </entry>
  <entry>
    <title>Joint neural architecture search and quantization</title>
    <url>/2020/01/23/jointNasQ/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1811.09426.pdf">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>In the realm of deep learning, designing efficient neural network architectures and compressing models for mobile deployment present significant challenges. Traditional approaches require extensive expertise and iterative trial-and-error, which is both time-consuming and resource-intensive. The advent of mobile devices with limited computational capabilities has further exacerbated the need for compact models that maintain high performance.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330185955896.png" alt="image-20240330185955896"></p>
<p>To address these issues, the paper introduces a unified framework that integrates neural architecture search (NAS) with model compression strategies, aiming to automate the discovery of compact, high-performing neural networks suitable for mobile platforms. By leveraging a multi-objective evolutionary search algorithm, the framework balances model size and accuracy, leading to the development of models that outperform existing methods in terms of efficiency and effectiveness.</p>
<p>The motivation behind this research is to alleviate the burden on human experts and to bridge the gap between cutting-edge AI research and real-world applications. By automating the process, the framework not only accelerates the development of neural networks but also paves the way for wider adoption of deep learning technologies across various mobile devices, despite their constrained resources.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>This research makes several pivotal contributions to the field of neural network optimization for mobile applications. Firstly, it presents a novel method that seamlessly integrates neural architecture search (NAS) with quantization policies, enabling the co-evolution of network topology and compression strategies. This joint approach is shown to yield models that are not only compact but also maintain superior performance, surpassing the capabilities of methods that focus solely on either architecture or quantization.</p>
<p>Secondly, the paper introduces a multi-objective evolutionary algorithm that effectively balances model size and accuracy, resulting in the discovery of high-accuracy, small-footprint models that are ideal for resource-constrained environments. The algorithm’s efficiency is demonstrated through its ability to complete the joint search within a short timeframe, using minimal computational resources.</p>
<p>Additionally, the research provides learning-based quantization policies for existing networks, significantly improving their performance on mobile platforms. The proposed method is not only effective for new architecture design but also offers a valuable tool for enhancing the performance of pre-trained models, making it a versatile solution for mobile deep learning applications.</p>
<p>Overall, these contributions represent a significant step forward in making deep learning more accessible and efficient for mobile devices, potentially revolutionizing the way AI models are developed and deployed in real-world scenarios.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190012000.png" alt="image-20240330190012000" style="zoom:50%;">

<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190022869.png" alt="image-20240330190022869" style="zoom:50%;">

<p>The methodology at the core of this research is an innovative integration of neural architecture search (NAS) and model quantization, designed to optimize the design of neural networks for mobile deployment. The approach is centered around a multi-objective evolutionary algorithm that aims to maximize the accuracy of neural networks while minimizing their size, thereby addressing the computational constraints of mobile devices.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190033602.png" alt="image-20240330190033602"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190049055.png" alt="image-20240330190049055"></p>
<p>The framework begins by defining a search space that encompasses both architectural choices and quantization policies. The architecture search space is inspired by existing NAS methods, utilizing a cell-based approach with Inception-like modules, including both normal and reduction cells. These cells are stacked in specific patterns to form the overall network structure. The quantization policy search space, on the other hand, focuses on determining the optimal bit allocation for each cell in the network, allowing for different layers to have varying levels of quantization based on their redundancy.</p>
<p>The evolutionary algorithm operates on a population of models, initializing them randomly and then iteratively improving them through selection, mutation, and training. During each iteration, a subset of the population is sampled, and their fitness, based on a multi-objective function, is evaluated. This function prioritizes accuracy but penalizes excessive model growth, ensuring that the search remains focused on compact models.</p>
<p>Mutation in the algorithm involves altering the architectural combinations and quantization bits, introducing diversity into the population and facilitating exploration of the search space. The models are then trained on a dataset, quantized according to their assigned policies, and evaluated on a validation set to measure their accuracy and size. The best-performing models are carried over to the next generation, while the least fit are discarded.</p>
<p>A key aspect of the methodology is its flexibility. It can be applied to existing networks, providing tailored quantization policies to enhance their performance on mobile devices. Furthermore, the joint search for architecture and quantization policies allows for the discovery of entirely new models, termed JASQNet and JASQNet-Small, which demonstrate competitive accuracy and significantly reduced size compared to existing methods.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190102680.png" alt="image-20240330190102680"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190216448.png" alt="image-20240330190216448"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190226229.png" alt="image-20240330190226229"></p>
<p>The quantization process itself is meticulous, involving linear scaling of weight vectors, bucketing to manage magnitude imbalance, and rounding to the nearest quantization point. This attention to detail ensures that the compression of models does not come at the cost of significant accuracy loss.</p>
<p>The experimental results showcase the effectiveness of the methodology. The approach is tested on CIFAR-10 and ImageNet, with the resulting JASQNet models outperforming their counterparts in terms of error rates and model size. The method also proves capable of improving the performance of pre-trained networks like ResNet and MobileNet when applied with learned quantization policies.</p>
<p>In summary, the methodology presented in this research is a significant advancement in the field of neural network optimization. It offers a systematic and automated approach to designing and compressing neural networks for mobile applications, resulting in models that are both efficient and effective.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330185939879.png" alt="image-20240330185939879"></p>
<p>The experimental setup in this research is meticulously designed to validate the effectiveness of the proposed methodology. The experiments are divided into two main phases: first, the application of the method to existing networks to provide tailored quantization policies, and second, the joint search for architecture and quantization policies to discover new models.</p>
<p>In the initial phase, the method is applied to a variety of state-of-the-art networks, including ResNet, DenseNet, and MobileNet architectures. The goal here is to demonstrate the ability of the method to enhance the performance of pre-trained models by learning optimal quantization policies. The networks are evaluated on the ImageNet dataset, and the results are compared against their non-quantized (float) counterparts as well as other quantization strategies, such as 2-bit, 4-bit, 8-bit, and 16-bit models.</p>
<p>The second phase involves a joint search for architecture and quantization policies on the CIFAR-10 dataset, aiming to discover new models, referred to as JASQNet and JASQNet-Small. The search space is vast, combining the architectural choices and quantization levels, and the evolutionary algorithm is run for a number of iterations to allow for thorough exploration.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190120339.png" alt="image-20240330190120339"></p>
<p>The experimental results are highly promising. For existing networks, the method successfully improves accuracy and reduces model size. For instance, the quantized ResNet18 model achieves a higher accuracy than the float model while having significantly fewer parameters. Similarly, MobileNet-v1 and MobileNet-v2 see improvements in accuracy when applying the learned quantization policies, outperforming their 8-bit and float counterparts.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330190137729.png" alt="image-20240330190137729"></p>
<p>In the joint search experiments, the discovered JASQNet models demonstrate competitive accuracy on both CIFAR-10 and ImageNet datasets, with JASQNet-Small showing particularly impressive results in terms of model size reduction. These models achieve lower error rates with smaller footprints compared to other methods that focus solely on architecture search. The JASQNet models also maintain a better balance between accuracy and size when compared to models resulting from only quantization search.</p>
<p>Overall, the experimental results substantiate the effectiveness of the proposed methodology in optimizing neural networks for mobile deployment. The method not only enhances existing models but also has the capability to discover new, efficient architectures that are well-suited for resource-constrained environments. These findings pave the way for more widespread adoption of deep learning on mobile platforms, potentially leading to advancements in various mobile-based applications.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
        <tag>Quantization</tag>
      </tags>
  </entry>
  <entry>
    <title>Designing Neural Network Architectures using Reinforcement Learning</title>
    <url>/2020/01/17/metaqnn/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1611.02167.pdf">https://arxiv.org/pdf/1611.02167.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330145949057.png" alt="image-20240330145949057"></p>
<p>In the realm of machine learning, the design of effective convolutional neural network (CNN) architectures has traditionally required significant human expertise and labor. The process involves a meticulous trial-and-error approach, where researchers draw upon theoretical insights and empirical observations to craft networks that perform well on specific tasks. As the complexity of CNNs has grown, so too has the design space, making manual exploration infeasible due to the combinatorial explosion of possible configurations.</p>
<p>The paper “Designing Neural Network Architectures Using Reinforcement Learning” addresses this challenge by introducing MetaQNN, a meta-modeling algorithm that leverages reinforcement learning to automate the generation of high-performing CNN architectures. The motivation behind this approach is to reduce the manual effort involved in CNN design and to potentially discover novel architectures that humans might overlook.</p>
<p>MetaQNN employs a Q-learning agent that learns to select CNN layers sequentially, guided by an ε-greedy exploration strategy and experience replay to navigate the large but finite space of architectural possibilities. The agent’s goal is to optimize the network’s performance on a given learning task, as measured by validation accuracy, which serves as the reward signal.</p>
<p>This research is motivated by the desire to streamline the process of network design, enabling the development of tailored CNNs for various tasks without extensive human intervention. By automating this process, researchers can focus on higher-level tasks and insights, while the algorithm handles the intricacies of architecture optimization. The potential applications of this work are vast, as it could lead to more efficient and effective machine learning solutions across a range of domains.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>The paper “Designing Neural Network Architectures Using Reinforcement Learning” makes several significant contributions to the field of automated machine learning (AutoML). Firstly, it introduces MetaQNN, a novel algorithm that applies reinforcement learning to the design of convolutional neural network (CNN) architectures. This algorithm is capable of discovering high-performing CNNs without human intervention, addressing the labor-intensive nature of manual network design.</p>
<p>Secondly, the paper demonstrates that MetaQNN can compete with, and even outperform, state-of-the-art methods that use more complex layer types. This is achieved by exploring a large but finite space of possible architectures and iteratively improving upon the designs based on validation accuracy.</p>
<p>Additionally, the paper shows that the networks designed by MetaQNN are not only optimized for the task they were trained on but also exhibit strong transfer learning capabilities. This indicates that the algorithm can produce versatile network architectures that are applicable to various image classification tasks.</p>
<p>Lastly, the research provides a new perspective on neural network design, suggesting that reinforcement learning can be an effective tool for automating the process. This opens up new avenues for future research in AutoML, where the goal is to further automate and optimize the machine learning pipeline.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330150424604.png" alt="image-20240330150424604"></p>
<p>The primary methodological innovation of the paper “Designing Neural Network Architectures Using Reinforcement Learning” is the MetaQNN algorithm, which utilizes Q-learning to automate the process of designing convolutional neural network (CNN) architectures. The core concept is to treat the selection of CNN layers as a Markov Decision Process (MDP), where the learning agent explores the space of possible network topologies to find high-performing architectures.</p>
<p>The MetaQNN agent is trained using Q-learning, a model-free reinforcement learning technique that focuses on learning the optimal policy through interaction with the environment. The agent operates in a state space defined by the parameters of the CNN layers and an action space that corresponds to the choices of adding a particular layer type. The agent starts with a random exploration of the architecture space, gradually transitioning to more informed decisions as it learns from experience.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330150453009.png" alt="image-20240330150453009"></p>
<p>A key component of the method is the ε-greedy exploration strategy, which balances exploration and exploitation. The agent initially explores widely by selecting actions randomly, with a probability ε. As the agent accumulates experience, it exploits its knowledge by choosing the action with the highest estimated Q-value with probability 1-ε. The value of ε is gradually decreased over time, shifting the agent’s behavior from exploration to exploitation.</p>
<p>To enhance learning efficiency, the MetaQNN algorithm employs experience replay, a technique where the agent’s past experiences are stored in a replay memory. The agent periodically samples from this memory to update its Q-values, allowing it to learn from its past actions more effectively.</p>
<p>The agent’s performance is evaluated based on the validation accuracy of the CNN architectures it designs. The architectures that yield higher validation accuracy are considered more desirable, and the agent’s Q-values are updated accordingly. The process continues until the agent converges on a policy that consistently produces high-performing CNN architectures.</p>
<p>This method represents a significant departure from traditional manual design processes, offering a data-driven approach to network architecture optimization. By automating the design process, MetaQNN has the potential to accelerate the development of CNNs and to discover novel architectures that could outperform those designed by human experts.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330150952925.png" alt="image-20240330150952925"></p>
<p>The experimental setup involves training a learning agent to sequentially choose CNN layers using Q-learning, with the goal of maximizing the validation accuracy on the given datasets. The agent’s state space is defined by the parameters of different layer types, such as convolution, pooling, and fully connected layers, and the action space is determined by the possible layer selections. The agent employs an ε-greedy strategy to balance exploration and exploitation, with ε gradually reduced over time to shift from random exploration to informed exploitation.</p>
<p>The training process for the agent involves aggressive training schemes with quick convergence, using techniques like dropout and learning rate adjustments to prevent overfitting and to ensure efficient learning. The agent’s performance is evaluated based on the validation accuracy of the CNN architectures it designs, which serves as the reward signal for the Q-learning updates.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330151218241.png" alt="image-20240330151218241"></p>
<p>The experimental results are highly promising. The networks designed by the MetaQNN agent outperform existing networks that use the same layer types, demonstrating the agent’s ability to make informed architectural decisions. Moreover, these networks are competitive with state-of-the-art methods that incorporate more complex layer types and training procedures, showcasing the potential of the MetaQNN approach to match or even surpass human-designed architectures.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330151313064.png" alt="image-20240330151313064"></p>
<p>In terms of transfer learning, the best MetaQNN model on CIFAR-10 dataset shows good performance when trained on other datasets, indicating the versatility of the architectures discovered by the algorithm. The ensemble of top models selected by the agent also leads to a significant boost in prediction performance, further validating the effectiveness of the MetaQNN approach.</p>
<p>Overall, the experiments in the paper provide strong evidence that reinforcement learning can be effectively applied to the automated design of neural network architectures, offering a promising direction for future research in automating the machine learning process. The success of MetaQNN in discovering high-performing and transferable CNN architectures underscores the potential of this approach to revolutionize the way we design neural networks for various tasks.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Architecture Search with Reinforcement Learning</title>
    <url>/2020/01/02/nas/</url>
    <content><![CDATA[<blockquote>
<p>Zoph B, Le Q V. Neural architecture search with reinforcement learning[J]. arXiv preprint arXiv:1611.01578, 2016.</p>
</blockquote>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Imagine you’re on a quest to build the ultimate digital brain, a neural network so smart it could tackle the toughest challenges in image recognition and language understanding. Sounds like a job for the world’s brightest minds, right? That’s what it used to be. But designing neural networks by hand is a grind, involving endless trial and error, and a whole lot of coffee-fueled late nights.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330092934794.png" alt="image-20240330092934794"></p>
<p>This paper kicks off a revolution. It’s about Neural Architecture Search with Reinforcement Learning - think of it as a neural network that designs other neural networks. We’re talking about a game-changer here, a way to automate the process of creating these complex networks, which have been the backbone of many AI breakthroughs.</p>
<p>The motivation? To free researchers from the tedious task of manual design and let them focus on the big picture. The authors saw a gap: current methods are either too rigid or too slow to keep up with the ever-growing scale of neural networks. They needed something flexible, something scalable, and something that could learn from its successes and failures.</p>
<p>So, they came up with an approach that treats architecture design as a reinforcement learning problem. It’s like teaching an AI to be an architect, learning from experience and getting better over time. The result? A system that can craft neural networks that are not only top-performing but also more efficient than ever before. It’s a bold step towards a future where AI can design itself, and this paper is where it all begins.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>The main contribution of this paper is a method that uses a recurrent neural network (RNN), think of it as a creative AI artist, to generate the blueprints for new neural networks. Then, it employs reinforcement learning to train this RNN, aiming to maximize the accuracy of these newly minted architectures on a validation set.</p>
<p>The results are nothing short of impressive. On the CIFAR-10 dataset, a benchmark for image recognition, the method crafted a novel network architecture that outperforms existing designs, achieving a test error rate of 3.65%. That’s not just a win on paper; it’s 1.05 times faster than the previous state-of-the-art model.</p>
<p>But that’s not all. On the Penn Treebank dataset, a language modeling task, the method composed a novel recurrent cell that outshines the widely-used LSTM cell. This new cell achieved a test set perplexity of 62.4, a significant 3.6 perplexity improvement over the previous best model.</p>
<p>The paper also dives into the nitty-gritty of the method, discussing how it can be accelerated with parallelism and asynchronous updates, and how it can increase architecture complexity with skip connections and other layer types. It even shows how the same method can be adapted to generate recurrent cell architectures, further expanding its versatility.</p>
<p>In a nutshell, this paper is a giant leap towards making neural network design more accessible and efficient, opening up new possibilities for AI research and applications. It’s like handing the keys to the AI kingdom to the next generation of machine learning enthusiasts.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Here’s where it gets interesting: this director, or controller RNN, is trained using reinforcement learning. It’s like training a newbie director by rewarding it for good takes (in this case, network designs that perform well on a validation set) and giving it constructive feedback (updating the RNN parameters) to help it improve shot after shot, design after design.</p>
<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102426278.png" alt="image-20240330102426278" style="zoom:33%;">

<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102447850.png" alt="image-20240330102447850" style="zoom:33%;">

<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102503614.png" alt="image-20240330102503614" style="zoom:33%;">

<p>The method starts with a simple premise: a neural network’s structure can be described by a variable-length string, which the RNN controller generates. It’s like the director writing the script for a movie. Once the script is written, it’s time to film the movie, which in this context means training the ‘child network’ on real data. The performance of the child network on a validation set is then used as a reward signal to train the director RNN.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102532285.png" alt="image-20240330102532285"></p>
<p>The paper presents a few key improvements to this core approach. One is the introduction of skip connections and other layer types to increase the complexity of the generated architectures. It’s like allowing the director to experiment with different cinematography techniques to make the movie more engaging.</p>
<p>Another trick up the paper’s sleeve is using a parameter server approach for distributed training. Imagine having a team of production assistants who can handle different parts of the movie simultaneously. This speeds up the process of shooting the film and allows the director to try out more scenes in less time.</p>
<p>The paper also talks about generating recurrent cell architectures, which is like directing a movie with complex plotlines that loop back on themselves. The controller RNN has to figure out how to best connect the plot points (or layers) to create a coherent narrative (or functioning neural network).</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102622433.png" alt="image-20240330102622433"></p>
<p>On the CIFAR-10 dataset, a playground for image recognition, the method brewed up a novel network architecture that outperforms the best human-invented recipes. The newly minted model sizzled with a test error rate of 3.65%, outclassing the previous state-of-the-art model by a whisker and cooking up results 1.05 times faster.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330102649905.png" alt="image-20240330102649905"></p>
<p>But the authors weren’t content with just dishing out success in image recognition. They turned to the Penn Treebank dataset, a staple in language modeling, and whipped up a recurrent cell architecture that dethroned the reigning LSTM cell. This new cell architecture achieved a test set perplexity of 62.4, besting the previous record by a solid 3.6 perplexity points.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>MONAS: Multi-Objective Neural Architecture Search</title>
    <url>/2020/01/16/monasnet/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1806.10332.pdf">https://arxiv.org/pdf/1806.10332.pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Despite the advancements in NAS, most existing works focus on optimizing models for high prediction accuracy, which often results in complex architectures that may not be suitable for deployment in computing environments with limited resources, such as battery-powered mobile devices. This limitation has motivated the development of MONAS (Multi-Objective Neural Architecture Search), a novel framework that extends the capabilities of NAS by considering not only prediction accuracy but also other critical objectives like power consumption during the search for neural network architectures.</p>
<p>The primary motivation behind MONAS is to bridge the gap between high-performance models and deployability in resource-constrained settings. By integrating multi-objective optimization, MONAS allows for the search process to be guided by various performance metrics and application-specific constraints, leading to the discovery of networks that offer a balance between accuracy and efficiency. This approach aims to democratize the development of neural network architectures by making them accessible to a wider range of platforms and applications without compromising on performance.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>The paper introduces MONAS, a pioneering framework for Multi-Objective Neural Architectural Search, which stands out with its unique contributions to the field of neural network optimization. Firstly, it proposes a multi-objective approach that simultaneously considers prediction accuracy and additional critical objectives, such as energy consumption. This dual focus addresses the limitations of previous NAS methods, which often overlooked the trade-offs between performance and deployability in resource-constrained environments.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330155922697.png" alt="image-20240330155922697"></p>
<p>Secondly, MONAS demonstrates adaptability by allowing users to incorporate customized constraints that reflect the specific requirements of different applications or platforms. This flexibility ensures that the search process is guided by a diverse set of objectives, leading to a broader spectrum of optimized architectures.</p>
<p>Additionally, the paper presents MONAS-S, an extension of the original framework, designed to enhance scalability and speed. By adopting weight-sharing techniques, MONAS-S is capable of searching larger design spaces more efficiently, significantly reducing the time required to find optimal network architectures.</p>
<p>The empirical results of the study show that models discovered by MONAS not only achieve comparable or better classification accuracy on computer vision tasks but also satisfy additional objectives such as peak power consumption. This evidences the effectiveness of MONAS in guiding the search process towards architectures that meet predefined constraints, outperforming state-of-the-art models in both accuracy and energy efficiency.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330160020944.png" alt="image-20240330160020944"></p>
<p>The MONAS framework is meticulously crafted to address the multifaceted challenges of neural architecture search. At its core, MONAS employs a two-stage framework that leverages reinforcement learning to navigate the complex landscape of neural network design. The first stage involves the generation of candidate architectures through a recurrent neural network (RNN), which serves as the “robot network” in the search process. This RNN generates a sequence of hyperparameters for a CNN, which are then used to construct and train the “target network” in the second stage. The performance of the target network, in terms of both accuracy and energy consumption, is feedback to the robot network as a reward signal, guiding the search process.</p>
<p>One of the distinguishing features of MONAS is its adaptability. It allows users to define customized constraints that can be seamlessly integrated into the search process as additional objectives. These constraints are transformed into reward functions that work in tandem with the primary objectives of accuracy and efficiency, ensuring that the search is aligned with the specific needs of various applications and platforms.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330160105915.png" alt="image-20240330160105915"></p>
<p>To enhance scalability, MONAS introduces a weight-sharing technique, which is further developed into MONAS-S. This extension pre-trains a directed acyclic graph (DAG) that encompasses all possible networks, allowing for a more efficient search process. The controller, which is the RNN in MONAS, is updated using the REINFORCE policy learning method, where the validation accuracy and other objective measures serve as the reward signal.</p>
<p>The reward function in MONAS is carefully designed to reflect the trade-offs between different objectives. It includes a mixed reward that balances accuracy and energy consumption, as well as constraints on peak power and Multiply-ACcumulate (MAC) operations, which serve as a proxy for power usage. This nuanced approach to the reward function enables MONAS to discover architectures that not only perform well in terms of accuracy but also meet the energy efficiency requirements.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330160154008.png" alt="image-20240330160154008"></p>
<p>The experimental setup of the MONAS framework is designed to thoroughly evaluate its performance and efficacy in searching for neural network architectures that balance prediction accuracy with energy efficiency. The experiments are conducted using Python with the TensorFlow library, running on a system equipped with an Intel XEON E5-2620v4 processor and GeForce GTX1080Ti GPU cards. The NVIDIA profiling tool, nvprof, is utilized to measure the peak power and energy consumption of the target networks, which are crucial for updating the robot network’s weights.</p>
<p>The CIFAR-10 dataset is chosen to validate the target networks, with 5000 randomly selected images from the training set used as the validation set. The validation accuracy is taken as an objective in the reward function. The Multiply-ACcumulate (MAC) operations of a sampled target network are calculated based on the approach described in MnasNet, providing a measure of the computational cost.</p>
<p>The experimental results demonstrate the adaptability and efficiency of MONAS in guiding the search process. When constraints are applied to limit peak power or require a minimum accuracy, MONAS successfully directs its search to regions that satisfy these conditions. For instance, out of 600 iterations, a significant number of target networks fall within the specified power or accuracy thresholds, indicating the framework’s ability to adapt to different reward functions.</p>
<p>The paper also explores how the Pareto Front changes under different reward functions, showing that MONAS can discover architectures with either higher accuracy or lower energy consumption depending on the weightage given to each objective. Notably, MONAS outperforms random search in finding better architectures within a limited search time, highlighting its efficiency.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330160230178.png" alt="image-20240330160230178"></p>
<p>In terms of discovering noteworthy architectures, MONAS is compared with state-of-the-art models like CondenseNet, demonstrating that it can find models with higher accuracy and lower energy consumption. The best-performing models discovered by MONAS are reported, along with their hyperparameters and energy costs per 1000 inferences, providing evidence of the framework’s capability to surpass existing solutions.</p>
<p>The scalability of MONAS is further validated through MONAS-S, which is applied to a search space of 1.6 × 10^29 possible networks in a 12-layered architecture. Even in this vast search space, MONAS-S successfully guides the search, finding models that require fewer MAC operations while maintaining high accuracy.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
  <entry>
    <title>Proxylessnas: Direct neural architecture search on target task and hardware</title>
    <url>/2020/02/07/proxylessnas/</url>
    <content><![CDATA[<p><a class="link" href="https://arxiv.org/pdf/1812.00332.pdf%C3%AF%C2%BC%E2%80%B0">pdf <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184702681.png" alt="image-20240330184702681"></p>
<p>In the realm of deep learning, the design of neural network architectures has been significantly advanced by automated methods, offering a promising solution to the labor-intensive process of manual design. However, these conventional algorithms demand substantial computational resources, often requiring thousands of models to be trained on a single task, making their application to large-scale problems like ImageNet computationally prohibitive.</p>
<p>To address this challenge, researchers have historically employed proxy tasks—smaller datasets or simplified versions of the target task—to expedite the search process. While this approach reduces the computational burden, it introduces a critical limitation: architectures optimized for proxy tasks may not perform optimally on the actual target task, particularly when hardware metrics such as latency are considered.</p>
<p>This discrepancy between proxy-based training and real-world application has motivated the development of a novel approach that bypasses the need for proxies, allowing for direct learning of neural network architectures tailored to specific tasks and hardware platforms. By doing so, this method not only promises to enhance the performance and efficiency of neural networks but also aims to democratize the application of neural architecture search across various domains by significantly reducing the computational and memory costs associated with the process.</p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>In the realm of deep learning, the design of neural network architectures has been significantly advanced by automated methods, offering a promising solution to the labor-intensive process of manual design. However, these conventional algorithms demand substantial computational resources, often requiring thousands of models to be trained on a single task, making their application to large-scale problems like ImageNet computationally prohibitive.</p>
<p>To address this challenge, researchers have historically employed proxy tasks—smaller datasets or simplified versions of the target task—to expedite the search process. While this approach reduces the computational burden, it introduces a critical limitation: architectures optimized for proxy tasks may not perform optimally on the actual target task, particularly when hardware metrics such as latency are considered.</p>
<p>This discrepancy between proxy-based training and real-world application has motivated the development of a novel approach that bypasses the need for proxies, allowing for direct learning of neural network architectures tailored to specific tasks and hardware platforms. By doing so, this method not only promises to enhance the performance and efficiency of neural networks but also aims to democratize the application of neural architecture search across various domains by significantly reducing the computational and memory costs associated with the process.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>The methodology presented in the paper revolves around a novel approach to Neural Architecture Search (NAS) that directly targets the optimization of neural network architectures for specific tasks and hardware without relying on proxy tasks. This approach, known as ProxylessNAS, introduces a series of innovative techniques to tackle the computational and memory challenges associated with traditional NAS methods.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184719856.png" alt="image-20240330184719856"></p>
<p>At the core of ProxylessNAS is the concept of path-level pruning, which treats the architecture search as a process of training an over-parameterized network that contains all candidate paths. This network is then pruned to eliminate redundancy and arrive at an optimized architecture. The method introduces architecture parameters to learn which paths are redundant, and these paths are pruned at the end of the training process.</p>
<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184728689.png" alt="image-20240330184728689" style="zoom:50%;">

<p>To manage the high GPU memory consumption, the authors propose a binarization strategy for the architecture parameters. By converting the real-valued path weights into binary gates, the method ensures that only one path is active at runtime, thus reducing the memory requirement to the level of training a compact model. This binarization process is facilitated by a gradient-based training approach that leverages ideas from BinaryConnect.</p>
<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184741634.png" alt="image-20240330184741634" style="zoom:50%;">

<img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184754650.png" alt="image-20240330184754650" style="zoom:50%;">

<p>In addition to addressing computational efficiency, the paper also presents techniques for handling non-differentiable hardware objectives such as latency. The authors propose a method to make latency differentiable by modeling it as a continuous function of the network’s architecture parameters. This allows the latency to be incorporated as a regularization term in the loss function, enabling the direct optimization of the network’s latency alongside its accuracy.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184815706.png" alt="image-20240330184815706"></p>
<p>Furthermore, the paper introduces an alternative strategy based on the REINFORCE algorithm to handle hardware metrics. This approach views the update of binarized architecture parameters as a process of maximizing a certain reward, which can handle non-differentiable objectives without requiring them to be differentiable.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184827879.png" alt="image-20240330184827879"></p>
<p>The methodology is validated through extensive experiments on CIFAR-10 and ImageNet, where the proposed approach demonstrates its ability to achieve superior empirical results. The direct search strategy allows for a more efficient exploration of the architecture space, leading to architectures that are not only accurate but also optimized for hardware-specific metrics such as latency.</p>
<p>Overall, the paper’s methodology represents a significant advancement in the field of NAS. It provides a new perspective on how to conduct architecture search in a way that is both computationally efficient and directly aligned with the target task and hardware. The introduction of techniques to handle non-differentiable objectives expands the scope of NAS and paves the way for the development of specialized neural network architectures for a variety of hardware platforms.</p>
<h2 id="Figures-Conclusion"><a href="#Figures-Conclusion" class="headerlink" title="Figures & Conclusion"></a>Figures &amp; Conclusion</h2><p>The experimental framework of the paper is meticulously designed to validate the effectiveness of the proposed method, ProxylessNAS, through a series of comprehensive experiments on benchmark datasets. The experiments are aimed at demonstrating the method’s capacity to directly learn neural network architectures for large-scale tasks and target hardware platforms, as well as its ability to optimize for both accuracy and hardware-specific metrics such as latency.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184840327.png" alt="image-20240330184840327"></p>
<p>The experiments on the CIFAR-10 dataset utilize a tree-structured architecture space, employing a validation set of 5,000 images to learn architecture parameters. The method is evaluated based on the test error rate, with the ProxylessNAS achieving a remarkable 2.08% test error, outperforming previous state-of-the-art architectures like AmoebaNet-B. Notably, the model operates with only 5.7M parameters, significantly fewer than the 34.9M parameters of AmoebaNet-B, showcasing the efficiency of the proposed method in terms of parameter count.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184848419.png" alt="image-20240330184848419"></p>
<p>On the ImageNet dataset, the experiments focus on learning efficient Convolutional Neural Network (CNN) architectures that balance high accuracy with low latency, particularly on mobile phones, GPUs, and CPUs. The architecture space is based on MobileNetV2, with a variety of mobile inverted bottleneck convolution (MBConv) layers and expansion ratios allowed. The method incorporates a latency prediction model during the search process, which correlates strongly with real-world latency measurements, demonstrating the model’s predictive accuracy.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184906645.png" alt="image-20240330184906645"></p>
<p>The results on ImageNet are equally impressive, with the proposed method improving the top-1 accuracy by 2.6% over MobileNetV2 while maintaining a similar latency on mobile phones. The model also outperforms MnasNet in terms of accuracy, with a 0.6% higher top-1 accuracy and slightly lower mobile latency, all while using significantly fewer GPU hours for the search process.</p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184920271.png" alt="image-20240330184920271"></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/hjchen-thu/picb/img_blog/image-20240330184942387.png" alt="image-20240330184942387"></p>
<p>The paper further explores the specialized models optimized for different hardware platforms, revealing that each platform prefers distinct architectural designs. For instance, the GPU model tends to be shallower and wider, while the CPU model opts for deeper and narrower architectures. The mobile model exhibits a preference for larger MBConv operations in the initial stages of each layer where downsampling occurs.</p>
]]></content>
      <categories>
        <category>Work hard</category>
        <category>PaperReading</category>
      </categories>
      <tags>
        <tag>NAS</tag>
      </tags>
  </entry>
</search>
